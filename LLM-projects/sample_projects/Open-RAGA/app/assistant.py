from langchain_core.runnables import Runnable, RunnableConfig
from typing import Any, Dict, Optional
from langchain_core.runnables.utils import Input, Output
from .generation.prompt_templates import get_template
from langchain_core.output_parsers import StrOutputParser
from langchain.chains import LLMChain
from langchain.chains import create_history_aware_retriever
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.runnables import RunnableLambda, RunnableMap
import os
from .settings import PROMPT_TEMPLATES_PATH 
import yaml
from langchain.memory import ConversationBufferWindowMemory
from pathlib import Path
import re



# Load a .yaml or .yml file
with open(PROMPT_TEMPLATES_PATH , "r",encoding="utf-8") as file:
    prompt_config = yaml.safe_load(file)




translate_prompt_Temp = get_template('translator')['translator']
history_aware_prompt = prompt_config['context_aware_prompt']['prompt']
details_agent_prompt = prompt_config['details_agent_prompt']['prompt']
translate_to_hindi = get_template('translator_english_hindi')['translator_english_hindi']

# Chat manager class to manage chat
class ChatManager:
    """ 
    this chat manager class will handle vector database and llm with langchain this class create the translation chain,history_aware_retriever_chain and  q_a_chain
    with langchain.
    """
    def __init__(self, llm, vector_db):

        self.llm = llm
        self.vector_db = vector_db
        self.rag_chain = self._create_rag_chain()


    
     # Create a rag chain
    def _create_rag_chain(self):
        """this will create a combine chain :
        first it will retrieve the similar context then it will pass to the qa chain in which llm will use this retrieved context for question answer.
        """
        return create_retrieval_chain(
            self.history_aware_retriever_chain(),
            self.q_a_chain()
        )



    # history aware/context aware chain for retrieving the context from the vector database
    def history_aware_retriever_chain(self):
        """ 
        this function will take the chat history,user query and history_aware_prompt to create a standalone question then this will pass to the history_aware_retriever_chain
        chain that will retrieve the simlilar context.    
        """


       # ChatPromptTemplate to pass list of args or prompts to create a standlone question 
        contextualize_q_prompt = ChatPromptTemplate.from_messages(
            [
                ("assistant", history_aware_prompt),
                MessagesPlaceholder("chat_history"),
                ("user", "{input}"),
            ]
        )


        # retrieving the context using similar to the question
        history_aware_retriever_chain = create_history_aware_retriever(
            self.llm, self.vector_db, contextualize_q_prompt
        )

        return history_aware_retriever_chain




    # question answer chain for the answer  the  questions from the retrieved context
    def q_a_chain(self):
        """ 
        this qa chain will use to answer the question this is a details agent chain this will generate the final answer
        """
        
        qa_prompt = ChatPromptTemplate.from_messages(
            [
                ("assistant", details_agent_prompt),
                MessagesPlaceholder("chat_history"),
                ("user", "{input}"),
            ])

        details_agent_chain = create_stuff_documents_chain(self.llm, qa_prompt)

        return details_agent_chain


   # combine all steps in run function that will takes query and return the reponse
    async def run(self,query,chat_history)->Any:
        """
        process query and  combine all chains.
    
        Args:
        query(str): query/question asked by user
        chat_history(list): user chat history
    
        Returns:
          response/answer generated by llm
        
        """

        # translate the text from romanized hindi to english
        translated = (translate_prompt_Temp | self.llm ).invoke({'query': query})

        # get the actual translations
        translated_text = translated.content.strip()

        # call the history_aware_retriever_chain  and q_a chain
        result = self.rag_chain.invoke({"input": translated_text, "chat_history": chat_history })

        # translate from english to hindi   
        translated = ( translate_to_hindi | self.llm).invoke({'query': result['answer']})
        
        print("----translated---",translated.content.strip())
        
        return  translated.content.strip()



