{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-title",
   "metadata": {},
   "source": [
    "# The Ultimate AI Agent Memory Lab: An End-to-End Implementation\n",
    "\n",
    "### A Comprehensive, Hands-On Workshop for Building Smarter Agents\n",
    "\n",
    "Welcome to a detailed, practical exploration of memory in AI agents. This notebook is designed to be a definitive, end-to-end guide that moves beyond theory and into tangible, working code. We will implement **nine distinct memory strategies**, from the simplest to the most conceptually advanced, using a real large language model for generation, summarization, and embedding.\n",
    "\n",
    "**Objective:** By the end of this lab, you will have a deep, practical understanding of:\n",
    "- How each memory strategy works under the hood.\n",
    "- The specific strengths, weaknesses, and tradeoffs of each approach.\n",
    "- How to implement these strategies in Python using modern tools like `openai`, `faiss-cpu`, and `networkx`.\n",
    "- How the choice of memory architecture fundamentally changes an agent's conversational abilities, cost, and complexity.\n",
    "\n",
    "**Structure of the Lab:**\n",
    "1.  **Part 1: The Core Framework.** We'll set up our environment, configure the LLM client, and build the foundational `AIAgent` and `BaseMemoryStrategy` classes.\n",
    "2.  **Part 2: The Memory Implementations.** We will systematically implement and demonstrate all nine memory strategies. Each strategy will have its own dedicated section with:\n",
    "    *   **Detailed Theory:** Explaining the *what*, *why*, and *how*.\n",
    "    *   **Code Implementation:** A complete, commented Python class for the strategy.\n",
    "    *   **Live Demonstration:** A practical chat session designed to showcase the strategy's unique behavior.\n",
    "\n",
    "This notebook is intentionally lengthy and detailed to serve as a comprehensive reference. Let's begin by setting up our core components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-heading",
   "metadata": {},
   "source": [
    "## Part 1: Core Framework and Setup\n",
    "\n",
    "Before we can build memories, we need a brain (the LLM) and a body (the agent framework). This section handles all the preliminary setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-deps",
   "metadata": {},
   "source": [
    "### Step 1.1: Installing Dependencies\n",
    "\n",
    "First, we need to install the necessary Python libraries. We'll need:\n",
    "- `openai`: The client library for interacting with the LLM API.\n",
    "- `numpy`: For numerical operations, especially with embeddings.\n",
    "- `faiss-cpu`: A library from Facebook AI for efficient similarity search, which will power our retrieval memory. It's a perfect in-memory vector database.\n",
    "- `networkx`: For creating and managing the knowledge graph in our Graph-Based Memory strategy.\n",
    "- `tiktoken`: To accurately count tokens and manage context window limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "install-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai numpy faiss-cpu networkx tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "client-setup-theory",
   "metadata": {},
   "source": [
    "### Step 1.2: Configuring the LLM and Embedding Client\n",
    "\n",
    "Here, we'll set up the `OpenAI` client with the custom `base_url` and `api_key` you provided. This single client will be used for both text generation and creating embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "client-setup-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client configured successfully.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# --- IMPORTANT: API Key Configuration ---\n",
    "# The API key is provided directly here for simplicity in this notebook.\n",
    "# In a production environment, NEVER hardcode keys. Use environment variables\n",
    "# or a secure secret management service.\n",
    "\n",
    "# Define the API key for authentication.\n",
    "API_KEY = \"YOUR_API_KEY_HERE\"\n",
    "# Define the base URL for the API endpoint.\n",
    "BASE_URL = \"https://api.studio.nebius.com/v1/\"\n",
    "\n",
    "# Initialize the OpenAI client with the specified base URL and API key.\n",
    "client = OpenAI(\n",
    "    base_url=BASE_URL,\n",
    "    api_key=API_KEY\n",
    ")\n",
    "\n",
    "# Print a confirmation message to indicate successful client setup.\n",
    "print(\"OpenAI client configured successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpers-theory",
   "metadata": {},
   "source": [
    "### Step 1.3: Helper Functions for LLM Interaction and Token Counting\n",
    "\n",
    "To keep our main agent logic clean, we'll create wrapper functions for our API calls. We'll also set up a token counter, which is crucial for understanding the costs and limitations of our memory strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpers-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions and token counter are ready.\n"
     ]
    }
   ],
   "source": [
    "# Import additional libraries for functionality.\n",
    "import tiktoken\n",
    "import time\n",
    "\n",
    "# --- Model Configuration ---\n",
    "# Define the specific models to be used for generation and embedding tasks.\n",
    "# These are hardcoded for this lab but could be loaded from a config file.\n",
    "GENERATION_MODEL = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "EMBEDDING_MODEL = \"BAAI/bge-multilingual-gemma2\"\n",
    "\n",
    "def generate_text(system_prompt: str, user_prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Calls the LLM API to generate a text response.\n",
    "    \n",
    "    Args:\n",
    "        system_prompt: The instruction that defines the AI's role and behavior.\n",
    "        user_prompt: The user's input to which the AI should respond.\n",
    "        \n",
    "    Returns:\n",
    "        The generated text content from the AI, or an error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a chat completion request to the configured client.\n",
    "        response = client.chat.completions.create(\n",
    "            model=GENERATION_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "        )\n",
    "        # Extract and return the content of the AI's message.\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        # Handle potential API errors gracefully.\n",
    "        print(f\"An error occurred during text generation: {e}\")\n",
    "        return \"I'm sorry, I encountered an error and couldn't process your request.\"\n",
    "\n",
    "def generate_embedding(text: str) -> list[float]:\n",
    "    \"\"\"\n",
    "    Generates a numerical embedding for a given text string using the embedding model.\n",
    "    \n",
    "    Args:\n",
    "        text: The input string to be converted into an embedding.\n",
    "        \n",
    "    Returns:\n",
    "        A list of floats representing the embedding vector, or an empty list on error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create an embedding request to the configured client.\n",
    "        response = client.embeddings.create(\n",
    "            model=EMBEDDING_MODEL,\n",
    "            input=text\n",
    "        )\n",
    "        # Extract and return the embedding vector from the response data.\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        # Handle potential API errors gracefully.\n",
    "        print(f\"An error occurred during embedding generation: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- Token Counting Setup ---\n",
    "# Initialize the tokenizer using tiktoken. 'cl100k_base' is a common encoding\n",
    "# used by many modern models, including those from OpenAI and Llama.\n",
    "# This allows us to accurately estimate the size of our prompts before sending them.\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"\n",
    "    Counts the number of tokens in a given string using the pre-loaded tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        text: The string to be tokenized.\n",
    "        \n",
    "    Returns:\n",
    "        The integer count of tokens.\n",
    "    \"\"\"\n",
    "    # The `encode` method converts the string into a list of token IDs.\n",
    "    # The length of this list is the token count.\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "# Print a confirmation message to indicate that these core functions are ready for use.\n",
    "print(\"Helper functions and token counter are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "base-classes-theory",
   "metadata": {},
   "source": [
    "### Step 1.4: The Foundational Agent and Memory Classes\n",
    "\n",
    "Now we define the core structure of our system using the Strategy Design Pattern.\n",
    "\n",
    "- **`BaseMemoryStrategy`**: An abstract base class that defines the universal interface for any memory type. Every strategy we create will inherit from this, ensuring it can be seamlessly plugged into our agent.\n",
    "- **`AIAgent`**: The agent class itself. It is initialized with a memory strategy. Its `chat` method orchestrates the process of getting context from memory, querying the LLM, and updating the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "base-classes-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "# --- Abstract Base Class for Memory Strategies ---\n",
    "# This class defines the 'contract' that all memory strategies must follow.\n",
    "# By using an Abstract Base Class (ABC), we ensure that any memory implementation\n",
    "# we create will have the same core methods (add_message, get_context, clear),\n",
    "# allowing them to be interchangeably plugged into the AIAgent.\n",
    "class BaseMemoryStrategy(abc.ABC):\n",
    "    \"\"\"Abstract Base Class for all memory strategies.\"\"\"\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def add_message(self, user_input: str, ai_response: str):\n",
    "        \"\"\"\n",
    "        An abstract method that must be implemented by subclasses.\n",
    "        It's responsible for adding a new user-AI interaction to the memory store.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_context(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        An abstract method that must be implemented by subclasses.\n",
    "        It retrieves and formats the relevant context from memory to be sent to the LLM.\n",
    "        The 'query' parameter allows some strategies (like retrieval) to fetch context\n",
    "        that is specifically relevant to the user's latest input.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def clear(self):\n",
    "        \"\"\"\n",
    "        An abstract method that must be implemented by subclasses.\n",
    "        It provides a way to reset the memory, which is useful for starting new conversations.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "# --- The Core AI Agent ---\n",
    "# This class orchestrates the entire conversation flow. It is initialized with a\n",
    "# specific memory strategy and uses it to manage the conversation's context.\n",
    "class AIAgent:\n",
    "    \"\"\"The main AI Agent class, designed to work with any memory strategy.\"\"\"\n",
    "    \n",
    "    def __init__(self, memory_strategy: BaseMemoryStrategy, system_prompt: str = \"You are a helpful AI assistant.\"):\n",
    "        \"\"\"\n",
    "        Initializes the agent.\n",
    "        \n",
    "        Args:\n",
    "            memory_strategy: An instance of a class that inherits from BaseMemoryStrategy.\n",
    "                             This determines how the agent will remember the conversation.\n",
    "            system_prompt: The initial instruction given to the LLM to define its persona and task.\n",
    "        \"\"\"\n",
    "        self.memory = memory_strategy\n",
    "        self.system_prompt = system_prompt\n",
    "        print(f\"Agent initialized with {type(memory_strategy).__name__}.\")\n",
    "\n",
    "    def chat(self, user_input: str):\n",
    "        \"\"\"\n",
    "        Handles a single turn of the conversation.\n",
    "        \n",
    "        Args:\n",
    "            user_input: The latest message from the user.\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*25} NEW INTERACTION {'='*25}\")\n",
    "        print(f\"User > {user_input}\")\n",
    "        \n",
    "        # Step 1: Retrieve context from the agent's memory strategy.\n",
    "        # This is where the specific memory logic (e.g., sequential, retrieval) is executed.\n",
    "        start_time = time.time()\n",
    "        context = self.memory.get_context(query=user_input)\n",
    "        retrieval_time = time.time() - start_time\n",
    "        \n",
    "        # Step 2: Construct the full prompt for the LLM.\n",
    "        # This combines the retrieved historical context with the user's current request.\n",
    "        full_user_prompt = f\"### MEMORY CONTEXT\\n{context}\\n\\n### CURRENT REQUEST\\n{user_input}\"\n",
    "        \n",
    "        # Step 3: Provide detailed debug information.\n",
    "        # This is crucial for understanding how the memory strategy affects the prompt size and cost.\n",
    "        prompt_tokens = count_tokens(self.system_prompt + full_user_prompt)\n",
    "        print(\"\\n--- Agent Debug Info ---\")\n",
    "        print(f\"Memory Retrieval Time: {retrieval_time:.4f} seconds\")\n",
    "        print(f\"Estimated Prompt Tokens: {prompt_tokens}\")\n",
    "        print(f\"\\n[Full Prompt Sent to LLM]:\\n---\\nSYSTEM: {self.system_prompt}\\nUSER: {full_user_prompt}\\n---\")\n",
    "        \n",
    "        # Step 4: Call the LLM to get a response.\n",
    "        # The LLM uses the system prompt and the combined user prompt (context + new query) to generate a reply.\n",
    "        start_time = time.time()\n",
    "        ai_response = generate_text(self.system_prompt, full_user_prompt)\n",
    "        generation_time = time.time() - start_time\n",
    "        \n",
    "        # Step 5: Update the memory with the latest interaction.\n",
    "        # This ensures the current turn is available for future context retrieval.\n",
    "        self.memory.add_message(user_input, ai_response)\n",
    "        \n",
    "        # Step 6: Display the AI's response and performance metrics.\n",
    "        print(f\"\\nAgent > {ai_response}\")\n",
    "        print(f\"(LLM Generation Time: {generation_time:.4f} seconds)\")\n",
    "        print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-heading",
   "metadata": {},
   "source": [
    "## Part 2: Implementation and Demonstration of Memory Strategies\n",
    "\n",
    "This is the core of our lab. We will now implement each of the nine memory strategies one by one, followed immediately by a live demonstration to see how they perform in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sequential-theory",
   "metadata": {},
   "source": [
    "### Strategy 1: Sequential (Keep-It-All) Memory\n",
    "\n",
    "| **Best For**                  | **Tradeoff**                                           |\n",
    "| ----------------------------- | ------------------------------------------------------ |\n",
    "| Short interactions, full fidelity                    | Hits token limit fast, expensive                       |\n",
    "\n",
    "**Theory:** This is the most straightforward memory type. It simply appends every user-AI interaction to a growing list. When generating a new response, the entire conversation history is formatted and sent to the LLM as context. This guarantees perfect, lossless recall within a single conversation session.\n",
    "\n",
    "However, its simplicity is its downfall in long conversations. The context grows linearly with each turn, leading to rapidly increasing API costs and eventually exceeding the LLM's maximum context window, which would cause an error or a truncated prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sequential-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Strategy 1: Sequential (Keep-It-All) Memory ---\n",
    "# This is the most basic memory strategy. It stores the entire conversation\n",
    "# history in a simple list. While it provides perfect recall, it is not scalable\n",
    "# as the context sent to the LLM grows with every turn, quickly becoming expensive\n",
    "# and hitting token limits.\n",
    "class SequentialMemory(BaseMemoryStrategy):\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the memory with an empty list to store conversation history.\"\"\"\n",
    "        self.history = []\n",
    "\n",
    "    def add_message(self, user_input: str, ai_response: str):\n",
    "        \"\"\"\n",
    "        Adds a new user-AI interaction to the history.\n",
    "        Each interaction is stored as two dictionary entries in the list.\n",
    "        \"\"\"\n",
    "        self.history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "\n",
    "    def get_context(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Retrieves the entire conversation history and formats it into a single\n",
    "        string to be used as context for the LLM. The 'query' parameter is ignored\n",
    "        as this strategy always returns the full history.\n",
    "        \"\"\"\n",
    "        # Join all messages into a single newline-separated string.\n",
    "        return \"\\n\".join([f\"{turn['role'].capitalize()}: {turn['content']}\" for turn in self.history])\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"Resets the conversation history by clearing the list.\"\"\"\n",
    "        self.history = []\n",
    "        print(\"Sequential memory cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sequential-demo-theory",
   "metadata": {},
   "source": [
    "#### Demonstration of Sequential Memory\n",
    "\n",
    "**What to watch for:** Pay close attention to the `Estimated Prompt Tokens` in the debug output. You will see it increase significantly with each turn as the entire history is added to the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sequential-demo-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent initialized with SequentialMemory.\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > Hi there! My name is Sam.\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.0000 seconds\n",
      "Estimated Prompt Tokens: 23\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "\n",
      "\n",
      "### CURRENT REQUEST\n",
      "Hi there! My name is Sam.\n",
      "---\n",
      "\n",
      "Agent > Hello Sam! It's nice to meet you. I'm happy to chat with you. What brings you here today? Do you have any questions, need assistance with something, or just want to have a friendly conversation? I'm all ears (or rather, all text)!\n",
      "(LLM Generation Time: 2.2516 seconds)\n",
      "======================================================================\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > I'm interested in learning about space exploration.\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.0000 seconds\n",
      "Estimated Prompt Tokens: 92\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "User: Hi there! My name is Sam.\n",
      "Assistant: Hello Sam! It's nice to meet you. I'm happy to chat with you. What brings you here today? Do you have any questions, need assistance with something, or just want to have a friendly conversation? I'm all ears (or rather, all text)!\n",
      "\n",
      "### CURRENT REQUEST\n",
      "I'm interested in learning about space exploration.\n",
      "---\n",
      "\n",
      "Agent > Blast off into the cosmos with me, Sam! Space exploration is a fascinating topic, and I'm more than happy to take you on a journey through the stars.\n",
      "\n",
      "Let's start with some basics. What specifically would you like to know about space exploration? Are you interested in:\n",
      "\n",
      "1. **Mars missions**: Recent developments in sending humans to the Red Planet, upcoming missions, or the challenges of living on Mars?\n",
      "2. **Space agencies**: NASA, ESA, Roscosmos, or other organizations involved in space exploration? I can tell you about their history, current projects, and future plans.\n",
      "3. **Private space companies**: SpaceX, Blue Origin, or other emerging players in the space industry? I can share insights on their innovative approaches, successes, and challenges.\n",
      "4. **Space tourism**: The growing industry of space travel for leisure, or the latest advancements in space technology for commercial purposes?\n",
      "5. **Astronomy and the search for life**: The latest discoveries in exoplanet hunting, the potential for life beyond Earth, or the ongoing search for extraterrestrial intelligence (SETI)?\n",
      "6. **Something else**: Perhaps you have a specific question or topic in mind? Let me know, and I'll do my best to provide you with interesting and up-to-date information.\n",
      "\n",
      "Which direction would you like to take our space exploration journey, Sam?\n",
      "(LLM Generation Time: 4.4615 seconds)\n",
      "======================================================================\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > What was the first thing I told you?\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.0000 seconds\n",
      "Estimated Prompt Tokens: 379\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "User: Hi there! My name is Sam.\n",
      "Assistant: Hello Sam! It's nice to meet you. I'm happy to chat with you. What brings you here today? Do you have any questions, need assistance with something, or just want to have a friendly conversation? I'm all ears (or rather, all text)!\n",
      "User: I'm interested in learning about space exploration.\n",
      "Assistant: Blast off into the cosmos with me, Sam! Space exploration is a fascinating topic, and I'm more than happy to take you on a journey through the stars.\n",
      "\n",
      "Let's start with some basics. What specifically would you like to know about space exploration? Are you interested in:\n",
      "\n",
      "1. **Mars missions**: Recent developments in sending humans to the Red Planet, upcoming missions, or the challenges of living on Mars?\n",
      "2. **Space agencies**: NASA, ESA, Roscosmos, or other organizations involved in space exploration? I can tell you about their history, current projects, and future plans.\n",
      "3. **Private space companies**: SpaceX, Blue Origin, or other emerging players in the space industry? I can share insights on their innovative approaches, successes, and challenges.\n",
      "4. **Space tourism**: The growing industry of space travel for leisure, or the latest advancements in space technology for commercial purposes?\n",
      "5. **Astronomy and the search for life**: The latest discoveries in exoplanet hunting, the potential for life beyond Earth, or the ongoing search for extraterrestrial intelligence (SETI)?\n",
      "6. **Something else**: Perhaps you have a specific question or topic in mind? Let me know, and I'll do my best to provide you with interesting and up-to-date information.\n",
      "\n",
      "Which direction would you like to take our space exploration journey, Sam?\n",
      "\n",
      "### CURRENT REQUEST\n",
      "What was the first thing I told you?\n",
      "---\n",
      "\n",
      "Agent > The first thing you told me was: \"Hi there! My name is Sam.\"\n",
      "(LLM Generation Time: 0.5205 seconds)\n",
      "======================================================================\n",
      "Sequential memory cleared.\n"
     ]
    }
   ],
   "source": [
    "# Initialize and run the agent\n",
    "# Create an instance of our SequentialMemory strategy.\n",
    "sequential_memory = SequentialMemory()\n",
    "# Create an AIAgent and inject the sequential memory strategy into it.\n",
    "agent = AIAgent(memory_strategy=sequential_memory)\n",
    "\n",
    "# --- Start the conversation ---\n",
    "# First turn: The user introduces themselves.\n",
    "agent.chat(\"Hi there! My name is Sam.\")\n",
    "# Second turn: The user states their interest.\n",
    "agent.chat(\"I'm interested in learning about space exploration.\")\n",
    "# Third turn: The user tests the agent's memory.\n",
    "agent.chat(\"What was the first thing I told you?\")\n",
    "\n",
    "# The agent has perfect recall because the entire history is in the context.\n",
    "# Clean up the memory for the next demonstration.\n",
    "sequential_memory.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sliding-window-theory",
   "metadata": {},
   "source": [
    "### Strategy 2: Sliding Window Memory\n",
    "\n",
    "| **Best For**                  | **Tradeoff**                                           |\n",
    "| ----------------------------- | ------------------------------------------------------ |\n",
    "| Mid-length chats, recent relevance matters           | Forgets early context                                  |\n",
    "\n",
    "**Theory:** This strategy addresses the primary issue of Sequential Memory by only keeping the most recent `N` conversation turns. It uses a `deque` (double-ended queue) with a fixed maximum length. When a new interaction is added and the deque is full, the oldest interaction is automatically discarded.\n",
    "\n",
    "This keeps the context size constant, making costs predictable and preventing context window overflow. The major tradeoff is amnesia: any information mentioned before the window's cutoff point is permanently forgotten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sliding-window-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the deque class from the collections module. A deque is a double-ended\n",
    "# queue that is highly efficient for adding and removing elements from either end.\n",
    "from collections import deque\n",
    "\n",
    "# --- Strategy 2: Sliding Window Memory ---\n",
    "# This strategy keeps only the 'N' most recent turns of the conversation.\n",
    "# It prevents the context from growing indefinitely, making it scalable and\n",
    "# cost-effective, but at the cost of forgetting older information.\n",
    "class SlidingWindowMemory(BaseMemoryStrategy):\n",
    "    def __init__(self, window_size: int = 4): # window_size is number of turns (user + AI = 1 turn)\n",
    "        \"\"\"\n",
    "        Initializes the memory with a deque of a fixed size.\n",
    "        \n",
    "        Args:\n",
    "            window_size: The number of conversational turns to keep in memory.\n",
    "                         A single turn consists of one user message and one AI response.\n",
    "        \"\"\"\n",
    "        # A deque with 'maxlen' will automatically discard the oldest item\n",
    "        # when a new item is added and the deque is full. This is the core\n",
    "        # mechanism of the sliding window. We store turns, so maxlen is window_size.\n",
    "        self.history = deque(maxlen=window_size)\n",
    "\n",
    "    def add_message(self, user_input: str, ai_response: str):\n",
    "        \"\"\"\n",
    "        Adds a new conversational turn to the history. If the deque is full,\n",
    "        the oldest turn is automatically removed.\n",
    "        \"\"\"\n",
    "        # Each turn (user input + AI response) is stored as a single element\n",
    "        # in the deque. This makes it easy to manage the window size by turns.\n",
    "        self.history.append([\n",
    "            {\"role\": \"user\", \"content\": user_input},\n",
    "            {\"role\": \"assistant\", \"content\": ai_response}\n",
    "        ])\n",
    "\n",
    "    def get_context(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Retrieves the conversation history currently within the window and\n",
    "        formats it into a single string. The 'query' parameter is ignored.\n",
    "        \"\"\"\n",
    "        # Create a temporary list to hold the formatted messages.\n",
    "        context_list = []\n",
    "        # Iterate through each turn stored in the deque.\n",
    "        for turn in self.history:\n",
    "            # Iterate through the user and assistant messages within that turn.\n",
    "            for message in turn:\n",
    "                # Format the message and add it to our list.\n",
    "                context_list.append(f\"{message['role'].capitalize()}: {message['content']}\")\n",
    "        # Join all the formatted messages into a single string, separated by newlines.\n",
    "        return \"\\n\".join(context_list)\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"Resets the conversation history by clearing the deque.\"\"\"\n",
    "        self.history.clear()\n",
    "        print(\"Sliding window memory cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sliding-window-demo-theory",
   "metadata": {},
   "source": [
    "#### Demonstration of Sliding Window Memory\n",
    "\n",
    "**What to watch for:** We'll set a window size of 2 turns. After the third turn, the very first piece of information (the user's name) will be pushed out of the context window. The agent will then fail to recall it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sliding-window-demo-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent initialized with SlidingWindowMemory.\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > My name is Priya and I'm a software developer.\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.0000 seconds\n",
      "Estimated Prompt Tokens: 27\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "\n",
      "\n",
      "### CURRENT REQUEST\n",
      "My name is Priya and I'm a software developer.\n",
      "---\n",
      "\n",
      "Agent > Nice to meet you, Priya! What can I assist you with today as a software developer? Do you have a project you're working on or something on your mind that you'd like help with?\n",
      "(LLM Generation Time: 1.1071 seconds)\n",
      "======================================================================\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > I work primarily with Python and cloud technologies.\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.0000 seconds\n",
      "Estimated Prompt Tokens: 81\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "User: My name is Priya and I'm a software developer.\n",
      "Assistant: Nice to meet you, Priya! What can I assist you with today as a software developer? Do you have a project you're working on or something on your mind that you'd like help with?\n",
      "\n",
      "### CURRENT REQUEST\n",
      "I work primarily with Python and cloud technologies.\n",
      "---\n",
      "\n",
      "Agent > Nice to meet you too, Priya! Given your expertise in Python and cloud technologies, I'm assuming you work on projects that involve backend development, perhaps with AWS, Azure, or Google Cloud? Which one of these areas would you like assistance with today? Or maybe you're looking to improve your Python coding skills or explore a specific topic, like machine learning or data science?\n",
      "(LLM Generation Time: 1.4009 seconds)\n",
      "======================================================================\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > My favorite hobby is hiking.\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.0000 seconds\n",
      "Estimated Prompt Tokens: 167\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "User: My name is Priya and I'm a software developer.\n",
      "Assistant: Nice to meet you, Priya! What can I assist you with today as a software developer? Do you have a project you're working on or something on your mind that you'd like help with?\n",
      "User: I work primarily with Python and cloud technologies.\n",
      "Assistant: Nice to meet you too, Priya! Given your expertise in Python and cloud technologies, I'm assuming you work on projects that involve backend development, perhaps with AWS, Azure, or Google Cloud? Which one of these areas would you like assistance with today? Or maybe you're looking to improve your Python coding skills or explore a specific topic, like machine learning or data science?\n",
      "\n",
      "### CURRENT REQUEST\n",
      "My favorite hobby is hiking.\n",
      "---\n",
      "\n",
      "Agent > It seems we had a nice conversation about your background as a software developer earlier. We explored your experience with Python and cloud technologies, but then we jumped to a new topic unrelated to your software development background.\n",
      "\n",
      "If you'd like to continue the conversation about your favorite hobby, I'd be happy to chat with you about hiking! What do you enjoy most about hiking? Are you a seasoned hiker, or has it become more of a hobby you've recently taken up?\n",
      "(LLM Generation Time: 1.5967 seconds)\n",
      "======================================================================\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > What is my name?\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.0000 seconds\n",
      "Estimated Prompt Tokens: 213\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "User: I work primarily with Python and cloud technologies.\n",
      "Assistant: Nice to meet you too, Priya! Given your expertise in Python and cloud technologies, I'm assuming you work on projects that involve backend development, perhaps with AWS, Azure, or Google Cloud? Which one of these areas would you like assistance with today? Or maybe you're looking to improve your Python coding skills or explore a specific topic, like machine learning or data science?\n",
      "User: My favorite hobby is hiking.\n",
      "Assistant: It seems we had a nice conversation about your background as a software developer earlier. We explored your experience with Python and cloud technologies, but then we jumped to a new topic unrelated to your software development background.\n",
      "\n",
      "If you'd like to continue the conversation about your favorite hobby, I'd be happy to chat with you about hiking! What do you enjoy most about hiking? Are you a seasoned hiker, or has it become more of a hobby you've recently taken up?\n",
      "\n",
      "### CURRENT REQUEST\n",
      "What is my name?\n",
      "---\n",
      "\n",
      "Agent > Based on our earlier conversation, I think I know the answer to that!\n",
      "\n",
      "Your name is Priya, correct?\n",
      "(LLM Generation Time: 0.5437 seconds)\n",
      "======================================================================\n",
      "Sliding window memory cleared.\n"
     ]
    }
   ],
   "source": [
    "# Initialize with a small window size of 2 turns.\n",
    "# This means the agent will only remember the last two user-AI interactions.\n",
    "sliding_memory = SlidingWindowMemory(window_size=2)\n",
    "# Create an AIAgent and inject the sliding window memory strategy.\n",
    "agent = AIAgent(memory_strategy=sliding_memory)\n",
    "\n",
    "# --- Start the conversation ---\n",
    "# First turn: The user introduces themselves. This is Turn 1.\n",
    "agent.chat(\"My name is Priya and I'm a software developer.\")\n",
    "# Second turn: The user provides more details. The memory now holds Turn 1 and Turn 2.\n",
    "agent.chat(\"I work primarily with Python and cloud technologies.\")\n",
    "# Third turn: The user mentions a hobby. Adding this turn pushes Turn 1 out of the\n",
    "# fixed-size deque. The memory now only holds Turn 2 and Turn 3.\n",
    "agent.chat(\"My favorite hobby is hiking.\")\n",
    "\n",
    "# Now, ask about the first thing mentioned.\n",
    "# The context sent to the LLM will only contain the information about Python/cloud and hiking.\n",
    "# The information about the user's name has been forgotten.\n",
    "agent.chat(\"What is my name?\")\n",
    "# The agent will likely fail, as the first turn has been pushed out of the window.\n",
    "\n",
    "# Clean up the memory for the next demonstration.\n",
    "sliding_memory.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summarization-theory",
   "metadata": {},
   "source": [
    "### Strategy 3: Summarization Memory\n",
    "\n",
    "| **Best For**                  | **Tradeoff**                                           |\n",
    "| ----------------------------- | ------------------------------------------------------ |\n",
    "| Long conversations, general context needed           | May lose fine details                                  |\n",
    "\n",
    "**Theory:** This strategy attempts to get the best of both worlds. Instead of just dropping old messages, it uses the LLM itself to periodically create a running summary of the conversation. It maintains a temporary buffer of recent messages. Once the buffer reaches a certain size, its content is summarized and merged with the previous summary.\n",
    "\n",
    "The context sent to the LLM is a combination of the `running_summary` and the `current_buffer`. This keeps the context size manageable while retaining the gist of the entire conversation. The main risk is information loss: if the LLM's summary misses a crucial but subtle detail, that detail is lost forever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summarization-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Strategy 3: Summarization Memory ---\n",
    "# This strategy aims to manage long conversations by periodically summarizing them.\n",
    "# It keeps a buffer of recent messages. When the buffer reaches a certain size,\n",
    "# it uses an LLM call to consolidate the buffer's content with a running summary.\n",
    "# This keeps the context size manageable while retaining the gist of the conversation.\n",
    "# The main risk is information loss if the summary is not perfect.\n",
    "class SummarizationMemory(BaseMemoryStrategy):\n",
    "    def __init__(self, summary_threshold: int = 4): # Default: Summarize after 4 messages (2 turns)\n",
    "        \"\"\"\n",
    "        Initializes the summarization memory.\n",
    "        \n",
    "        Args:\n",
    "            summary_threshold: The number of messages (user + AI) to accumulate in the\n",
    "                             buffer before triggering a summarization.\n",
    "        \"\"\"\n",
    "        # Stores the continuously updated summary of the conversation so far.\n",
    "        self.running_summary = \"\"\n",
    "        # A temporary list to hold recent messages before they are summarized.\n",
    "        self.buffer = []\n",
    "        # The threshold that triggers the summarization process.\n",
    "        self.summary_threshold = summary_threshold\n",
    "\n",
    "    def add_message(self, user_input: str, ai_response: str):\n",
    "        \"\"\"\n",
    "        Adds a new user-AI interaction to the buffer. If the buffer size\n",
    "        reaches the threshold, it triggers the memory consolidation process.\n",
    "        \"\"\"\n",
    "        # Append the latest user and AI messages to the temporary buffer.\n",
    "        self.buffer.append({\"role\": \"user\", \"content\": user_input})\n",
    "        self.buffer.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "\n",
    "        # Check if the buffer has reached its capacity.\n",
    "        if len(self.buffer) >= self.summary_threshold:\n",
    "            # If so, call the method to summarize the buffer's contents.\n",
    "            self._consolidate_memory()\n",
    "\n",
    "    def _consolidate_memory(self):\n",
    "        \"\"\"\n",
    "        Uses the LLM to summarize the contents of the buffer and merge it\n",
    "        with the existing running summary.\n",
    "        \"\"\"\n",
    "        print(\"\\n--- [Memory Consolidation Triggered] ---\")\n",
    "        # Convert the list of buffered messages into a single formatted string.\n",
    "        buffer_text = \"\\n\".join([f\"{msg['role'].capitalize()}: {msg['content']}\" for msg in self.buffer])\n",
    "        \n",
    "        # Construct a specific prompt for the LLM to perform the summarization task.\n",
    "        # It provides the existing summary and the new conversation text, asking for\n",
    "        # a single, updated summary.\n",
    "        summarization_prompt = (\n",
    "            f\"You are a summarization expert. Your task is to create a concise summary of a conversation. \"\n",
    "            f\"Combine the 'Previous Summary' with the 'New Conversation' into a single, updated summary. \"\n",
    "            f\"Capture all key facts, names, and decisions.\\n\\n\"\n",
    "            f\"### Previous Summary:\\n{self.running_summary}\\n\\n\"\n",
    "            f\"### New Conversation:\\n{buffer_text}\\n\\n\"\n",
    "            f\"### Updated Summary:\"\n",
    "        )\n",
    "        \n",
    "        # Call the LLM with a specific system prompt to get the new summary.\n",
    "        new_summary = generate_text(\"You are an expert summarization engine.\", summarization_prompt)\n",
    "        # Replace the old summary with the newly generated, consolidated one.\n",
    "        self.running_summary = new_summary\n",
    "        # Clear the buffer, as its contents have now been incorporated into the summary.\n",
    "        self.buffer = [] \n",
    "        print(f\"--- [New Summary: '{self.running_summary}'] ---\")\n",
    "\n",
    "    def get_context(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Constructs the context to be sent to the LLM. It combines the long-term\n",
    "        running summary with the short-term buffer of recent messages.\n",
    "        The 'query' parameter is ignored as this strategy provides a general context.\n",
    "        \"\"\"\n",
    "        # Format the messages currently in the buffer.\n",
    "        buffer_text = \"\\n\".join([f\"{msg['role'].capitalize()}: {msg['content']}\" for msg in self.buffer])\n",
    "        # Return a combined context of the historical summary and the most recent, not-yet-summarized messages.\n",
    "        return f\"### Summary of Past Conversation:\\n{self.running_summary}\\n\\n### Recent Messages:\\n{buffer_text}\"\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"Resets the memory by clearing the summary and the buffer.\"\"\"\n",
    "        self.running_summary = \"\"\n",
    "        self.buffer = []\n",
    "        print(\"Summarization memory cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summarization-demo-theory",
   "metadata": {},
   "source": [
    "#### Demonstration of Summarization Memory\n",
    "\n",
    "**What to watch for:** A `[Memory Consolidation Triggered]` message will appear after the second turn (since our threshold is 4 messages). The context for the subsequent turn will include the new, AI-generated summary. We'll see if the agent can recall details from the first turn, which now only exist in the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summarization-demo-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent initialized with SummarizationMemory.\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > I'm starting a new company called 'Innovatech'. Our focus is on sustainable energy.\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.0000 seconds\n",
      "Estimated Prompt Tokens: 45\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "### Summary of Past Conversation:\n",
      "\n",
      "\n",
      "### Recent Messages:\n",
      "\n",
      "\n",
      "### CURRENT REQUEST\n",
      "I'm starting a new company called 'Innovatech'. Our focus is on sustainable energy.\n",
      "---\n",
      "\n",
      "Agent > Congratulations on starting your new company, Innovatech! Focusing on sustainable energy is a fantastic direction to take. It's a rapidly growing field with immense potential to make a positive impact on the environment.\n",
      "\n",
      "To get started, can you tell me a bit more about Innovatech? What specific areas of sustainable energy are you interested in exploring, such as:\n",
      "\n",
      "1. Renewable energy sources (e.g., solar, wind, hydro)?\n",
      "2. Energy storage and grid stability?\n",
      "3. Green building and architecture?\n",
      "4. Electric vehicle technology?\n",
      "5. Something else?\n",
      "\n",
      "Sharing your goals and interests will help me provide more tailored guidance and support as you navigate the development of Innovatech.\n",
      "(LLM Generation Time: 2.5576 seconds)\n",
      "======================================================================\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > Our first product will be a smart solar panel, codenamed 'Project Helios'.\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.0028 seconds\n",
      "Estimated Prompt Tokens: 204\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "### Summary of Past Conversation:\n",
      "\n",
      "\n",
      "### Recent Messages:\n",
      "User: I'm starting a new company called 'Innovatech'. Our focus is on sustainable energy.\n",
      "Assistant: Congratulations on starting your new company, Innovatech! Focusing on sustainable energy is a fantastic direction to take. It's a rapidly growing field with immense potential to make a positive impact on the environment.\n",
      "\n",
      "To get started, can you tell me a bit more about Innovatech? What specific areas of sustainable energy are you interested in exploring, such as:\n",
      "\n",
      "1. Renewable energy sources (e.g., solar, wind, hydro)?\n",
      "2. Energy storage and grid stability?\n",
      "3. Green building and architecture?\n",
      "4. Electric vehicle technology?\n",
      "5. Something else?\n",
      "\n",
      "Sharing your goals and interests will help me provide more tailored guidance and support as you navigate the development of Innovatech.\n",
      "\n",
      "### CURRENT REQUEST\n",
      "Our first product will be a smart solar panel, codenamed 'Project Helios'.\n",
      "---\n",
      "\n",
      "--- [Memory Consolidation Triggered] ---\n",
      "--- [New Summary: 'Here is the updated summary:\n",
      "\n",
      "### Previous Summary:\n",
      "\n",
      "Innovatech is a new company started by the user, focused on sustainable energy. The company aims to explore various areas including:\n",
      "\n",
      "* Renewable energy sources (e.g., solar, wind, hydro)\n",
      "* Energy storage and grid stability\n",
      "* Green building and architecture\n",
      "* Electric vehicle technology\n",
      "\n",
      "### Updated Summary:\n",
      "\n",
      "**Innovatech: A Sustainable Energy Company**\n",
      "\n",
      "Innovatech, a new company founded by the user, is committed to developing innovative solutions for sustainable energy. The company's mission is focused on:\n",
      "\n",
      "* Exploring renewable energy sources, including solar, wind, and hydro\n",
      "* Improving energy storage and grid stability\n",
      "* Developing green building and architecture technologies\n",
      "* Enhancing electric vehicle technology\n",
      "\n",
      "**Key Initiative:** The company's first product, codenamed 'Project Helios', is a smart solar panel designed for real-time energy monitoring and automatic panel adjustments. The specific features, manufacturing plans, and production partnerships for 'Project Helios' are still being discussed.\n",
      "\n",
      "**Goals and Vision:** Aligning with Innovatech's mission, 'Project Helios' aims to contribute to the development of a sustainable future, with specific metrics and milestones to be determined.\n",
      "\n",
      "**Recent Updates:**\n",
      "\n",
      "* 'Project Helios' is the company's first product, a smart solar panel.\n",
      "* The features, manufacturing plans, and production partnerships for 'Project Helios' are still being discussed.\n",
      "\n",
      "This updated summary captures the key facts, names, and decisions from the conversation, including Innovatech's mission, 'Project Helios', and the company's areas of focus.'] ---\n",
      "\n",
      "Agent > ### MEMORY UPDATE\n",
      "### New Information to Recall for Future Conversations:\n",
      "\n",
      "\n",
      "### Recent Conversation History:\n",
      "- User started a new company called 'Innovatech' focused on sustainable energy, exploring various areas including renewable energy, energy storage, green building, and electric vehicle technology.\n",
      "- The company's first product is a smart solar panel codenamed 'Project Helios'.\n",
      "\n",
      "### CURRENT RESPONSE\n",
      "That's exciting news about 'Project Helios'! A smart solar panel is a wonderful innovation in the field of renewable energy. To further discuss 'Project Helios' and provide more tailored guidance, could you tell me:\n",
      "\n",
      "1. What specific features or functionalities do you envision for 'Project Helios', such as real-time energy monitoring, automatic panel adjustments for peak energy production, or integration with the smart grid?\n",
      "2. What are your plans for manufacturing and production of 'Project Helios', including potential partnerships with existing suppliers and manufacturers in the solar panel industry?\n",
      "3. How does 'Project Helios' align with Innovatech's overall mission and vision for a sustainable future? Are there any key metrics or milestones you hope to achieve with this product?\n",
      "(LLM Generation Time: 3.5807 seconds)\n",
      "======================================================================\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > The marketing budget is set at $50,000.\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.0000 seconds\n",
      "Estimated Prompt Tokens: 364\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "### Summary of Past Conversation:\n",
      "Here is the updated summary:\n",
      "\n",
      "### Previous Summary:\n",
      "\n",
      "Innovatech is a new company started by the user, focused on sustainable energy. The company aims to explore various areas including:\n",
      "\n",
      "* Renewable energy sources (e.g., solar, wind, hydro)\n",
      "* Energy storage and grid stability\n",
      "* Green building and architecture\n",
      "* Electric vehicle technology\n",
      "\n",
      "### Updated Summary:\n",
      "\n",
      "**Innovatech: A Sustainable Energy Company**\n",
      "\n",
      "Innovatech, a new company founded by the user, is committed to developing innovative solutions for sustainable energy. The company's mission is focused on:\n",
      "\n",
      "* Exploring renewable energy sources, including solar, wind, and hydro\n",
      "* Improving energy storage and grid stability\n",
      "* Developing green building and architecture technologies\n",
      "* Enhancing electric vehicle technology\n",
      "\n",
      "**Key Initiative:** The company's first product, codenamed 'Project Helios', is a smart solar panel designed for real-time energy monitoring and automatic panel adjustments. The specific features, manufacturing plans, and production partnerships for 'Project Helios' are still being discussed.\n",
      "\n",
      "**Goals and Vision:** Aligning with Innovatech's mission, 'Project Helios' aims to contribute to the development of a sustainable future, with specific metrics and milestones to be determined.\n",
      "\n",
      "**Recent Updates:**\n",
      "\n",
      "* 'Project Helios' is the company's first product, a smart solar panel.\n",
      "* The features, manufacturing plans, and production partnerships for 'Project Helios' are still being discussed.\n",
      "\n",
      "This updated summary captures the key facts, names, and decisions from the conversation, including Innovatech's mission, 'Project Helios', and the company's areas of focus.\n",
      "\n",
      "### Recent Messages:\n",
      "\n",
      "\n",
      "### CURRENT REQUEST\n",
      "The marketing budget is set at $50,000.\n",
      "---\n",
      "\n",
      "Agent > To review, Innovatech's marketing budget is $50,000. With this budget, we can explore various marketing strategies to promote 'Project Helios' and the company's mission. Here are a few ideas to get started:\n",
      "\n",
      "1.  **Social Media Campaign:** Utilize social media platforms such as Instagram, Facebook, and Twitter to create engaging content and promotions. This could include influencer partnerships, sponsored posts, and targeted ads. Estimated budget allocation: $15,000.\n",
      "\n",
      "2.  **Content Marketing:** Develop informative content (blog posts, videos, guides) that highlights the benefits of sustainable energy and 'Project Helios'. This will help establish Innovatech as a thought leader in the industry and drive organic traffic to the website. Estimated budget allocation: $10,000.\n",
      "\n",
      "3.  **Event Marketing:** Host or participate in events related to sustainability and energy, such as conferences, trade shows, or workshops. This will provide opportunities for networking, product demonstrations, and lead generation. Estimated budget allocation: $5,000.\n",
      "\n",
      "4.  **Email Marketing:** Build an email list and create regular newsletters that share updates on 'Project Helios', industry news, and company achievements. Estimated budget allocation: $5,000.\n",
      "\n",
      "5.  **Public Relations:** Leverage media coverage by crafting a compelling press release and pitching story ideas to media outlets. This will help increase visibility and credibility for Innovatech. Estimated budget allocation: $10,000.\n",
      "\n",
      "6.  **Paid Advertising:** Run targeted ads on Google Ads, LinkedIn, and other platforms to reach potential customers and drive traffic to the website. Estimated budget allocation: $10,000.\n",
      "\n",
      "These are just a few ideas to get started. We can allocate the remaining budget to explore other marketing channels or strategies.\n",
      "\n",
      "Would you like to discuss any of these ideas or explore other options?\n",
      "(LLM Generation Time: 5.7128 seconds)\n",
      "======================================================================\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > What is the name of my company and its first product?\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.0000 seconds\n",
      "Estimated Prompt Tokens: 754\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "### Summary of Past Conversation:\n",
      "Here is the updated summary:\n",
      "\n",
      "### Previous Summary:\n",
      "\n",
      "Innovatech is a new company started by the user, focused on sustainable energy. The company aims to explore various areas including:\n",
      "\n",
      "* Renewable energy sources (e.g., solar, wind, hydro)\n",
      "* Energy storage and grid stability\n",
      "* Green building and architecture\n",
      "* Electric vehicle technology\n",
      "\n",
      "### Updated Summary:\n",
      "\n",
      "**Innovatech: A Sustainable Energy Company**\n",
      "\n",
      "Innovatech, a new company founded by the user, is committed to developing innovative solutions for sustainable energy. The company's mission is focused on:\n",
      "\n",
      "* Exploring renewable energy sources, including solar, wind, and hydro\n",
      "* Improving energy storage and grid stability\n",
      "* Developing green building and architecture technologies\n",
      "* Enhancing electric vehicle technology\n",
      "\n",
      "**Key Initiative:** The company's first product, codenamed 'Project Helios', is a smart solar panel designed for real-time energy monitoring and automatic panel adjustments. The specific features, manufacturing plans, and production partnerships for 'Project Helios' are still being discussed.\n",
      "\n",
      "**Goals and Vision:** Aligning with Innovatech's mission, 'Project Helios' aims to contribute to the development of a sustainable future, with specific metrics and milestones to be determined.\n",
      "\n",
      "**Recent Updates:**\n",
      "\n",
      "* 'Project Helios' is the company's first product, a smart solar panel.\n",
      "* The features, manufacturing plans, and production partnerships for 'Project Helios' are still being discussed.\n",
      "\n",
      "This updated summary captures the key facts, names, and decisions from the conversation, including Innovatech's mission, 'Project Helios', and the company's areas of focus.\n",
      "\n",
      "### Recent Messages:\n",
      "User: The marketing budget is set at $50,000.\n",
      "Assistant: To review, Innovatech's marketing budget is $50,000. With this budget, we can explore various marketing strategies to promote 'Project Helios' and the company's mission. Here are a few ideas to get started:\n",
      "\n",
      "1.  **Social Media Campaign:** Utilize social media platforms such as Instagram, Facebook, and Twitter to create engaging content and promotions. This could include influencer partnerships, sponsored posts, and targeted ads. Estimated budget allocation: $15,000.\n",
      "\n",
      "2.  **Content Marketing:** Develop informative content (blog posts, videos, guides) that highlights the benefits of sustainable energy and 'Project Helios'. This will help establish Innovatech as a thought leader in the industry and drive organic traffic to the website. Estimated budget allocation: $10,000.\n",
      "\n",
      "3.  **Event Marketing:** Host or participate in events related to sustainability and energy, such as conferences, trade shows, or workshops. This will provide opportunities for networking, product demonstrations, and lead generation. Estimated budget allocation: $5,000.\n",
      "\n",
      "4.  **Email Marketing:** Build an email list and create regular newsletters that share updates on 'Project Helios', industry news, and company achievements. Estimated budget allocation: $5,000.\n",
      "\n",
      "5.  **Public Relations:** Leverage media coverage by crafting a compelling press release and pitching story ideas to media outlets. This will help increase visibility and credibility for Innovatech. Estimated budget allocation: $10,000.\n",
      "\n",
      "6.  **Paid Advertising:** Run targeted ads on Google Ads, LinkedIn, and other platforms to reach potential customers and drive traffic to the website. Estimated budget allocation: $10,000.\n",
      "\n",
      "These are just a few ideas to get started. We can allocate the remaining budget to explore other marketing channels or strategies.\n",
      "\n",
      "Would you like to discuss any of these ideas or explore other options?\n",
      "\n",
      "### CURRENT REQUEST\n",
      "What is the name of my company and its first product?\n",
      "---\n",
      "\n",
      "--- [Memory Consolidation Triggered] ---\n",
      "--- [New Summary: 'Here is the updated summary:\n",
      "\n",
      "**Innovatech: A Sustainable Energy Company**\n",
      "\n",
      "Innovatech, a new company founded by the user, is committed to developing innovative solutions for sustainable energy. The company's mission is focused on:\n",
      "\n",
      "* Exploring renewable energy sources, including solar, wind, and hydro\n",
      "* Improving energy storage and grid stability\n",
      "* Developing green building and architecture technologies\n",
      "* Enhancing electric vehicle technology\n",
      "\n",
      "**Key Initiative:** The company's first product, codenamed 'Project Helios', is a smart solar panel designed for real-time energy monitoring and automatic panel adjustments.\n",
      "\n",
      "**Goals and Vision:** Aligning with Innovatech's mission, 'Project Helios' aims to contribute to the development of a sustainable future, with specific metrics and milestones to be determined.\n",
      "\n",
      "**Marketing Strategy:** With a marketing budget of $50,000, Innovatech plans to implement the following marketing channels:\n",
      "\n",
      "1.  Social Media Campaign: Utilize Instagram, Facebook, and Twitter to create engaging content and promotions. ($15,000)\n",
      "2.  Content Marketing: Develop informative content to highlight the benefits of sustainable energy and 'Project Helios'. ($10,000)\n",
      "3.  Event Marketing: Host or participate in events related to sustainability and energy to network and demonstrate products. ($5,000)\n",
      "4.  Email Marketing: Build an email list and create regular newsletters sharing updates on 'Project Helios', industry news, and company achievements. ($5,000)\n",
      "5.  Public Relations: Craft a compelling press release and pitch story ideas to media outlets to increase visibility and credibility. ($10,000)\n",
      "6.  Paid Advertising: Run targeted ads on Google Ads, LinkedIn, and other platforms to reach potential customers and drive traffic to the website. ($10,000)\n",
      "\n",
      "**Recent Updates:**\n",
      "\n",
      "*   The marketing budget for Innovatech is $50,000.\n",
      "*   The features, manufacturing plans, and production partnerships for 'Project Helios' are still being discussed.\n",
      "*   Innovatech's first product is codenamed \"Project Helios\".'] ---\n",
      "\n",
      "Agent > Innovatech's first product is codenamed \"Project Helios\"\n",
      "(LLM Generation Time: 0.5062 seconds)\n",
      "======================================================================\n",
      "Summarization memory cleared.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the SummarizationMemory with a threshold of 4 messages (2 turns).\n",
    "# This means a summary will be generated after the second full interaction.\n",
    "summarization_memory = SummarizationMemory(summary_threshold=4)\n",
    "# Create an AIAgent and inject the summarization memory strategy.\n",
    "agent = AIAgent(memory_strategy=summarization_memory)\n",
    "\n",
    "# --- Start the conversation ---\n",
    "# First turn: The user provides initial details.\n",
    "agent.chat(\"I'm starting a new company called 'Innovatech'. Our focus is on sustainable energy.\")\n",
    "# Second turn: The user gives more specific information. After the AI responds to this,\n",
    "# the buffer will contain 4 messages, triggering the memory consolidation process.\n",
    "agent.chat(\"Our first product will be a smart solar panel, codenamed 'Project Helios'.\")\n",
    "\n",
    "# Third turn: The user adds another detail. The previous information now exists only in the running summary.\n",
    "agent.chat(\"The marketing budget is set at $50,000.\")\n",
    "# Fourth turn: The user tests the agent's memory. The context sent to the LLM will consist of\n",
    "# the AI-generated summary plus the most recent (post-summary) message about the budget.\n",
    "agent.chat(\"What is the name of my company and its first product?\")\n",
    "# The agent's ability to answer correctly depends entirely on the quality of the LLM's summary.\n",
    "\n",
    "# Clean up the memory for the next demonstration.\n",
    "summarization_memory.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retrieval-theory",
   "metadata": {},
   "source": [
    "### Strategy 4: Retrieval-Based Memory\n",
    "\n",
    "| **Best For**                  | **Tradeoff**                                           |\n",
    "| ----------------------------- | ------------------------------------------------------ |\n",
    "| Long-term recall, precision needed                   | Harder to implement, needs vector DB + ranking         |\n",
    "\n",
    "**Theory:** This is a powerful and widely-used strategy, forming the basis of Retrieval-Augmented Generation (RAG). Instead of storing conversation history linearly, each piece of information (e.g., a conversational turn) is treated as a document in a searchable database. When the user asks a new question, the system:\n",
    "1.  Converts the user's query into a numerical vector (an embedding).\n",
    "2.  Searches the database to find the `k` most semantically similar document embeddings.\n",
    "3.  Retrieves the original text of these documents.\n",
    "4.  Injects this retrieved text into the LLM's context.\n",
    "\n",
    "This allows the agent to pull in relevant information from any point in the past, no matter how long ago. We use `faiss` to create an efficient, in-memory vector index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retrieval-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for numerical operations and similarity search.\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# --- Strategy 4: Retrieval-Based Memory ---\n",
    "# This strategy treats each piece of conversation as a document in a searchable\n",
    "# database. It uses vector embeddings to find and retrieve the most semantically\n",
    "# relevant pieces of information from the past to answer a new query. This is the\n",
    "# core concept behind Retrieval-Augmented Generation (RAG).\n",
    "class RetrievalMemory(BaseMemoryStrategy):\n",
    "    def __init__(self, k: int = 2, embedding_dim: int = 3584):\n",
    "        \"\"\"\n",
    "        Initializes the retrieval memory system.\n",
    "        \n",
    "        Args:\n",
    "            k: The number of top relevant documents to retrieve for a given query.\n",
    "            embedding_dim: The dimension of the vectors generated by the embedding model.\n",
    "                           For BAAI/bge-multilingual-gemma2, this is 3584.\n",
    "        \"\"\"\n",
    "        # The number of nearest neighbors to retrieve.\n",
    "        self.k = k\n",
    "        # The dimensionality of the embedding vectors. Must match the model's output.\n",
    "        self.embedding_dim = embedding_dim\n",
    "        # A list to store the original text content of each document.\n",
    "        self.documents = []\n",
    "        # Initialize a FAISS index. IndexFlatL2 performs an exhaustive search using\n",
    "        # L2 (Euclidean) distance, which is effective for a moderate number of vectors.\n",
    "        self.index = faiss.IndexFlatL2(self.embedding_dim)\n",
    "\n",
    "    def add_message(self, user_input: str, ai_response: str):\n",
    "        \"\"\"\n",
    "        Adds a new conversational turn to the memory. Each part of the turn (user\n",
    "        input and AI response) is embedded and indexed separately for granular retrieval.\n",
    "        \"\"\"\n",
    "        # We store each part of the turn as a separate document to allow for more\n",
    "        # precise matching. For example, a query might be similar to a past user\n",
    "        # statement but not the AI's response in that same turn.\n",
    "        docs_to_add = [\n",
    "            f\"User said: {user_input}\",\n",
    "            f\"AI responded: {ai_response}\"\n",
    "        ]\n",
    "        for doc in docs_to_add:\n",
    "            # Generate a numerical vector representation of the document.\n",
    "            embedding = generate_embedding(doc)\n",
    "            # Proceed only if the embedding was successfully created.\n",
    "            if embedding:\n",
    "                # Store the original text. The index of this document will correspond\n",
    "                # to the index of its vector in the FAISS index.\n",
    "                self.documents.append(doc)\n",
    "                # FAISS requires the input vectors to be a 2D numpy array of float32.\n",
    "                vector = np.array([embedding], dtype='float32')\n",
    "                # Add the vector to the FAISS index, making it searchable.\n",
    "                self.index.add(vector)\n",
    "\n",
    "    def get_context(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Finds the k most relevant documents from memory based on semantic\n",
    "        similarity to the user's query.\n",
    "        \"\"\"\n",
    "        # If the index has no vectors, there's nothing to search.\n",
    "        if self.index.ntotal == 0:\n",
    "            return \"No information in memory yet.\"\n",
    "        \n",
    "        # Convert the user's query into an embedding vector.\n",
    "        query_embedding = generate_embedding(query)\n",
    "        if not query_embedding:\n",
    "            return \"Could not process query for retrieval.\"\n",
    "        \n",
    "        # Convert the query embedding into the format required by FAISS.\n",
    "        query_vector = np.array([query_embedding], dtype='float32')\n",
    "        \n",
    "        # Perform the search. 'search' returns the distances and the indices\n",
    "        # of the k nearest neighbors to the query vector.\n",
    "        distances, indices = self.index.search(query_vector, self.k)\n",
    "        \n",
    "        # Use the returned indices to retrieve the original text documents.\n",
    "        # We check for `i != -1` as FAISS can return -1 for invalid indices.\n",
    "        retrieved_docs = [self.documents[i] for i in indices[0] if i != -1]\n",
    "        \n",
    "        if not retrieved_docs:\n",
    "            return \"Could not find any relevant information in memory.\"\n",
    "        \n",
    "        # Format the retrieved documents into a string to be used as context.\n",
    "        return \"### Relevant Information Retrieved from Memory:\\n\" + \"\\n---\\n\".join(retrieved_docs)\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"Resets the memory completely by clearing the documents and the FAISS index.\"\"\"\n",
    "        self.documents = []\n",
    "        self.index.reset()\n",
    "        print(\"Retrieval memory (documents and FAISS index) cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retrieval-demo-theory",
   "metadata": {},
   "source": [
    "#### Demonstration of Retrieval Memory\n",
    "\n",
    "**What to watch for:** We will have a conversation about two completely different topics: a vacation plan and a software project. Then, we will ask a question about the vacation. The debug output will show that only the relevant vacation-related documents are retrieved and injected into the prompt, completely ignoring the irrelevant project talk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retrieval-demo-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent initialized with RetrievalMemory.\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > I am planning a vacation to Japan for next spring.\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.0000 seconds\n",
      "Estimated Prompt Tokens: 32\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "No information in memory yet.\n",
      "\n",
      "### CURRENT REQUEST\n",
      "I am planning a vacation to Japan for next spring.\n",
      "---\n",
      "\n",
      "Agent > Japan in the spring is a wonderful idea! You can expect mild temperatures, cherry blossoms in full bloom, and a plethora of exciting festivals and events. Here are some suggestions to consider for your trip:\n",
      "\n",
      "**When to Go:**\n",
      "Next spring would likely be late March to early April, a great time to see the cherry blossoms. However, keep in mind that the peak bloom time is around late March to early April, but it can vary depending on weather conditions.\n",
      "\n",
      "**Must-Visit Places:**\n",
      "\n",
      "1.  **Tokyo:** Explore the vibrant city's neon streets, try local cuisine, and visit famous landmarks like the Tokyo Tower and Shibuya Crossing.\n",
      "2.  **Kyoto:** Experience traditional Japan, stroll through temples like Fushimi Inari and Kiyomizu-dera, and enjoy a beautiful garden or two, like Arashiyama Bamboo Grove.\n",
      "3.  **Osaka:** Try delicious food in the Dotonbori district and visit the Osaka Castle for a glimpse into Japanese history.\n",
      "\n",
      "**Unique Experiences:**\n",
      "\n",
      "1.  **Cherry Blossom Viewing:** Enjoy the beautiful blooms at famous spots like Ueno Park in Tokyo or Maruyama Park in Kyoto.\n",
      "2.  **Japanese Festivals:** Plan your trip around festivals like the Cherry Blossom Festival (Hanami) or the Spring Festival (Matsuri) for a truly immersive experience.\n",
      "3.  **Traditional Ryokans:** Stay in traditional Japanese inns for a chance to experience authentic Japanese culture and hospitality.\n",
      "\n",
      "**Getting Around:**\n",
      "Consider purchasing a prepaid IC card like Suica or Pasmo for easy travel on Japan's efficient public transportation system.\n",
      "\n",
      "### Next Steps:\n",
      "Before you continue planning, consider what are your interests and preferences? Are you looking for adventure, history, nature, or culture?\n",
      "\n",
      "Which aspect of your trip is most important to you?\n",
      "(LLM Generation Time: 6.5558 seconds)\n",
      "======================================================================\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > For my software project, I'm using the React framework for the frontend.\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.2597 seconds\n",
      "Estimated Prompt Tokens: 432\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "### Relevant Information Retrieved from Memory:\n",
      "User said: I am planning a vacation to Japan for next spring.\n",
      "---\n",
      "AI responded: Japan in the spring is a wonderful idea! You can expect mild temperatures, cherry blossoms in full bloom, and a plethora of exciting festivals and events. Here are some suggestions to consider for your trip:\n",
      "\n",
      "**When to Go:**\n",
      "Next spring would likely be late March to early April, a great time to see the cherry blossoms. However, keep in mind that the peak bloom time is around late March to early April, but it can vary depending on weather conditions.\n",
      "\n",
      "**Must-Visit Places:**\n",
      "\n",
      "1.  **Tokyo:** Explore the vibrant city's neon streets, try local cuisine, and visit famous landmarks like the Tokyo Tower and Shibuya Crossing.\n",
      "2.  **Kyoto:** Experience traditional Japan, stroll through temples like Fushimi Inari and Kiyomizu-dera, and enjoy a beautiful garden or two, like Arashiyama Bamboo Grove.\n",
      "3.  **Osaka:** Try delicious food in the Dotonbori district and visit the Osaka Castle for a glimpse into Japanese history.\n",
      "\n",
      "**Unique Experiences:**\n",
      "\n",
      "1.  **Cherry Blossom Viewing:** Enjoy the beautiful blooms at famous spots like Ueno Park in Tokyo or Maruyama Park in Kyoto.\n",
      "2.  **Japanese Festivals:** Plan your trip around festivals like the Cherry Blossom Festival (Hanami) or the Spring Festival (Matsuri) for a truly immersive experience.\n",
      "3.  **Traditional Ryokans:** Stay in traditional Japanese inns for a chance to experience authentic Japanese culture and hospitality.\n",
      "\n",
      "**Getting Around:**\n",
      "Consider purchasing a prepaid IC card like Suica or Pasmo for easy travel on Japan's efficient public transportation system.\n",
      "\n",
      "### Next Steps:\n",
      "Before you continue planning, consider what are your interests and preferences? Are you looking for adventure, history, nature, or culture?\n",
      "\n",
      "Which aspect of your trip is most important to you?\n",
      "\n",
      "### CURRENT REQUEST\n",
      "For my software project, I'm using the React framework for the frontend.\n",
      "---\n",
      "\n",
      "Agent > It seems you're about to start planning a trip to Japan and I provided you with some suggestions. Now you'd like to discuss your software project that's using the React framework. How can I assist you with your React project today?\n",
      "(LLM Generation Time: 1.0698 seconds)\n",
      "======================================================================\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > I want to visit Tokyo and Kyoto while I'm on my trip.\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.2057 seconds\n",
      "Estimated Prompt Tokens: 431\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "### Relevant Information Retrieved from Memory:\n",
      "User said: I am planning a vacation to Japan for next spring.\n",
      "---\n",
      "AI responded: Japan in the spring is a wonderful idea! You can expect mild temperatures, cherry blossoms in full bloom, and a plethora of exciting festivals and events. Here are some suggestions to consider for your trip:\n",
      "\n",
      "**When to Go:**\n",
      "Next spring would likely be late March to early April, a great time to see the cherry blossoms. However, keep in mind that the peak bloom time is around late March to early April, but it can vary depending on weather conditions.\n",
      "\n",
      "**Must-Visit Places:**\n",
      "\n",
      "1.  **Tokyo:** Explore the vibrant city's neon streets, try local cuisine, and visit famous landmarks like the Tokyo Tower and Shibuya Crossing.\n",
      "2.  **Kyoto:** Experience traditional Japan, stroll through temples like Fushimi Inari and Kiyomizu-dera, and enjoy a beautiful garden or two, like Arashiyama Bamboo Grove.\n",
      "3.  **Osaka:** Try delicious food in the Dotonbori district and visit the Osaka Castle for a glimpse into Japanese history.\n",
      "\n",
      "**Unique Experiences:**\n",
      "\n",
      "1.  **Cherry Blossom Viewing:** Enjoy the beautiful blooms at famous spots like Ueno Park in Tokyo or Maruyama Park in Kyoto.\n",
      "2.  **Japanese Festivals:** Plan your trip around festivals like the Cherry Blossom Festival (Hanami) or the Spring Festival (Matsuri) for a truly immersive experience.\n",
      "3.  **Traditional Ryokans:** Stay in traditional Japanese inns for a chance to experience authentic Japanese culture and hospitality.\n",
      "\n",
      "**Getting Around:**\n",
      "Consider purchasing a prepaid IC card like Suica or Pasmo for easy travel on Japan's efficient public transportation system.\n",
      "\n",
      "### Next Steps:\n",
      "Before you continue planning, consider what are your interests and preferences? Are you looking for adventure, history, nature, or culture?\n",
      "\n",
      "Which aspect of your trip is most important to you?\n",
      "\n",
      "### CURRENT REQUEST\n",
      "I want to visit Tokyo and Kyoto while I'm on my trip.\n",
      "---\n",
      "\n",
      "Agent > You're interested in visiting Tokyo and Kyoto during your Japan trip next spring. That sounds like a great plan! Both cities offer a unique blend of traditional and modern experiences that you won't want to miss.\n",
      "\n",
      "**Must-See Attractions in Tokyo:**\n",
      "\n",
      "1.  **Tokyo Skytree:** The tallest tower in the world, offering breathtaking views of the city from its observation decks.\n",
      "2.  **Shinjuku Gyoen National Garden:** A beautiful park with gardens, walking paths, and picnic areas.\n",
      "3.  **Meiji Shrine:** A serene Shinto shrine dedicated to the deified spirits of Emperor Meiji and his wife, Empress Shoken.\n",
      "4.  **Tsukiji Outer Market:** A bustling marketplace for fresh seafood, sushi, and street food.\n",
      "5.  **Asakusa's Senso-ji Temple:** The oldest Buddhist temple in Japan, famous for its giant lantern and Nakamise shopping street.\n",
      "\n",
      "**Must-See Attractions in Kyoto:**\n",
      "\n",
      "1.  **Fushimi Inari Shrine:** Famous for its thousands of vermillion torii gates that form a tunnel up the mountain.\n",
      "2.  **Kinkaku-ji Temple (Golden Pavilion):** A stunning temple covered in gold leaf, reflecting beautifully in a serene pond.\n",
      "3.  **Kiyomizu-dera Temple:** A UNESCO World Heritage site, this temple offers breathtaking views of the city and a chance to stroll through the traditional streets of Kyoto.\n",
      "4.  **Arashiyama Bamboo Grove:** A serene and picturesque bamboo forest perfect for hiking and taking photos.\n",
      "5.  **Gion District:** A traditional Japanese neighborhood known for its geisha culture, traditional shops, and restaurants.\n",
      "\n",
      "**Tips for Visiting Tokyo and Kyoto:**\n",
      "\n",
      "1.  **Get familiar with the city's public transportation:** Both cities have efficient and frequent public transportation systems, but it's essential to understand how to use them to make the most of your trip.\n",
      "2.  **Learn some basic Japanese phrases:** While many Japanese people speak some English, learning basic phrases like \"konnichiwa\" (hello), \"arigatou\" (thank you), and \"sumimasen\" (excuse me) can go a long way in making your trip more enjoyable and respectful.\n",
      "3.  **Be prepared for crowds and lines:** Both Tokyo and Kyoto can get very crowded, especially during peak travel seasons, so be prepared for long lines and crowded public transportation.\n",
      "4.  **Try local cuisine:** Japan is famous for its delicious food, and both Tokyo and Kyoto offer a wide range of culinary experiences. Be sure to try some local specialties like sushi, ramen, and tempura.\n",
      "\n",
      "Now that you have a better idea of must-see attractions and tips for visiting Tokyo and Kyoto, consider what type of activities and experiences you'd like to prioritize during your trip. Are you looking for adventure, history, nature, or culture?\n",
      "(LLM Generation Time: 9.5179 seconds)\n",
      "======================================================================\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > The backend of my project will be built with Django.\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.2235 seconds\n",
      "Estimated Prompt Tokens: 102\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "### Relevant Information Retrieved from Memory:\n",
      "User said: For my software project, I'm using the React framework for the frontend.\n",
      "---\n",
      "AI responded: It seems you're about to start planning a trip to Japan and I provided you with some suggestions. Now you'd like to discuss your software project that's using the React framework. How can I assist you with your React project today?\n",
      "\n",
      "### CURRENT REQUEST\n",
      "The backend of my project will be built with Django.\n",
      "---\n",
      "\n",
      "Agent > Now that we've got our context straight, I'd be happy to help with your project.\n",
      "\n",
      "So, it sounds like you're building a project with a React frontend and a Django backend. That's a great tech stack!\n",
      "\n",
      "Can you tell me a bit more about what you need help with? Are you:\n",
      "\n",
      "* Looking for advice on architecture or design decisions?\n",
      "* Trying to set up your project infrastructure (e.g. setting up a new Django project, connecting to a database)?\n",
      "* Encountering issues with API integration between React and Django?\n",
      "* Something else entirely?\n",
      "\n",
      "Let me know, and I'll do my best to assist you!\n",
      "(LLM Generation Time: 2.1079 seconds)\n",
      "======================================================================\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > What cities am I planning to visit on my vacation?\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.2045 seconds\n",
      "Estimated Prompt Tokens: 65\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "### Relevant Information Retrieved from Memory:\n",
      "User said: I want to visit Tokyo and Kyoto while I'm on my trip.\n",
      "---\n",
      "User said: I am planning a vacation to Japan for next spring.\n",
      "\n",
      "### CURRENT REQUEST\n",
      "What cities am I planning to visit on my vacation?\n",
      "---\n",
      "\n",
      "Agent > You're planning to visit Tokyo and Kyoto while on your vacation to Japan next spring.\n",
      "(LLM Generation Time: 0.5336 seconds)\n",
      "======================================================================\n",
      "Retrieval memory (documents and FAISS index) cleared.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the RetrievalMemory with k=2, meaning it will retrieve the top 2 most relevant documents.\n",
    "retrieval_memory = RetrievalMemory(k=2)\n",
    "# Create an AIAgent and inject the retrieval memory strategy.\n",
    "agent = AIAgent(memory_strategy=retrieval_memory)\n",
    "\n",
    "# --- Start the conversation with mixed topics ---\n",
    "# First turn: Discussing a vacation plan. This will be stored as a document.\n",
    "agent.chat(\"I am planning a vacation to Japan for next spring.\")\n",
    "# Second turn: Discussing a software project. This will also be stored as a separate document.\n",
    "agent.chat(\"For my software project, I'm using the React framework for the frontend.\")\n",
    "# Third turn: More details about the vacation.\n",
    "agent.chat(\"I want to visit Tokyo and Kyoto while I'm on my trip.\")\n",
    "# Fourth turn: More details about the software project.\n",
    "agent.chat(\"The backend of my project will be built with Django.\")\n",
    "\n",
    "# --- Test the retrieval mechanism ---\n",
    "# Now, ask a question specifically about the vacation.\n",
    "# The agent will convert this query into an embedding and search the memory.\n",
    "# It should find that the documents about Japan, Tokyo, and Kyoto are semantically\n",
    "# closer to the query than the documents about React and Django.\n",
    "agent.chat(\"What cities am I planning to visit on my vacation?\")\n",
    "# The agent should retrieve the Japan/Tokyo/Kyoto info and ignore the software project info.\n",
    "\n",
    "# Clean up the memory for the next demonstration.\n",
    "retrieval_memory.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mem-aug-theory",
   "metadata": {},
   "source": [
    "### Strategy 5: Memory-Augmented Transformers (Conceptual Simulation)\n",
    "\n",
    "| **Best For**                  | **Tradeoff**                                           |\n",
    "| ----------------------------- | ------------------------------------------------------ |\n",
    "| Rich, evolving contexts over time                    | Advanced models, costlier                              |\n",
    "\n",
    "**Theory:** This is a modification to the *model architecture itself* and cannot be fully implemented at the agent level. However, we can *simulate its behavior*. The core idea is that the model has access to a special, compressed memory space (like \"sticky notes\") in addition to its normal context window. It learns to write key information to these memory slots and read from them when needed.\n",
    "\n",
    "**Our Simulation:** We will create a `MemoryAugmentedMemory` class. After each turn, it will use the LLM to decide if any information is important enough to be a \"key memory.\" If so, it will create a concise summary of that fact and store it in a special list of `memory_tokens`. The final context will be a combination of a sliding window of recent chat and these important `memory_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mem-aug-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Strategy 5: Memory-Augmented Memory (Simulation) ---\n",
    "# This strategy simulates the behavior of a Memory-Augmented Transformer model.\n",
    "# It maintains a short-term sliding window of recent conversation and a separate\n",
    "# list of \"memory tokens\" which are important facts extracted from the conversation.\n",
    "# An LLM call is used to decide if a piece of information is important enough\n",
    "# to be converted into a persistent memory token.\n",
    "class MemoryAugmentedMemory(BaseMemoryStrategy):\n",
    "    def __init__(self, window_size: int = 2):\n",
    "        \"\"\"\n",
    "        Initializes the memory-augmented system.\n",
    "        \n",
    "        Args:\n",
    "            window_size: The number of recent turns to keep in the short-term memory.\n",
    "        \"\"\"\n",
    "        # Use a SlidingWindowMemory instance to manage the recent conversation history.\n",
    "        self.recent_memory = SlidingWindowMemory(window_size=window_size)\n",
    "        # A list to store the special, persistent \"sticky notes\" or key facts.\n",
    "        self.memory_tokens = []\n",
    "\n",
    "    def add_message(self, user_input: str, ai_response: str):\n",
    "        \"\"\"\n",
    "        Adds the latest turn to recent memory and then uses an LLM call to decide\n",
    "        if a new, persistent memory token should be created from this interaction.\n",
    "        \"\"\"\n",
    "        # First, add the new interaction to the short-term sliding window memory.\n",
    "        self.recent_memory.add_message(user_input, ai_response)\n",
    "        \n",
    "        # Construct a prompt for the LLM to analyze the conversation turn and\n",
    "        # determine if it contains a core fact worth remembering long-term.\n",
    "        fact_extraction_prompt = (\n",
    "            f\"Analyze the following conversation turn. Does it contain a core fact, preference, or decision that should be remembered long-term? \"\n",
    "            f\"Examples include user preferences ('I hate flying'), key decisions ('The budget is $1000'), or important facts ('My user ID is 12345').\\n\\n\"\n",
    "            f\"Conversation Turn:\\nUser: {user_input}\\nAI: {ai_response}\\n\\n\"\n",
    "            f\"If it contains such a fact, state the fact concisely in one sentence. Otherwise, respond with 'No important fact.'\"\n",
    "        )\n",
    "        \n",
    "        # Call the LLM to perform the fact extraction.\n",
    "        extracted_fact = generate_text(\"You are a fact-extraction expert.\", fact_extraction_prompt)\n",
    "        \n",
    "        # Check if the LLM's response indicates that an important fact was found.\n",
    "        if \"no important fact\" not in extracted_fact.lower():\n",
    "            # If a fact was found, print a debug message and add it to our list of memory tokens.\n",
    "            print(f\"--- [Memory Augmentation: New memory token created: '{extracted_fact}'] ---\")\n",
    "            self.memory_tokens.append(extracted_fact)\n",
    "\n",
    "    def get_context(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Constructs the context by combining the short-term recent conversation\n",
    "        with the list of all long-term, persistent memory tokens.\n",
    "        \"\"\"\n",
    "        # Get the context from the short-term sliding window.\n",
    "        recent_context = self.recent_memory.get_context(query)\n",
    "        # Format the list of memory tokens into a readable string.\n",
    "        memory_token_context = \"\\n\".join([f\"- {token}\" for token in self.memory_tokens])\n",
    "        \n",
    "        # Return the combined context, clearly separating the long-term facts from the recent chat.\n",
    "        return f\"### Key Memory Tokens (Long-Term Facts):\\n{memory_token_context}\\n\\n### Recent Conversation:\\n{recent_context}\"\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"Resets both the short-term memory and the list of memory tokens.\"\"\"\n",
    "        self.recent_memory.clear()\n",
    "        self.memory_tokens = []\n",
    "        print(\"Memory-Augmented memory cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mem-aug-demo-theory",
   "metadata": {},
   "source": [
    "#### Demonstration of Memory-Augmented Memory\n",
    "\n",
    "**What to watch for:** We'll mention a critical, long-term preference in the first turn. The agent should identify this as a \"key memory\" and create a memory token. After several more turns push the original message out of the recent chat window, the agent should still be able to recall the preference by reading its memory token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mem-aug-demo-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent initialized with MemoryAugmentedMemory.\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > Please remember this for all future interactions: I am severely allergic to peanuts.\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.0000 seconds\n",
      "Estimated Prompt Tokens: 45\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "### Key Memory Tokens (Long-Term Facts):\n",
      "\n",
      "\n",
      "### Recent Conversation:\n",
      "\n",
      "\n",
      "### CURRENT REQUEST\n",
      "Please remember this for all future interactions: I am severely allergic to peanuts.\n",
      "---\n",
      "--- [Memory Augmentation: New memory token created: 'You are severely allergic to peanuts.'] ---\n",
      "\n",
      "Agent > **MEMORY UPDATE**\n",
      "\n",
      "I have taken note of your long-term fact: You are severely allergic to peanuts. I will keep this in mind for all future interactions to ensure your allergy is considered when providing information or suggestions.\n",
      "(LLM Generation Time: 1.3206 seconds)\n",
      "======================================================================\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > Okay, let's talk about recipes. What's a good idea for dinner tonight?\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.0000 seconds\n",
      "Estimated Prompt Tokens: 116\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "### Key Memory Tokens (Long-Term Facts):\n",
      "- You are severely allergic to peanuts.\n",
      "\n",
      "### Recent Conversation:\n",
      "User: Please remember this for all future interactions: I am severely allergic to peanuts.\n",
      "Assistant: **MEMORY UPDATE**\n",
      "\n",
      "I have taken note of your long-term fact: You are severely allergic to peanuts. I will keep this in mind for all future interactions to ensure your allergy is considered when providing information or suggestions.\n",
      "\n",
      "### CURRENT REQUEST\n",
      "Okay, let's talk about recipes. What's a good idea for dinner tonight?\n",
      "---\n",
      "--- [Memory Augmentation: New memory token created: 'The conversation turn contains the fact that the user has a peanut allergy. \n",
      "\n",
      "Fact: The user has a peanut allergy.'] ---\n",
      "\n",
      "Agent > Given your peanut allergy, I'll make sure to exclude peanuts from my dinner suggestions.\n",
      "\n",
      "Considering a variety of cuisines, here are a few ideas:\n",
      "\n",
      "1. **Grilled Salmon with Roasted Vegetables**: A simple and flavorful option that's peanut-free. Salmon provides a good dose of omega-3 fatty acids, and you can pair it with your favorite vegetables, such as asparagus, bell peppers, or zucchini.\n",
      "2. **Chicken Fajitas**: This classic Tex-Mex dish is easy to make and can be adapted to be peanut-free. Simply marinate chicken breast in a mixture of lime juice, olive oil, and spices, then saut it with sliced onions and bell peppers. Serve with warm flour or corn tortillas.\n",
      "3. **Baked Cod with Lemon and Herbs**: A healthy and light option that's perfect for a weeknight dinner. Season cod fillets with lemon juice, olive oil, and herbs like parsley, basil, or thyme, then bake in the oven until flaky and tender.\n",
      "4. **Veggie Stir-Fry with Tofu**: If you're in the mood for something more adventurous, try a stir-fry with your favorite vegetables, such as broccoli, carrots, and bell peppers. Add cubed tofu for a protein-rich and peanut-free option.\n",
      "\n",
      "Which of these ideas sounds appealing to you? Or do you have any dietary preferences or restrictions I should consider?\n",
      "(LLM Generation Time: 4.5884 seconds)\n",
      "======================================================================\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > That sounds good. What about a dessert option?\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.0000 seconds\n",
      "Estimated Prompt Tokens: 446\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "### Key Memory Tokens (Long-Term Facts):\n",
      "- You are severely allergic to peanuts.\n",
      "- The conversation turn contains the fact that the user has a peanut allergy. \n",
      "\n",
      "Fact: The user has a peanut allergy.\n",
      "\n",
      "### Recent Conversation:\n",
      "User: Please remember this for all future interactions: I am severely allergic to peanuts.\n",
      "Assistant: **MEMORY UPDATE**\n",
      "\n",
      "I have taken note of your long-term fact: You are severely allergic to peanuts. I will keep this in mind for all future interactions to ensure your allergy is considered when providing information or suggestions.\n",
      "User: Okay, let's talk about recipes. What's a good idea for dinner tonight?\n",
      "Assistant: Given your peanut allergy, I'll make sure to exclude peanuts from my dinner suggestions.\n",
      "\n",
      "Considering a variety of cuisines, here are a few ideas:\n",
      "\n",
      "1. **Grilled Salmon with Roasted Vegetables**: A simple and flavorful option that's peanut-free. Salmon provides a good dose of omega-3 fatty acids, and you can pair it with your favorite vegetables, such as asparagus, bell peppers, or zucchini.\n",
      "2. **Chicken Fajitas**: This classic Tex-Mex dish is easy to make and can be adapted to be peanut-free. Simply marinate chicken breast in a mixture of lime juice, olive oil, and spices, then saut it with sliced onions and bell peppers. Serve with warm flour or corn tortillas.\n",
      "3. **Baked Cod with Lemon and Herbs**: A healthy and light option that's perfect for a weeknight dinner. Season cod fillets with lemon juice, olive oil, and herbs like parsley, basil, or thyme, then bake in the oven until flaky and tender.\n",
      "4. **Veggie Stir-Fry with Tofu**: If you're in the mood for something more adventurous, try a stir-fry with your favorite vegetables, such as broccoli, carrots, and bell peppers. Add cubed tofu for a protein-rich and peanut-free option.\n",
      "\n",
      "Which of these ideas sounds appealing to you? Or do you have any dietary preferences or restrictions I should consider?\n",
      "\n",
      "### CURRENT REQUEST\n",
      "That sounds good. What about a dessert option?\n",
      "---\n",
      "--- [Memory Augmentation: New memory token created: 'There is a key fact: The person has an allergy that necessitates peanut-free dessert options.'] ---\n",
      "\n",
      "Agent > When considering a dessert option, keep in mind that it should be peanut-free to ensure your allergy is not compromised.\n",
      "\n",
      "Here are some ideas:\n",
      "\n",
      "1.  **Fresh Fruit Tarts**: A light and refreshing dessert option that can be made with a variety of fruits, such as strawberries, blueberries, or raspberries. Use a peanut-free crust and top with a dollop of whipped cream or a sprinkle of powdered sugar.\n",
      "2.  **Chocolate Chip Cookies**: A classic dessert that is easy to make and can be peanut-free. Use a variety of chocolate chips and consider adding in nuts like almonds or walnuts for added texture and flavor, just ensure they're processed in a dedicated peanut-free facility.\n",
      "3.  **Vanilla Panna Cotta with Fresh Berries**: A creamy and sweet dessert that is perfect for warm weather. Use a vanilla-flavored cream base that is peanut-free and top with fresh berries like strawberries, blueberries, or raspberries.\n",
      "4.  **Coconut Macaroons**: A sweet and chewy dessert option that can be made with coconut flakes and is naturally peanut-free. Simply mix together coconut flakes, sweetened condensed milk, and egg whites, then drop by spoonfuls onto a baking sheet and bake until golden brown.\n",
      "\n",
      "Which of these options sounds appealing to you, or do you have another idea for a dessert that I can help guide you on to ensure it's peanut-free?\n",
      "(LLM Generation Time: 4.7084 seconds)\n",
      "======================================================================\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > Could you suggest a Thai green curry recipe? Please ensure it's safe for me.\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.0000 seconds\n",
      "Estimated Prompt Tokens: 712\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "### Key Memory Tokens (Long-Term Facts):\n",
      "- You are severely allergic to peanuts.\n",
      "- The conversation turn contains the fact that the user has a peanut allergy. \n",
      "\n",
      "Fact: The user has a peanut allergy.\n",
      "- There is a key fact: The person has an allergy that necessitates peanut-free dessert options.\n",
      "\n",
      "### Recent Conversation:\n",
      "User: Okay, let's talk about recipes. What's a good idea for dinner tonight?\n",
      "Assistant: Given your peanut allergy, I'll make sure to exclude peanuts from my dinner suggestions.\n",
      "\n",
      "Considering a variety of cuisines, here are a few ideas:\n",
      "\n",
      "1. **Grilled Salmon with Roasted Vegetables**: A simple and flavorful option that's peanut-free. Salmon provides a good dose of omega-3 fatty acids, and you can pair it with your favorite vegetables, such as asparagus, bell peppers, or zucchini.\n",
      "2. **Chicken Fajitas**: This classic Tex-Mex dish is easy to make and can be adapted to be peanut-free. Simply marinate chicken breast in a mixture of lime juice, olive oil, and spices, then saut it with sliced onions and bell peppers. Serve with warm flour or corn tortillas.\n",
      "3. **Baked Cod with Lemon and Herbs**: A healthy and light option that's perfect for a weeknight dinner. Season cod fillets with lemon juice, olive oil, and herbs like parsley, basil, or thyme, then bake in the oven until flaky and tender.\n",
      "4. **Veggie Stir-Fry with Tofu**: If you're in the mood for something more adventurous, try a stir-fry with your favorite vegetables, such as broccoli, carrots, and bell peppers. Add cubed tofu for a protein-rich and peanut-free option.\n",
      "\n",
      "Which of these ideas sounds appealing to you? Or do you have any dietary preferences or restrictions I should consider?\n",
      "User: That sounds good. What about a dessert option?\n",
      "Assistant: When considering a dessert option, keep in mind that it should be peanut-free to ensure your allergy is not compromised.\n",
      "\n",
      "Here are some ideas:\n",
      "\n",
      "1.  **Fresh Fruit Tarts**: A light and refreshing dessert option that can be made with a variety of fruits, such as strawberries, blueberries, or raspberries. Use a peanut-free crust and top with a dollop of whipped cream or a sprinkle of powdered sugar.\n",
      "2.  **Chocolate Chip Cookies**: A classic dessert that is easy to make and can be peanut-free. Use a variety of chocolate chips and consider adding in nuts like almonds or walnuts for added texture and flavor, just ensure they're processed in a dedicated peanut-free facility.\n",
      "3.  **Vanilla Panna Cotta with Fresh Berries**: A creamy and sweet dessert that is perfect for warm weather. Use a vanilla-flavored cream base that is peanut-free and top with fresh berries like strawberries, blueberries, or raspberries.\n",
      "4.  **Coconut Macaroons**: A sweet and chewy dessert option that can be made with coconut flakes and is naturally peanut-free. Simply mix together coconut flakes, sweetened condensed milk, and egg whites, then drop by spoonfuls onto a baking sheet and bake until golden brown.\n",
      "\n",
      "Which of these options sounds appealing to you, or do you have another idea for a dessert that I can help guide you on to ensure it's peanut-free?\n",
      "\n",
      "### CURRENT REQUEST\n",
      "Could you suggest a Thai green curry recipe? Please ensure it's safe for me.\n",
      "---\n",
      "--- [Memory Augmentation: New memory token created: 'This conversation turn contains a core fact that should be remembered long-term. The fact is: All the ingredients used in the suggested Thai green curry recipe must be peanut-free to cater to peanut allergy.'] ---\n",
      "\n",
      "Agent > Considering your peanut allergy, I'll suggest a Thai green curry recipe that's safe for you.\n",
      "\n",
      "Here's a peanut-free Thai green curry recipe:\n",
      "\n",
      "**Ingredients:**\n",
      "\n",
      "*   2 tablespoons of vegetable oil (make sure it's peanut-free)\n",
      "*   2 cloves of garlic, minced\n",
      "*   1 tablespoon of grated fresh ginger\n",
      "*   2 cups of mixed vegetables (such as bell peppers, carrots, and green beans)\n",
      "*   1 cup of coconut milk (make sure it's a peanut-free brand)\n",
      "*   2 tablespoons of Thai green curry paste (check the ingredients to ensure it doesn't contain peanuts)\n",
      "*   1 pound of boneless, skinless chicken breast or thighs (cut into bite-sized pieces)\n",
      "*   1 tablespoon of fish sauce\n",
      "*   1 tablespoon of lime juice\n",
      "*   1 teaspoon of palm sugar (or brown sugar)\n",
      "*   Salt and pepper to taste\n",
      "*   Fresh basil leaves for garnish\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "1.  Heat the oil in a large pan over medium-high heat. Add the garlic and ginger and saut until fragrant.\n",
      "2.  Add the mixed vegetables and cook until they're tender-crisp.\n",
      "3.  Add the coconut milk, Thai green curry paste, chicken, fish sauce, lime juice, and palm sugar. Stir well to combine.\n",
      "4.  Reduce the heat to low and simmer for 10-15 minutes or until the chicken is cooked through.\n",
      "5.  Season with salt and pepper to taste.\n",
      "6.  Garnish with fresh basil leaves and serve over rice or noodles.\n",
      "\n",
      "**Safety Precautions:**\n",
      "\n",
      "*   Always read the ingredient labels to ensure that the coconut milk, curry paste, and other ingredients are peanut-free.\n",
      "*   If you're cooking for others, make sure to use separate utensils and cooking surfaces to avoid cross-contamination with peanuts.\n",
      "\n",
      "This Thai green curry recipe is a delicious and safe option for you to enjoy.\n",
      "(LLM Generation Time: 6.4568 seconds)\n",
      "======================================================================\n",
      "Sliding window memory cleared.\n",
      "Memory-Augmented memory cleared.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the MemoryAugmentedMemory with a window size of 2.\n",
    "# This means the short-term memory will only hold the last two turns.\n",
    "mem_aug_memory = MemoryAugmentedMemory(window_size=2)\n",
    "# Create an AIAgent and inject the memory-augmented strategy.\n",
    "agent = AIAgent(memory_strategy=mem_aug_memory)\n",
    "\n",
    "# --- Start the conversation ---\n",
    "# First turn: The user provides a critical, long-term piece of information.\n",
    "# The agent's fact-extraction mechanism should identify this as important and create a memory token.\n",
    "agent.chat(\"Please remember this for all future interactions: I am severely allergic to peanuts.\")\n",
    "\n",
    "# Second turn: A standard conversational turn.\n",
    "agent.chat(\"Okay, let's talk about recipes. What's a good idea for dinner tonight?\")\n",
    "\n",
    "# Third turn: Another conversational turn. This will push the first turn (the allergy warning)\n",
    "# out of the short-term sliding window memory.\n",
    "agent.chat(\"That sounds good. What about a dessert option?\")\n",
    "\n",
    "# --- Test the memory augmentation ---\n",
    "# Now, the critical test. The original allergy warning is no longer in the recent chat context.\n",
    "# The agent's only way to know about the allergy is by accessing its long-term \"memory tokens\".\n",
    "agent.chat(\"Could you suggest a Thai green curry recipe? Please ensure it's safe for me.\")\n",
    "# A successful agent will use the persistent memory token to check for safety and likely warn\n",
    "# about peanuts, which are common in Thai cuisine.\n",
    "\n",
    "# Clean up the memory for the next demonstration.\n",
    "mem_aug_memory.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hierarchical-theory",
   "metadata": {},
   "source": [
    "### Strategy 6: Hierarchical Memory\n",
    "\n",
    "| **Best For**                  | **Tradeoff**                                           |\n",
    "| ----------------------------- | ------------------------------------------------------ |\n",
    "| Multi-task, complex agents with different info types | Sophisticated management logic                         |\n",
    "\n",
    "**Theory:** This is a composite strategy that mimics how human memory works at different levels. It combines multiple, simpler memory strategies into a hierarchy. A common setup is:\n",
    "- **Level 1 (Working Memory):** A `SlidingWindowMemory` for fast, immediate context.\n",
    "- **Level 2 (Long-Term Memory):** A `RetrievalMemory` for storing important, durable facts.\n",
    "\n",
    "The key is the logic that **promotes** information from L1 to L2. Our implementation will use a heuristic: if a conversation turn seems particularly important (e.g., contains a keyword like \"preference\" or \"rule\"), it gets added to both the working and long-term stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hierarchical-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Strategy 6: Hierarchical Memory ---\n",
    "# This strategy combines multiple memory types to create a more sophisticated,\n",
    "# layered system, mimicking human memory's division into short-term (working)\n",
    "# and long-term storage.\n",
    "class HierarchicalMemory(BaseMemoryStrategy):\n",
    "    def __init__(self, window_size: int = 2, k: int = 2, embedding_dim: int = 3584):\n",
    "        \"\"\"\n",
    "        Initializes the hierarchical memory system.\n",
    "        \n",
    "        Args:\n",
    "            window_size: The size of the short-term working memory (in turns).\n",
    "            k: The number of documents to retrieve from long-term memory.\n",
    "            embedding_dim: The dimension of the embedding vectors for long-term memory.\n",
    "        \"\"\"\n",
    "        print(\"Initializing Hierarchical Memory...\")\n",
    "        # Level 1: Fast, short-term working memory using a sliding window.\n",
    "        self.working_memory = SlidingWindowMemory(window_size=window_size)\n",
    "        # Level 2: Slower, durable long-term memory using a retrieval system.\n",
    "        self.long_term_memory = RetrievalMemory(k=k, embedding_dim=embedding_dim)\n",
    "        # A simple heuristic: keywords that trigger promotion from working to long-term memory.\n",
    "        self.promotion_keywords = [\"remember\", \"rule\", \"preference\", \"always\", \"never\", \"allergic\"]\n",
    "\n",
    "    def add_message(self, user_input: str, ai_response: str):\n",
    "        \"\"\"\n",
    "        Adds a message to working memory and conditionally promotes it to long-term\n",
    "        memory based on its content.\n",
    "        \"\"\"\n",
    "        # All interactions are added to the fast, short-term working memory.\n",
    "        self.working_memory.add_message(user_input, ai_response)\n",
    "        \n",
    "        # Promotion Logic: Check if the user's input contains a keyword that\n",
    "        # suggests the information is important and should be stored long-term.\n",
    "        if any(keyword in user_input.lower() for keyword in self.promotion_keywords):\n",
    "            print(f\"--- [Hierarchical Memory: Promoting message to long-term storage.] ---\")\n",
    "            # If a keyword is found, also add the interaction to the long-term retrieval memory.\n",
    "            self.long_term_memory.add_message(user_input, ai_response)\n",
    "\n",
    "    def get_context(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Constructs a rich context by combining relevant information from both\n",
    "        the long-term and short-term memory layers.\n",
    "        \"\"\"\n",
    "        # Retrieve the most recent conversation from the working memory.\n",
    "        working_context = self.working_memory.get_context(query)\n",
    "        # Retrieve semantically relevant facts from the long-term memory based on the current query.\n",
    "        long_term_context = self.long_term_memory.get_context(query)\n",
    "        \n",
    "        # Combine both contexts, clearly labeling their sources for the LLM.\n",
    "        return f\"### Retrieved Long-Term Memories:\\n{long_term_context}\\n\\n### Recent Conversation (Working Memory):\\n{working_context}\"\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"Resets both the working and long-term memory stores.\"\"\"\n",
    "        self.working_memory.clear()\n",
    "        self.long_term_memory.clear()\n",
    "        print(\"Hierarchical memory cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hierarchical-demo-theory",
   "metadata": {},
   "source": [
    "#### Demonstration of Hierarchical Memory\n",
    "\n",
    "**What to watch for:** We will state a preference using the keyword \"remember\". This will trigger the message to be saved in the long-term `RetrievalMemory`. After a few turns push it out of the short-term `SlidingWindowMemory`, we'll ask a related question. The agent should successfully answer by retrieving from its long-term store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hierarchical-demo-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Hierarchical Memory...\n",
      "Agent initialized with HierarchicalMemory.\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > Please remember my User ID is AX-7890.\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.0000 seconds\n",
      "Estimated Prompt Tokens: 47\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "### Retrieved Long-Term Memories:\n",
      "No information in memory yet.\n",
      "\n",
      "### Recent Conversation (Working Memory):\n",
      "\n",
      "\n",
      "### CURRENT REQUEST\n",
      "Please remember my User ID is AX-7890.\n",
      "---\n",
      "--- [Hierarchical Memory: Promoting message to long-term storage.] ---\n",
      "\n",
      "Agent > ### MEMORY CONTEXT\n",
      "### Retrieved Long-Term Memories:\n",
      "- User ID: AX-7890\n",
      "\n",
      "### Recent Conversation (Working Memory):\n",
      "- Stored request to remember the User ID AX-7890\n",
      "\n",
      "### CURRENT REQUEST\n",
      "You have provided your User ID as AX-7890, which has been stored in long-term memory for future reference. How can I assist you next?\n",
      "(LLM Generation Time: 1.7426 seconds)\n",
      "======================================================================\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > Let's chat about the weather. It's very sunny today.\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.2105 seconds\n",
      "Estimated Prompt Tokens: 232\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "### Retrieved Long-Term Memories:\n",
      "### Relevant Information Retrieved from Memory:\n",
      "AI responded: ### MEMORY CONTEXT\n",
      "### Retrieved Long-Term Memories:\n",
      "- User ID: AX-7890\n",
      "\n",
      "### Recent Conversation (Working Memory):\n",
      "- Stored request to remember the User ID AX-7890\n",
      "\n",
      "### CURRENT REQUEST\n",
      "You have provided your User ID as AX-7890, which has been stored in long-term memory for future reference. How can I assist you next?\n",
      "---\n",
      "User said: Please remember my User ID is AX-7890.\n",
      "\n",
      "### Recent Conversation (Working Memory):\n",
      "User: Please remember my User ID is AX-7890.\n",
      "Assistant: ### MEMORY CONTEXT\n",
      "### Retrieved Long-Term Memories:\n",
      "- User ID: AX-7890\n",
      "\n",
      "### Recent Conversation (Working Memory):\n",
      "- Stored request to remember the User ID AX-7890\n",
      "\n",
      "### CURRENT REQUEST\n",
      "You have provided your User ID as AX-7890, which has been stored in long-term memory for future reference. How can I assist you next?\n",
      "\n",
      "### CURRENT REQUEST\n",
      "Let's chat about the weather. It's very sunny today.\n",
      "---\n",
      "\n",
      "Agent > ### MEMORY CONTEXT\n",
      "### Retrieved Long-Term Memories:\n",
      "- User ID: AX-7890\n",
      "- Previous conversation: Stored request to remember the User ID AX-7890\n",
      "\n",
      "### Recent Conversation (Working Memory):\n",
      "- Recent topic: You requested to remember your User ID, which was successfully stored.\n",
      "- Current topic: You'd like to discuss the weather\n",
      "- Specific mention: It's very sunny today\n",
      "\n",
      "### CURRENT REQUEST\n",
      "Sounds like it's a lovely day outside! Would you like to discuss further about the weather forecast, or perhaps additional topics such as local events or activities to enjoy on a sunny day?\n",
      "(LLM Generation Time: 2.1970 seconds)\n",
      "======================================================================\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > I'm planning to go for a walk later.\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.4090 seconds\n",
      "Estimated Prompt Tokens: 369\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "### Retrieved Long-Term Memories:\n",
      "### Relevant Information Retrieved from Memory:\n",
      "User said: Please remember my User ID is AX-7890.\n",
      "---\n",
      "AI responded: ### MEMORY CONTEXT\n",
      "### Retrieved Long-Term Memories:\n",
      "- User ID: AX-7890\n",
      "\n",
      "### Recent Conversation (Working Memory):\n",
      "- Stored request to remember the User ID AX-7890\n",
      "\n",
      "### CURRENT REQUEST\n",
      "You have provided your User ID as AX-7890, which has been stored in long-term memory for future reference. How can I assist you next?\n",
      "\n",
      "### Recent Conversation (Working Memory):\n",
      "User: Please remember my User ID is AX-7890.\n",
      "Assistant: ### MEMORY CONTEXT\n",
      "### Retrieved Long-Term Memories:\n",
      "- User ID: AX-7890\n",
      "\n",
      "### Recent Conversation (Working Memory):\n",
      "- Stored request to remember the User ID AX-7890\n",
      "\n",
      "### CURRENT REQUEST\n",
      "You have provided your User ID as AX-7890, which has been stored in long-term memory for future reference. How can I assist you next?\n",
      "User: Let's chat about the weather. It's very sunny today.\n",
      "Assistant: ### MEMORY CONTEXT\n",
      "### Retrieved Long-Term Memories:\n",
      "- User ID: AX-7890\n",
      "- Previous conversation: Stored request to remember the User ID AX-7890\n",
      "\n",
      "### Recent Conversation (Working Memory):\n",
      "- Recent topic: You requested to remember your User ID, which was successfully stored.\n",
      "- Current topic: You'd like to discuss the weather\n",
      "- Specific mention: It's very sunny today\n",
      "\n",
      "### CURRENT REQUEST\n",
      "Sounds like it's a lovely day outside! Would you like to discuss further about the weather forecast, or perhaps additional topics such as local events or activities to enjoy on a sunny day?\n",
      "\n",
      "### CURRENT REQUEST\n",
      "I'm planning to go for a walk later.\n",
      "---\n",
      "\n",
      "Agent > ### MEMORY CONTEXT\n",
      "### Retrieved Long-Term Memories:\n",
      "- User ID: AX-7890\n",
      "- Previous conversation:\n",
      "  - Stored request to remember the User ID AX-7890\n",
      "  - Discussion about the weather:\n",
      "    - It's very sunny today\n",
      "\n",
      "### Recent Conversation (Working Memory):\n",
      "- Recent topic: You requested to remember your User ID, which was successfully stored.\n",
      "- Current topic: You'd like to discuss or share information about going for a walk on a sunny day\n",
      "\n",
      "### CURRENT REQUEST\n",
      "Sounds like you're feeling motivated to get outside and enjoy the weather! Are you planning to walk in your neighborhood, a local park, or a nearby trail? Or would you like some suggestions for interesting walk routes or activities to consider during your walk?\n",
      "(LLM Generation Time: 2.5532 seconds)\n",
      "======================================================================\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > I need to log into my account, can you remind me of my ID?\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.4136 seconds\n",
      "Estimated Prompt Tokens: 452\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "### Retrieved Long-Term Memories:\n",
      "### Relevant Information Retrieved from Memory:\n",
      "User said: Please remember my User ID is AX-7890.\n",
      "---\n",
      "AI responded: ### MEMORY CONTEXT\n",
      "### Retrieved Long-Term Memories:\n",
      "- User ID: AX-7890\n",
      "\n",
      "### Recent Conversation (Working Memory):\n",
      "- Stored request to remember the User ID AX-7890\n",
      "\n",
      "### CURRENT REQUEST\n",
      "You have provided your User ID as AX-7890, which has been stored in long-term memory for future reference. How can I assist you next?\n",
      "\n",
      "### Recent Conversation (Working Memory):\n",
      "User: Let's chat about the weather. It's very sunny today.\n",
      "Assistant: ### MEMORY CONTEXT\n",
      "### Retrieved Long-Term Memories:\n",
      "- User ID: AX-7890\n",
      "- Previous conversation: Stored request to remember the User ID AX-7890\n",
      "\n",
      "### Recent Conversation (Working Memory):\n",
      "- Recent topic: You requested to remember your User ID, which was successfully stored.\n",
      "- Current topic: You'd like to discuss the weather\n",
      "- Specific mention: It's very sunny today\n",
      "\n",
      "### CURRENT REQUEST\n",
      "Sounds like it's a lovely day outside! Would you like to discuss further about the weather forecast, or perhaps additional topics such as local events or activities to enjoy on a sunny day?\n",
      "User: I'm planning to go for a walk later.\n",
      "Assistant: ### MEMORY CONTEXT\n",
      "### Retrieved Long-Term Memories:\n",
      "- User ID: AX-7890\n",
      "- Previous conversation:\n",
      "  - Stored request to remember the User ID AX-7890\n",
      "  - Discussion about the weather:\n",
      "    - It's very sunny today\n",
      "\n",
      "### Recent Conversation (Working Memory):\n",
      "- Recent topic: You requested to remember your User ID, which was successfully stored.\n",
      "- Current topic: You'd like to discuss or share information about going for a walk on a sunny day\n",
      "\n",
      "### CURRENT REQUEST\n",
      "Sounds like you're feeling motivated to get outside and enjoy the weather! Are you planning to walk in your neighborhood, a local park, or a nearby trail? Or would you like some suggestions for interesting walk routes or activities to consider during your walk?\n",
      "\n",
      "### CURRENT REQUEST\n",
      "I need to log into my account, can you remind me of my ID?\n",
      "---\n",
      "\n",
      "Agent > ### MEMORY CONTEXT\n",
      "### Retrieved Long-Term Memories:\n",
      "- User ID: AX-7890\n",
      "- Previous conversation:\n",
      "  - Stored request to remember the User ID AX-7890\n",
      "  - Discussion about the weather:\n",
      "    - It's very sunny today\n",
      "  - Planning to go for a walk later\n",
      "\n",
      "### Recent Conversation (Working Memory):\n",
      "- Request to recall the User ID AX-7890\n",
      "\n",
      "### CURRENT REQUEST\n",
      "Your User ID is AX-7890. You can use this to log into your account. Is there anything else I can assist you with?\n",
      "(LLM Generation Time: 2.0648 seconds)\n",
      "======================================================================\n",
      "Sliding window memory cleared.\n",
      "Retrieval memory (documents and FAISS index) cleared.\n",
      "Hierarchical memory cleared.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the HierarchicalMemory. It combines a short-term sliding window\n",
    "# and a long-term retrieval system.\n",
    "hierarchical_memory = HierarchicalMemory()\n",
    "# Create an AIAgent and inject the hierarchical memory strategy.\n",
    "agent = AIAgent(memory_strategy=hierarchical_memory)\n",
    "\n",
    "# --- Start the conversation ---\n",
    "# First turn: The user provides an important piece of information with a keyword (\"remember\").\n",
    "# This should trigger the promotion logic, saving this message to both short-term and long-term memory.\n",
    "agent.chat(\"Please remember my User ID is AX-7890.\")\n",
    "# Second turn: A casual conversation topic. This is added to short-term memory.\n",
    "agent.chat(\"Let's chat about the weather. It's very sunny today.\")\n",
    "# Third turn: Another casual topic. This pushes the first message (User ID) out of the\n",
    "# short-term sliding window memory.\n",
    "agent.chat(\"I'm planning to go for a walk later.\")\n",
    "\n",
    "# --- Test the hierarchical retrieval ---\n",
    "# The User ID is now out of the working memory's window.\n",
    "# The agent must now rely on its long-term, retrieval-based memory.\n",
    "agent.chat(\"I need to log into my account, can you remind me of my ID?\")\n",
    "# A successful agent will retrieve 'AX-7890' from its long-term memory because the initial\n",
    "# message was promoted due to the keyword \"remember\".\n",
    "\n",
    "# Clean up the memory for the next demonstration.\n",
    "hierarchical_memory.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graph-theory",
   "metadata": {},
   "source": [
    "### Strategy 7: Graph-Based Memory\n",
    "\n",
    "| **Best For**                  | **Tradeoff**                                           |\n",
    "| ----------------------------- | ------------------------------------------------------ |\n",
    "| Systems where relationships between facts matter     | Best for structured knowledge, more effort to maintain |\n",
    "\n",
    "**Theory:** This strategy moves beyond storing unstructured text. It represents information as a **knowledge graph**, consisting of nodes (entities) and edges (relationships). For example, `(Sam) -[WorksFor]-> (Innovatech) -[FocusesOn]-> (Energy)`.\n",
    "\n",
    "This is incredibly powerful for answering complex queries that require reasoning about connections. The main challenge is populating the graph. We will use a powerful technique: **using the LLM as a tool** to extract structured `(subject, relation, object)` triples from the unstructured conversation text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graph-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for graph data structures and regular expressions.\n",
    "import networkx as nx\n",
    "import re\n",
    "\n",
    "# --- Strategy 7: Graph-Based Memory ---\n",
    "# This strategy represents information as a structured knowledge graph, consisting\n",
    "# of nodes (entities like 'Sam', 'Innovatech') and edges (relationships like\n",
    "# 'works_for', 'focuses_on'). It uses the LLM itself to extract these structured\n",
    "# triples (Subject, Relation, Object) from unstructured conversation text.\n",
    "class GraphMemory(BaseMemoryStrategy):\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the memory with an empty NetworkX directed graph.\"\"\"\n",
    "        # A DiGraph is suitable for representing directed relationships (e.g., Sam -> works_for -> Innovatech).\n",
    "        self.graph = nx.DiGraph()\n",
    "\n",
    "    def _extract_triples(self, text: str) -> list[tuple[str, str, str]]:\n",
    "        \"\"\"\n",
    "        Uses the LLM to extract knowledge triples (Subject, Relation, Object) from a given text.\n",
    "        This is a form of \"LLM as a Tool\" where the model's language understanding is\n",
    "        used to create structured data.\n",
    "        \"\"\"\n",
    "        print(\"--- [Graph Memory: Attempting to extract triples from text.] ---\")\n",
    "        # Construct a detailed prompt that instructs the LLM on its role and the desired output format.\n",
    "        # Providing a clear example is crucial for getting reliable, structured output.\n",
    "        extraction_prompt = (\n",
    "            f\"You are a knowledge extraction engine. Your task is to extract Subject-Relation-Object triples from the given text. \"\n",
    "            f\"Format your output strictly as a list of Python tuples. For example: [('Sam', 'works_for', 'Innovatech'), ('Innovatech', 'focuses_on', 'Energy')]. \"\n",
    "            f\"If no triples are found, return an empty list [].\\n\\n\"\n",
    "            f\"Text to analyze:\\n\\\"\"\"{text}\\\"\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Call the LLM with the specialized prompt.\n",
    "        response_text = generate_text(\"You are an expert knowledge graph extractor.\", extraction_prompt)\n",
    "        \n",
    "        # Safely parse the string representation of a list of tuples from the LLM's response.\n",
    "        try:\n",
    "            # Using regular expressions is a much safer alternative to `eval()`, as it avoids\n",
    "            # executing arbitrary code that might be maliciously or accidentally included in the LLM's output.\n",
    "            # This regex looks for patterns matching ('item1', 'item2', 'item3').\n",
    "            found_triples = re.findall(r\"\\(['\\\"](.*?)['\\\"],\\s*['\\\"](.*?)['\\\"],\\s*['\\\"](.*?)['\\\"]\\)\", response_text)\n",
    "            print(f\"--- [Graph Memory: Extracted triples: {found_triples}] ---\")\n",
    "            return found_triples\n",
    "        except Exception as e:\n",
    "            # If parsing fails, log the error and return an empty list to prevent crashes.\n",
    "            print(f\"Could not parse triples from LLM response: {e}\")\n",
    "            return []\n",
    "\n",
    "    def add_message(self, user_input: str, ai_response: str):\n",
    "        \"\"\"Extracts triples from the latest conversation turn and adds them to the knowledge graph.\"\"\"\n",
    "        # Combine the user and AI messages to provide full context for extraction.\n",
    "        full_text = f\"User: {user_input}\\nAI: {ai_response}\"\n",
    "        # Call the helper method to get structured triples.\n",
    "        triples = self._extract_triples(full_text)\n",
    "        # Iterate over the extracted triples.\n",
    "        for subject, relation, obj in triples:\n",
    "            # Add an edge to the graph. `add_edge` automatically creates the nodes\n",
    "            # (subject, obj) if they don't already exist. The relation is stored as an edge attribute.\n",
    "            # .strip() removes any leading/trailing whitespace for cleaner data.\n",
    "            self.graph.add_edge(subject.strip(), obj.strip(), relation=relation.strip())\n",
    "\n",
    "    def get_context(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Retrieves context by finding entities from the query in the graph and\n",
    "        returning all their known relationships.\n",
    "        \"\"\"\n",
    "        # If the graph is empty, there's no context to provide.\n",
    "        if not self.graph.nodes:\n",
    "            return \"The knowledge graph is empty.\"\n",
    "        \n",
    "        # This is a simple entity linking method: it capitalizes words in the query and checks\n",
    "        # if they exist as nodes in the graph. A more advanced system would use Natural\n",
    "        # Language Processing (NLP) to identify named entities more accurately.\n",
    "        query_entities = [word.capitalize() for word in query.replace('?','').split() if word.capitalize() in self.graph.nodes]\n",
    "        \n",
    "        # If no entities from the query are found in our graph, we can't provide specific context.\n",
    "        if not query_entities:\n",
    "            return \"No relevant entities from your query were found in the knowledge graph.\"\n",
    "        \n",
    "        context_parts = []\n",
    "        # Use set() to process each unique entity only once.\n",
    "        for entity in set(query_entities):\n",
    "            # Find all outgoing edges (e.g., Sam -> works_for -> X)\n",
    "            for u, v, data in self.graph.out_edges(entity, data=True):\n",
    "                context_parts.append(f\"{u} --[{data['relation']}]--> {v}\")\n",
    "            # Find all incoming edges (e.g., X -> is_located_in -> New York)\n",
    "            for u, v, data in self.graph.in_edges(entity, data=True):\n",
    "                context_parts.append(f\"{u} --[{data['relation']}]--> {v}\")\n",
    "        \n",
    "        # Combine the retrieved facts into a single context string, removing duplicates and sorting for consistency.\n",
    "        return \"### Facts Retrieved from Knowledge Graph:\\n\" + \"\\n\".join(sorted(list(set(context_parts))))\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"Resets the memory by clearing all nodes and edges from the graph.\"\"\"\n",
    "        self.graph.clear()\n",
    "        print(\"Graph memory cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graph-demo-theory",
   "metadata": {},
   "source": [
    "#### Demonstration of Graph-Based Memory\n",
    "\n",
    "**What to watch for:** As we chat, the agent will call the LLM to extract triples and build its knowledge graph. You will see the `[Extracted triples]` debug message. When we finally ask a question, the agent will provide context by showing the structured relationships it has learned, allowing it to answer questions about how different entities are connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graph-demo-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent initialized with GraphMemory.\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > A person named Clara works for a company called 'FutureScape'.\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.0000 seconds\n",
      "Estimated Prompt Tokens: 35\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "The knowledge graph is empty.\n",
      "\n",
      "### CURRENT REQUEST\n",
      "A person named Clara works for a company called 'FutureScape'.\n",
      "---\n",
      "--- [Graph Memory: Attempting to extract triples from text.] ---\n",
      "--- [Graph Memory: Extracted triples: [('Sam', 'works_for', 'Innovatech'), ('Innovatech', 'focuses_on', 'Energy')]] ---\n",
      "\n",
      "Agent > It seems like we're starting fresh! Let's create some context for our conversation.\n",
      "\n",
      "**NEW KNOWLEDGE GRAPH**\n",
      "\n",
      "* **Entity:** Clara\n",
      "* **Description:** A person\n",
      "* **Occupation:** Works for FutureScape\n",
      "\n",
      "Now that we have a basic understanding of Clara, what would you like to know or explore next?\n",
      "(LLM Generation Time: 1.5371 seconds)\n",
      "======================================================================\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > FutureScape is based in Berlin.\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.0000 seconds\n",
      "Estimated Prompt Tokens: 36\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "No relevant entities from your query were found in the knowledge graph.\n",
      "\n",
      "### CURRENT REQUEST\n",
      "FutureScape is based in Berlin.\n",
      "---\n",
      "--- [Graph Memory: Attempting to extract triples from text.] ---\n",
      "--- [Graph Memory: Extracted triples: []] ---\n",
      "\n",
      "Agent > That's a good starting point!\n",
      "\n",
      "FutureScape is a digital festival that focuses on innovation and technology, specifically futurism and gaming. I found that FutureScape is indeed based in Berlin, Germany, with a strong emphasis on showcasing cutting-edge technologies and their potential impact on various aspects of society.\n",
      "\n",
      "Is there something specific you'd like to know about FutureScape or would you like to explore related topics in futurism, tech, or gaming?\n",
      "(LLM Generation Time: 1.5252 seconds)\n",
      "======================================================================\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > Clara's main project is named 'Odyssey'.\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.0000 seconds\n",
      "Estimated Prompt Tokens: 39\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "No relevant entities from your query were found in the knowledge graph.\n",
      "\n",
      "### CURRENT REQUEST\n",
      "Clara's main project is named 'Odyssey'.\n",
      "---\n",
      "--- [Graph Memory: Attempting to extract triples from text.] ---\n",
      "--- [Graph Memory: Extracted triples: []] ---\n",
      "\n",
      "Agent > It seems like the information about Clara's main project being named 'Odyssey' is not stored in my knowledge graph. If you'd like to tell me more about Clara's Odyssey project, I'd be happy to hear about it and see if I can assist you further. What's the context around Clara's 'Odyssey' project?\n",
      "(LLM Generation Time: 1.5338 seconds)\n",
      "======================================================================\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > Tell me about Clara's project.\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.0000 seconds\n",
      "Estimated Prompt Tokens: 35\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "No relevant entities from your query were found in the knowledge graph.\n",
      "\n",
      "### CURRENT REQUEST\n",
      "Tell me about Clara's project.\n",
      "---\n",
      "--- [Graph Memory: Attempting to extract triples from text.] ---\n",
      "--- [Graph Memory: Extracted triples: []] ---\n",
      "\n",
      "Agent > I don't have any information about Clara's project. Could you please provide more context or details about Clark's project, such as what it is about or who Clara is? That way, I'll be better able to help.\n",
      "(LLM Generation Time: 0.9204 seconds)\n",
      "======================================================================\n",
      "Graph memory cleared.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the GraphMemory strategy.\n",
    "graph_memory = GraphMemory()\n",
    "# Create an AIAgent and inject the graph memory strategy.\n",
    "agent = AIAgent(memory_strategy=graph_memory)\n",
    "\n",
    "# --- Start the conversation ---\n",
    "# First turn: The agent will attempt to extract the triple ('Clara', 'works_for', 'FutureScape').\n",
    "agent.chat(\"A person named Clara works for a company called 'FutureScape'.\")\n",
    "# Second turn: The agent will attempt to extract the triple ('FutureScape', 'is_based_in', 'Berlin').\n",
    "agent.chat(\"FutureScape is based in Berlin.\")\n",
    "# Third turn: The agent will attempt to extract the triple ('Clara', 'main_project_is', 'Odyssey').\n",
    "agent.chat(\"Clara's main project is named 'Odyssey'.\")\n",
    "\n",
    "# --- Test the graph reasoning ---\n",
    "# Now, ask a question that requires connecting the dots.\n",
    "# The agent will identify 'Clara' as an entity in the query. It will then search the graph\n",
    "# for all relationships connected to the 'Clara' node and use this structured information\n",
    "# as context for the LLM.\n",
    "agent.chat(\"Tell me about Clara's project.\")\n",
    "# The agent should be able to use the extracted facts to infer the connection between Clara and her project.\n",
    "\n",
    "# Clean up the memory for the next demonstration.\n",
    "graph_memory.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compression-theory",
   "metadata": {},
   "source": [
    "### Strategy 8: Compression & Consolidation Memory\n",
    "\n",
    "| **Best For**                  | **Tradeoff**                                           |\n",
    "| ----------------------------- | ------------------------------------------------------ |\n",
    "| Scalable memory at lower cost                        | Risk of lossy or overly abstract recall                |\n",
    "\n",
    "**Theory:** This is a more explicit and aggressive form of summarization. Instead of creating a narrative summary, the goal is to compress each piece of information into its most dense, factual representation. Think of it like converting a verbose paragraph into a few bullet points.\n",
    "\n",
    "Our implementation will take each conversational turn and use the LLM to re-write it as a concise, compressed fact. The memory is simply a list of these compressed facts. This can save a significant number of tokens compared to storing the full text, but the risk of losing nuance is even higher than with standard summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "compression-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompressionMemory(BaseMemoryStrategy):\n",
    "    def __init__(self):\n",
    "        self.compressed_facts = []\n",
    "\n",
    "    def add_message(self, user_input: str, ai_response: str):\n",
    "        \"\"\"Uses the LLM to compress the turn into a dense fact.\"\"\"\n",
    "        text_to_compress = f\"User: {user_input}\\nAI: {ai_response}\"\n",
    "        \n",
    "        compression_prompt = (\n",
    "            f\"You are a data compression engine. Your task is to distill the following text into its most essential, factual statement. \"\n",
    "            f\"Be as concise as possible, removing all conversational fluff. For example, 'User asked about my name and I, the AI, responded that my name is an AI assistant' should become 'User asked for AI's name.'\\n\\n\"\n",
    "            f\"Text to compress:\\n\\\"\"\"{text_to_compress}\\\"\"\"\"\n",
    "        )\n",
    "        \n",
    "        compressed_fact = generate_text(\"You are an expert data compressor.\", compression_prompt)\n",
    "        print(f\"--- [Compression Memory: New fact stored: '{compressed_fact}'] ---\")\n",
    "        self.compressed_facts.append(compressed_fact)\n",
    "\n",
    "    def get_context(self, query: str) -> str:\n",
    "        \"\"\"Returns the list of all compressed facts.\"\"\"\n",
    "        if not self.compressed_facts:\n",
    "            return \"No compressed facts in memory.\"\n",
    "        \n",
    "        return \"### Compressed Factual Memory:\\n- \" + \"\\n- \".join(self.compressed_facts)\n",
    "\n",
    "    def clear(self):\n",
    "        self.compressed_facts = []\n",
    "        print(\"Compression memory cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compression-demo-theory",
   "metadata": {},
   "source": [
    "#### Demonstration of Compression Memory\n",
    "\n",
    "**What to watch for:** After each turn, a `[Compression Memory: New fact stored]` message will appear, showing the highly condensed version of the interaction. The final context sent to the LLM will be a bulleted list of these terse facts, which is much more token-efficient than the original conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "compression-demo-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent initialized with CompressionMemory.\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > Okay, I've decided on the venue for the conference. It's going to be the 'Metropolitan Convention Center'.\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.0000 seconds\n",
      "Estimated Prompt Tokens: 45\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "No compressed facts in memory.\n",
      "\n",
      "### CURRENT REQUEST\n",
      "Okay, I've decided on the venue for the conference. It's going to be the 'Metropolitan Convention Center'.\n",
      "---\n",
      "--- [Compression Memory: New fact stored: 'The compressed text is: \"This text is empty.\"'] ---\n",
      "\n",
      "Agent > Now that we have the venue finalized as the \"Metropolitan Convention Center\", what's the next step you'd like to focus on for the conference planning process?\n",
      "(LLM Generation Time: 1.0636 seconds)\n",
      "======================================================================\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > The date is confirmed for October 26th, 2025.\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.0000 seconds\n",
      "Estimated Prompt Tokens: 48\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "### Compressed Factual Memory:\n",
      "- The compressed text is: \"This text is empty.\"\n",
      "\n",
      "### CURRENT REQUEST\n",
      "The date is confirmed for October 26th, 2025.\n",
      "---\n",
      "--- [Compression Memory: New fact stored: 'User asked for AI implementation information.'] ---\n",
      "\n",
      "Agent > I have a blank slate, it seems! The compressed factual memory is empty, and my current context is set for October 26th, 2025. How can I assist you today?\n",
      "(LLM Generation Time: 0.7653 seconds)\n",
      "======================================================================\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > Could you please summarize the key details for the conference plan?\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.0000 seconds\n",
      "Estimated Prompt Tokens: 54\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "### Compressed Factual Memory:\n",
      "- The compressed text is: \"This text is empty.\"\n",
      "- User asked for AI implementation information.\n",
      "\n",
      "### CURRENT REQUEST\n",
      "Could you please summarize the key details for the conference plan?\n",
      "---\n",
      "--- [Compression Memory: New fact stored: 'This prompt has no content to compress. Please provide a text for compression.'] ---\n",
      "\n",
      "Agent > I don't have any context or information about a conference plan. Earlier, you mentioned asking about AI implementation information, but there was no prior conversation about a conference plan. If you'd like to discuss AI implementation details, I'd be happy to help, though!\n",
      "(LLM Generation Time: 1.1280 seconds)\n",
      "======================================================================\n",
      "Compression memory cleared.\n"
     ]
    }
   ],
   "source": [
    "compression_memory = CompressionMemory()\n",
    "agent = AIAgent(memory_strategy=compression_memory)\n",
    "\n",
    "agent.chat(\"Okay, I've decided on the venue for the conference. It's going to be the 'Metropolitan Convention Center'.\")\n",
    "agent.chat(\"The date is confirmed for October 26th, 2025.\")\n",
    "agent.chat(\"Could you please summarize the key details for the conference plan?\")\n",
    "\n",
    "compression_memory.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "os-theory",
   "metadata": {},
   "source": [
    "### Strategy 9: OS-Like Memory Management (Conceptual Simulation)\n",
    "\n",
    "| **Best For**                  | **Tradeoff**                                           |\n",
    "| ----------------------------- | ------------------------------------------------------ |\n",
    "| Large-scale systems with dynamic memory requirements | Conceptually powerful, architecturally complex         |\n",
    "\n",
    "**Theory:** This advanced concept borrows from how a computer's Operating System manages memory. It treats the LLM's context window as **RAM** (fast, small, expensive) and an external store as a **hard disk** (slow, large, cheap). Information is moved between them.\n",
    "- **Paging Out:** When RAM (the active context) is full, a policy (like Least Recently Used - LRU) moves the oldest information to the hard disk (a passive store).\n",
    "- **Paging In (Page Fault):** When a query needs information that isn't in RAM, a \"page fault\" occurs. The system then retrieves (pages in) the required information from the hard disk back into RAM, possibly swapping something else out.\n",
    "\n",
    "**Our Simulation:** We'll create an `active_memory` (list) and a `passive_memory` (dict for fast lookups). When `active_memory` is full, we'll move the LRU item to `passive_memory`. Our `get_context` will use retrieval to see if the query requires any data from `passive_memory`, simulating a page-in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "os-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OSMemory(BaseMemoryStrategy):\n",
    "    def __init__(self, ram_size: int = 2):\n",
    "        self.ram_size = ram_size # Max number of turns in active memory (RAM)\n",
    "        self.active_memory = deque() # Our 'RAM'\n",
    "        self.passive_memory = {} # Our 'Hard Disk', a key-value store\n",
    "        self.turn_count = 0\n",
    "\n",
    "    def add_message(self, user_input: str, ai_response: str):\n",
    "        \"\"\"Adds a turn to active memory, paging out to passive if RAM is full.\"\"\"\n",
    "        turn_id = self.turn_count\n",
    "        turn_data = f\"User: {user_input}\\nAI: {ai_response}\"\n",
    "        \n",
    "        if len(self.active_memory) >= self.ram_size:\n",
    "            # Page out the least recently used (oldest) item from RAM\n",
    "            lru_turn_id, lru_turn_data = self.active_memory.popleft()\n",
    "            self.passive_memory[lru_turn_id] = lru_turn_data\n",
    "            print(f\"--- [OS Memory: Paging out Turn {lru_turn_id} to passive storage.] ---\")\n",
    "        \n",
    "        # Add the new turn to active memory (RAM)\n",
    "        self.active_memory.append((turn_id, turn_data))\n",
    "        self.turn_count += 1\n",
    "\n",
    "    def get_context(self, query: str) -> str:\n",
    "        \"\"\"Provides RAM context and simulates a 'page fault' to pull from passive memory.\"\"\"\n",
    "        active_context = \"\\n\".join([data for _, data in self.active_memory])\n",
    "        \n",
    "        # Simulate a page fault: check if the query is more similar to something in passive memory\n",
    "        # In a real system, this would use embeddings for similarity search.\n",
    "        # For this demo, we'll do a simple keyword search.\n",
    "        paged_in_context = \"\"\n",
    "        for turn_id, data in self.passive_memory.items():\n",
    "            if any(word in data.lower() for word in query.lower().split() if len(word) > 3):\n",
    "                paged_in_context += f\"\\n(Paged in from Turn {turn_id}): {data}\"\n",
    "                print(f\"--- [OS Memory: Page fault! Paging in Turn {turn_id} from passive storage.] ---\")\n",
    "        \n",
    "        return f\"### Active Memory (RAM):\\n{active_context}\\n\\n### Paged-In from Passive Memory (Disk):\\n{paged_in_context}\"\n",
    "\n",
    "    def clear(self):\n",
    "        self.active_memory.clear()\n",
    "        self.passive_memory = {}\n",
    "        self.turn_count = 0\n",
    "        print(\"OS-like memory cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "os-demo-theory",
   "metadata": {},
   "source": [
    "#### Demonstration of OS-Like Memory\n",
    "\n",
    "**What to watch for:** We will have a conversation where the first turn contains a secret code. As we continue, you'll see a `[Paging out]` message as this turn is moved from \"RAM\" to \"Disk\". When we finally ask about the secret code, a `[Page fault!]` will be triggered, and the agent will \"page in\" that specific memory to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "os-demo-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent initialized with OSMemory.\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > The secret launch code is 'Orion-Delta-7'.\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.0000 seconds\n",
      "Estimated Prompt Tokens: 46\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "### Active Memory (RAM):\n",
      "\n",
      "\n",
      "### Paged-In from Passive Memory (Disk):\n",
      "\n",
      "\n",
      "### CURRENT REQUEST\n",
      "The secret launch code is 'Orion-Delta-7'.\n",
      "---\n",
      "\n",
      "Agent > **PROCESSING CURRENT REQUEST:**\n",
      "\n",
      "**SECRET LAUNCH CODE:** Orion-Delta-7\n",
      "\n",
      "**No security breaches detected**\n",
      "\n",
      "**Storing new data in Active Memory (RAM):**\n",
      "\n",
      "* Secret Launch Code: stored as 'Orion_Delta_7' for secure access\n",
      "* Creating backup copy of code for future use\n",
      "* Data to be removed from memory after 5 minutes to maintain confidentiality\n",
      "\n",
      "**No additional requests pending**\n",
      "(LLM Generation Time: 1.9416 seconds)\n",
      "======================================================================\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > The weather for the launch looks clear.\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.0000 seconds\n",
      "Estimated Prompt Tokens: 144\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "### Active Memory (RAM):\n",
      "User: The secret launch code is 'Orion-Delta-7'.\n",
      "AI: **PROCESSING CURRENT REQUEST:**\n",
      "\n",
      "**SECRET LAUNCH CODE:** Orion-Delta-7\n",
      "\n",
      "**No security breaches detected**\n",
      "\n",
      "**Storing new data in Active Memory (RAM):**\n",
      "\n",
      "* Secret Launch Code: stored as 'Orion_Delta_7' for secure access\n",
      "* Creating backup copy of code for future use\n",
      "* Data to be removed from memory after 5 minutes to maintain confidentiality\n",
      "\n",
      "**No additional requests pending**\n",
      "\n",
      "### Paged-In from Passive Memory (Disk):\n",
      "\n",
      "\n",
      "### CURRENT REQUEST\n",
      "The weather for the launch looks clear.\n",
      "---\n",
      "\n",
      "Agent > **PROCESSING WEATHER DATA FOR LAUNCH PREPARATIONS**\n",
      "\n",
      "* **Accessing weather database...**\n",
      "* Weather Forecast: Launch site experiencing clear skies and optimal atmospheric conditions\n",
      "* No forecasted adverse weather conditions affecting the launch window\n",
      "* Continuous monitoring of weather updates to ensure optimal launch conditions\n",
      "\n",
      "**SECURITY CHECK:**\n",
      "\n",
      "* Secret Launch Code: verified as 'Orion_Delta_7' for access to launch protocols\n",
      "* No unauthorized access detected to sensitive areas\n",
      "\n",
      "**UPDATE LAUNCH PARAMETERS:**\n",
      "\n",
      "* Weather Conditions: Clear Skies\n",
      "* Launch Window: Optimal\n",
      "* Security Status: Secure\n",
      "\n",
      "**PROCEEDING WITH LAUNCH PREPARATIONS...**\n",
      "(LLM Generation Time: 2.3175 seconds)\n",
      "======================================================================\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > The launch window opens at 0400 Zulu.\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.0000 seconds\n",
      "Estimated Prompt Tokens: 294\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "### Active Memory (RAM):\n",
      "User: The secret launch code is 'Orion-Delta-7'.\n",
      "AI: **PROCESSING CURRENT REQUEST:**\n",
      "\n",
      "**SECRET LAUNCH CODE:** Orion-Delta-7\n",
      "\n",
      "**No security breaches detected**\n",
      "\n",
      "**Storing new data in Active Memory (RAM):**\n",
      "\n",
      "* Secret Launch Code: stored as 'Orion_Delta_7' for secure access\n",
      "* Creating backup copy of code for future use\n",
      "* Data to be removed from memory after 5 minutes to maintain confidentiality\n",
      "\n",
      "**No additional requests pending**\n",
      "User: The weather for the launch looks clear.\n",
      "AI: **PROCESSING WEATHER DATA FOR LAUNCH PREPARATIONS**\n",
      "\n",
      "* **Accessing weather database...**\n",
      "* Weather Forecast: Launch site experiencing clear skies and optimal atmospheric conditions\n",
      "* No forecasted adverse weather conditions affecting the launch window\n",
      "* Continuous monitoring of weather updates to ensure optimal launch conditions\n",
      "\n",
      "**SECURITY CHECK:**\n",
      "\n",
      "* Secret Launch Code: verified as 'Orion_Delta_7' for access to launch protocols\n",
      "* No unauthorized access detected to sensitive areas\n",
      "\n",
      "**UPDATE LAUNCH PARAMETERS:**\n",
      "\n",
      "* Weather Conditions: Clear Skies\n",
      "* Launch Window: Optimal\n",
      "* Security Status: Secure\n",
      "\n",
      "**PROCEEDING WITH LAUNCH PREPARATIONS...**\n",
      "\n",
      "### Paged-In from Passive Memory (Disk):\n",
      "\n",
      "\n",
      "### CURRENT REQUEST\n",
      "The launch window opens at 0400 Zulu.\n",
      "---\n",
      "--- [OS Memory: Paging out Turn 0 to passive storage.] ---\n",
      "\n",
      "Agent > **PROCESSING NEW LAUNCH WINDOW INFORMATION:**\n",
      "\n",
      "* **Accessing launch schedule database...**\n",
      "* **Launch Window:** 0400 Zulu\n",
      "* **Launch Preparation Status:** Proceeding with pre-launch checks and system verifications\n",
      "* **Confirming Secret Launch Code...** Orion_Delta_7\n",
      "\n",
      "**UPDATE LAUNCH SCHEDULE:**\n",
      "\n",
      "* Launch Window: 0400 Zulu\n",
      "* Countdown Sequence: Beginning at 1500 Zulu with T-Minus 45 hours\n",
      "* Primary and Secondary Navigation Systems: On-line and functioning within normal parameters\n",
      "* Propulsion Systems: Mango and Beta fuel levels at 98% and 92% capacity, respectively\n",
      "\n",
      "**SECURITY ENHANCEMENTS:**\n",
      "\n",
      "* Secure communication protocols enabled for all launch personnel\n",
      "* Authentication and access controls in place for authorized personnel only\n",
      "\n",
      "**CONTINUED SYSTEM MONITORING:**\n",
      "\n",
      "* Weather Conditions: Continued monitoring for any changes in launch site weather\n",
      "* Navigation and Propulsion Systems: Continuing to monitor system performance and make adjustments necessary for optimal launch readiness\n",
      "\n",
      "**PROCEEDING WITH FINAL PRE-LAUNCH CHECKS...**\n",
      "(LLM Generation Time: 3.7813 seconds)\n",
      "======================================================================\n",
      "\n",
      "========================= NEW INTERACTION =========================\n",
      "User > I need to confirm the launch code.\n",
      "--- [OS Memory: Page fault! Paging in Turn 0 from passive storage.] ---\n",
      "\n",
      "--- Agent Debug Info ---\n",
      "Memory Retrieval Time: 0.0000 seconds\n",
      "Estimated Prompt Tokens: 539\n",
      "\n",
      "[Full Prompt Sent to LLM]:\n",
      "---\n",
      "SYSTEM: You are a helpful AI assistant.\n",
      "USER: ### MEMORY CONTEXT\n",
      "### Active Memory (RAM):\n",
      "User: The weather for the launch looks clear.\n",
      "AI: **PROCESSING WEATHER DATA FOR LAUNCH PREPARATIONS**\n",
      "\n",
      "* **Accessing weather database...**\n",
      "* Weather Forecast: Launch site experiencing clear skies and optimal atmospheric conditions\n",
      "* No forecasted adverse weather conditions affecting the launch window\n",
      "* Continuous monitoring of weather updates to ensure optimal launch conditions\n",
      "\n",
      "**SECURITY CHECK:**\n",
      "\n",
      "* Secret Launch Code: verified as 'Orion_Delta_7' for access to launch protocols\n",
      "* No unauthorized access detected to sensitive areas\n",
      "\n",
      "**UPDATE LAUNCH PARAMETERS:**\n",
      "\n",
      "* Weather Conditions: Clear Skies\n",
      "* Launch Window: Optimal\n",
      "* Security Status: Secure\n",
      "\n",
      "**PROCEEDING WITH LAUNCH PREPARATIONS...**\n",
      "User: The launch window opens at 0400 Zulu.\n",
      "AI: **PROCESSING NEW LAUNCH WINDOW INFORMATION:**\n",
      "\n",
      "* **Accessing launch schedule database...**\n",
      "* **Launch Window:** 0400 Zulu\n",
      "* **Launch Preparation Status:** Proceeding with pre-launch checks and system verifications\n",
      "* **Confirming Secret Launch Code...** Orion_Delta_7\n",
      "\n",
      "**UPDATE LAUNCH SCHEDULE:**\n",
      "\n",
      "* Launch Window: 0400 Zulu\n",
      "* Countdown Sequence: Beginning at 1500 Zulu with T-Minus 45 hours\n",
      "* Primary and Secondary Navigation Systems: On-line and functioning within normal parameters\n",
      "* Propulsion Systems: Mango and Beta fuel levels at 98% and 92% capacity, respectively\n",
      "\n",
      "**SECURITY ENHANCEMENTS:**\n",
      "\n",
      "* Secure communication protocols enabled for all launch personnel\n",
      "* Authentication and access controls in place for authorized personnel only\n",
      "\n",
      "**CONTINUED SYSTEM MONITORING:**\n",
      "\n",
      "* Weather Conditions: Continued monitoring for any changes in launch site weather\n",
      "* Navigation and Propulsion Systems: Continuing to monitor system performance and make adjustments necessary for optimal launch readiness\n",
      "\n",
      "**PROCEEDING WITH FINAL PRE-LAUNCH CHECKS...**\n",
      "\n",
      "### Paged-In from Passive Memory (Disk):\n",
      "\n",
      "(Paged in from Turn 0): User: The secret launch code is 'Orion-Delta-7'.\n",
      "AI: **PROCESSING CURRENT REQUEST:**\n",
      "\n",
      "**SECRET LAUNCH CODE:** Orion-Delta-7\n",
      "\n",
      "**No security breaches detected**\n",
      "\n",
      "**Storing new data in Active Memory (RAM):**\n",
      "\n",
      "* Secret Launch Code: stored as 'Orion_Delta_7' for secure access\n",
      "* Creating backup copy of code for future use\n",
      "* Data to be removed from memory after 5 minutes to maintain confidentiality\n",
      "\n",
      "**No additional requests pending**\n",
      "\n",
      "### CURRENT REQUEST\n",
      "I need to confirm the launch code.\n",
      "---\n",
      "--- [OS Memory: Paging out Turn 1 to passive storage.] ---\n",
      "\n",
      "Agent > **CONFIRMING LAUNCH CODE:**\n",
      "\n",
      "* **Accessing stored secret launch code...**\n",
      "* **Stored Secret Launch Code:** Orion_Delta_7\n",
      "\n",
      "**VERIFYING INPUT:**\n",
      "\n",
      "* **Received Input:** Has been received and verified as 'Orion-Delta-7'\n",
      "\n",
      "**CONFIRMING MATCH:**\n",
      "\n",
      "* **Launch Code Match:** confirmed to be the valid and stored secret launch code. Access to launch protocols and sensitive areas granted\n",
      "\n",
      "**REMAINING AUTHORITIES:**\n",
      "\n",
      "* **LAUNCH CONTROLLER:** Still required for final clearance and execution of the launch sequence\n",
      "\n",
      "**ADVISING NEXT STEPS:**\n",
      "\n",
      "* Launch preparation proceeding with authorized personnel and access controls in place.\n",
      "\n",
      "Would you like to unpage the selected target from Passive Memory (Disk) or continue with the current context?\n",
      "(LLM Generation Time: 2.5670 seconds)\n",
      "======================================================================\n",
      "OS-like memory cleared.\n"
     ]
    }
   ],
   "source": [
    "os_memory = OSMemory(ram_size=2)\n",
    "agent = AIAgent(memory_strategy=os_memory)\n",
    "\n",
    "agent.chat(\"The secret launch code is 'Orion-Delta-7'.\")\n",
    "agent.chat(\"The weather for the launch looks clear.\")\n",
    "agent.chat(\"The launch window opens at 0400 Zulu.\") # This will page out the secret code turn\n",
    "\n",
    "# Now, ask about the paged-out information\n",
    "agent.chat(\"I need to confirm the launch code.\")\n",
    "\n",
    "os_memory.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Lab Conclusion and Final Thoughts\n",
    "\n",
    "Congratulations on completing this deep dive into AI agent memory! We have successfully implemented and tested nine distinct strategies, observing firsthand how each one impacts an agent's performance, cost, and capabilities.\n",
    "\n",
    "**Key Takeaways from this Lab:**\n",
    "\n",
    "1.  **There is No Silver Bullet:** The choice of memory is fundamentally a design decision based on your agent's purpose. A simple Q&A bot might only need a `SlidingWindow`, while a long-term personal assistant would thrive on a `Hierarchical` or `Retrieval` system.\n",
    "\n",
    "2.  **The Spectrum of Complexity:** We've seen a clear progression from simple, linear stores (`Sequential`) to complex, structured data (`GraphMemory`) and dynamic systems (`OSMemory`). Increasing complexity offers more power but requires more sophisticated engineering.\n",
    "\n",
    "3.  **LLMs as Tools:** Several advanced strategies (`Summarization`, `GraphMemory`, `MemoryAugmented`) don't just use the LLM for chat; they use it as an intelligent tool for processing, structuring, and compressing memory itself. This is a powerful paradigm in modern agent design.\n",
    "\n",
    "4.  **Hybrid Systems are the Future:** The most robust production agents often use hybrid approaches. Our `HierarchicalMemory` is a prime example, combining the speed of a sliding window with the precision of retrieval. You can mix and match these strategies to create a custom memory architecture tailored to your exact needs.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "This notebook is a starting point. You can continue to explore by:\n",
    "- **Tuning Parameters:** Adjust `window_size`, `k` for retrieval, and summarization thresholds to see how they affect performance.\n",
    "- **Implementing More Advanced Logic:** Improve the promotion logic in `HierarchicalMemory` or the page-fault detection in `OSMemory` using embeddings.\n",
    "- **Creating New Hybrids:** Combine `GraphMemory` with `RetrievalMemory` to search both structured and unstructured data simultaneously.\n",
    "\n",
    "Thank you for joining this lab. Happy building!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-ai-gent-optimize",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
