{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96e291a8-eeaa-4393-b19c-e588fabebf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import zip_longest\n",
    "import re\n",
    "from pathlib import Path\n",
    "from os.path import abspath\n",
    "import os\n",
    "\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35b70757-3a06-4435-bd62-b4067c71eea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogParser:\n",
    "    \n",
    "    def __init__(self, file_folder = \"files\", *args, **kwargs):\n",
    "        self.path_to_files = os.path.join(abspath(os.getcwd()), file_folder)\n",
    "        if kwargs.get(\"process_all\"):\n",
    "            self.files = self.get_all_files()\n",
    "\n",
    "    def get_all_files(self):\n",
    "        \"\"\"Open all of the files if you want\"\"\"\n",
    "        return [self.get_file(filename = file) for file in os.listdir(self.path_to_files)]\n",
    "\n",
    "    def get_file(self,filename):\n",
    "        \"\"\"Get particular file\"\"\"\n",
    "        full_path_to_file = os.path.join(self.path_to_files, filename)\n",
    "        check_file = Path(full_path_to_file)\n",
    "        if not check_file.is_file():\n",
    "            raise Exception(f\"Not a File: {full_path_to_file}\")\n",
    "\n",
    "        with open(full_path_to_file, \"r\") as f:\n",
    "            corpus = f.read()\n",
    "        return corpus\n",
    "    \n",
    "    @staticmethod\n",
    "    def grouper(iterable, n, fillvalue=None):\n",
    "        \"\"\"Collect data into fixed-length chunks or blocks.\n",
    "\n",
    "        >>> grouper('ABCDEFG', 3, 'x')\n",
    "        ['ABC', 'DEF', 'Gxx']\n",
    "        Source:\n",
    "            https://docs.python.org/3/library/itertools.html#itertools-recipes\n",
    "        \"\"\"\n",
    "        args = [iter(iterable)] * n\n",
    "        return zip_longest(*args, fillvalue=fillvalue)\n",
    "\n",
    "    def group_positions(self,sentences,pattern = r\"^(?![\\s\\S])\"):\n",
    "        \"\"\"Group together sentences based on splitting condition\n",
    "\n",
    "        Args:\n",
    "            sentences (list(str)): list of strings split on new line\n",
    "            pattern (str): pattern indicating start and end of new group\n",
    "        return:\n",
    "            groups (list(tuple)): list of groups containing data to be parsed\n",
    "        \"\"\"\n",
    "        positions = []\n",
    "        for idx,sentence in enumerate(sentences):\n",
    "            if re.search(pattern,sentence):\n",
    "                positions.append(idx)\n",
    "        return list(self.grouper(positions,2))\n",
    "    \n",
    "    def get_groups(self,sentences,groups):\n",
    "        \"\"\"Group the correct data together\n",
    "        \n",
    "        Args:\n",
    "            sentences (list(str)): list of sentences \n",
    "            groups (list(tuple(int,int))): list of positions indicating groups\n",
    "        \n",
    "        Return:\n",
    "            All of the correct groupings\n",
    "        \"\"\"\n",
    "        groupings = []\n",
    "        for group in groups:\n",
    "            start, end = group[0], group[-1]\n",
    "            groupings.append(tuple(sentences[start+1:end]))\n",
    "        return groupings\n",
    "    \n",
    "    def extract_data(self,filename):\n",
    "        \"\"\"You can build on this to process all files at once\"\"\"\n",
    "        corpus = self.get_file(filename = filename)\n",
    "        sentences = corpus.split(\"\\n\")\n",
    "        groups = self.group_positions(sentences)\n",
    "        extracted_data = self.get_groups(sentences,groups)\n",
    "        return extracted_data\n",
    "\n",
    "@dataclass  \n",
    "class Record:\n",
    "    \"\"\"Define a schema\"\"\"\n",
    "    job_name: Optional[str] = \"\"\n",
    "    status: Optional[str] = \"\"\n",
    "    start_time: Optional[str] = \"\"\n",
    "    start_date: Optional[str] = \"\"\n",
    "    last_ran_time: Optional[str] = \"\"\n",
    "    last_ran_date: Optional[str] = \"\"\n",
    "    elapsed_time: Optional[str] = \"\"\n",
    "\n",
    "class Transformer:\n",
    "    \n",
    "    base_pattern = r\"[a-zA-Z_]+\\:|[a-zA-Z]+\\W+[a-zA-Z_]+\\:\"\n",
    "    on_date = r\"(([a-zA-Z_]+\\:)|([a-zA-Z]+\\W+[a-zA-Z_]+\\:)) ((0?[1-9]|1[012])\\/(0?[1-9]|[12][0-9]|3[01])\\/\\d{4})\"\n",
    "    started_and_ran = r\"(([a-zA-Z_]+\\:)|([a-zA-Z]+\\W+[a-zA-Z_]+\\:)) (([1-9]|0[1-9]|1[0-2]):[0-5][0-9] ([AaPp][Mm]))\"\n",
    "    elapsed = r\"(([a-zA-Z_]+\\:)|([a-zA-Z]+\\W+[a-zA-Z_]+\\:)) (\\d{2}:\\d{2}:\\d{2})\"\n",
    "    \n",
    "    \n",
    "    def get_job_or_status(self,sentence):\n",
    "        \"\"\"Used to get data after semicolon\"\"\"\n",
    "        return sentence.strip().split(\":\")[-1]\n",
    "    \n",
    "    def get_elapsed(self,sentence):\n",
    "        matches = re.search(self.elapsed,sentence)\n",
    "        if not matches:\n",
    "            return \"\"\n",
    "        match_span = matches.span()\n",
    "        subsequence = sentence[match_span[0]:match_span[1]]\n",
    "        name,value = subsequence.split(\":\",1)\n",
    "        return value\n",
    "    \n",
    "    def get_start_and_ran(self,sentence):\n",
    "        matches = re.finditer(self.started_and_ran,sentence)\n",
    "        if not matches:\n",
    "            return {}\n",
    "        \n",
    "        output = {}\n",
    "        for match in matches:\n",
    "            group, span = match.group(), match.span()\n",
    "            beginning,end = span[0], span[1]\n",
    "            subsequence = sentence[beginning:end]\n",
    "            name,value = subsequence.split(\":\",1)\n",
    "            output[name.strip().lower()] = value\n",
    "        return output\n",
    "\n",
    "    def get_dates(self,sentence):\n",
    "        matches = re.finditer(self.on_date,sentence)\n",
    "        if not matches:\n",
    "            return {}\n",
    "        \n",
    "        output = {}\n",
    "        for idx,match in enumerate(matches):\n",
    "            group, span = match.group(), match.span()\n",
    "            beginning,end = span[0], span[1]\n",
    "            subsequence = sentence[beginning:end]\n",
    "            name,value = subsequence.split(\":\",1)\n",
    "            output[str(idx)] = value\n",
    "        return output\n",
    "    \n",
    "    def create_record(self,group):\n",
    "        \"\"\"Main function to create records based on the groups we identified\"\"\"\n",
    "        if len(group) < 3:\n",
    "            return None\n",
    "        name,status,runtime_info = group[0], group[1], group[2]\n",
    "        \n",
    "        status = self.get_job_or_status(sentence = status)\n",
    "        job_name = self.get_job_or_status(sentence = name)\n",
    "        elapsed_time = self.get_elapsed(sentence = runtime_info)\n",
    "        \n",
    "        dates = self.get_dates(runtime_info)\n",
    "        times = self.get_start_and_ran(runtime_info)\n",
    "        \n",
    "        record = Record()\n",
    "        \n",
    "        record.job_name = job_name\n",
    "        record.status = status\n",
    "        record.elapsed_time = elapsed_time\n",
    "        record.start_time = times.get(\"started\",\"\")\n",
    "        record.last_ran_time = times.get(\"last ran\",\"\")\n",
    "        record.start_date = dates.get(\"0\",\"\")\n",
    "        record.last_ran_date = dates.get(\"1\",\"\")\n",
    "        \n",
    "        return record\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34283a47-2e0d-4b24-9146-916131192b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"{}.txt\"\n",
    "parser = LogParser()\n",
    "transformer = Transformer()\n",
    "\n",
    "groups = parser.extract_data(filename)\n",
    "data = [transformer.create_record(group) for group in groups]\n",
    "\n",
    "pd.DataFrame([asdict(d) for d in data if d]).to_csv(\"logs.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb4da0c-60ad-49e6-bc74-b5828185dd94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b469c82f-3913-42f6-9354-48fdbf7e1e35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34d8575",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
