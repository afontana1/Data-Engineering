{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50% Consensus: BERT-Based Financial Sentiment Classifier\n",
    "# Run in colab on an A100 using `finetune_bert.py` from our Docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u7FjXss-ziIw",
    "outputId": "1aa21fe4-40f9-4cf9-efff-fc91350a4e2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Obtaining dependency information for wandb from https://files.pythonhosted.org/packages/1c/5e/0362fa88679852c7fd3ac85ee5bd949426c4a51a61379010d4089be6d7ac/wandb-0.15.12-py3-none-any.whl.metadata\n",
      "  Downloading wandb-0.15.12-py3-none-any.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /Users/iankelk/anaconda3/lib/python3.11/site-packages (from wandb) (8.0.4)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Obtaining dependency information for GitPython!=3.1.29,>=1.0.0 from https://files.pythonhosted.org/packages/8a/7e/20f7e45878b5aed34320fbeeae8f78acc806e7bd708d00b1c6e64b016f5b/GitPython-3.1.37-py3-none-any.whl.metadata\n",
      "  Downloading GitPython-3.1.37-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /Users/iankelk/anaconda3/lib/python3.11/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Users/iankelk/anaconda3/lib/python3.11/site-packages (from wandb) (5.9.0)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Obtaining dependency information for sentry-sdk>=1.0.0 from https://files.pythonhosted.org/packages/62/3a/765a7699a26884dcbf8b071dbe2a2486cc1cafcfb5f5d2e64ffe745dd0c6/sentry_sdk-1.31.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading sentry_sdk-1.31.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: PyYAML in /Users/iankelk/anaconda3/lib/python3.11/site-packages (from wandb) (6.0.1)\n",
      "Collecting pathtools (from wandb)\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting setproctitle (from wandb)\n",
      "  Obtaining dependency information for setproctitle from https://files.pythonhosted.org/packages/c9/17/7f9d5ddf4cfc4386e74565ccf63b8381396336e4629bb165b52b803ceddb/setproctitle-1.3.3-cp311-cp311-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading setproctitle-1.3.3-cp311-cp311-macosx_10_9_universal2.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: setuptools in /Users/iankelk/anaconda3/lib/python3.11/site-packages (from wandb) (68.0.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /Users/iankelk/anaconda3/lib/python3.11/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /Users/iankelk/anaconda3/lib/python3.11/site-packages (from wandb) (4.24.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /Users/iankelk/anaconda3/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Using cached gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/iankelk/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/iankelk/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/iankelk/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/iankelk/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Obtaining dependency information for smmap<6,>=3.0.1 from https://files.pythonhosted.org/packages/a7/a5/10f97f73544edcdef54409f1d839f6049a0d79df68adbc1ceb24d1aaca42/smmap-5.0.1-py3-none-any.whl.metadata\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading wandb-0.15.12-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading GitPython-3.1.37-py3-none-any.whl (190 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.0/190.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-1.31.0-py2.py3-none-any.whl (224 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.8/224.8 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setproctitle-1.3.3-cp311-cp311-macosx_10_9_universal2.whl (16 kB)\n",
      "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: pathtools\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=c86e11e670807cab54b42b3939c05bcd831591bbfc29607a40c9927e19c0cecc\n",
      "  Stored in directory: /Users/iankelk/Library/Caches/pip/wheels/ea/b7/8b/84e94095ea418b9442f5abeba4ca7b0ad52d3fe7b69d6238a6\n",
      "Successfully built pathtools\n",
      "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
      "Successfully installed GitPython-3.1.37 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.31.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.15.12\n",
      "Requirement already satisfied: transformers in /Users/iankelk/anaconda3/lib/python3.11/site-packages (4.29.2)\n",
      "Requirement already satisfied: filelock in /Users/iankelk/anaconda3/lib/python3.11/site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/iankelk/anaconda3/lib/python3.11/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/iankelk/anaconda3/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/iankelk/anaconda3/lib/python3.11/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/iankelk/anaconda3/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/iankelk/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/iankelk/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/iankelk/anaconda3/lib/python3.11/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/iankelk/anaconda3/lib/python3.11/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec in /Users/iankelk/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/iankelk/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/iankelk/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/iankelk/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/iankelk/anaconda3/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/iankelk/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "# We did need to install these two libraries, as they're handled in our docker by the Pipfile\n",
    "!pip install wandb\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HrL9a-ThztIH",
    "outputId": "088e6c5d-244e-4616-d118-b08bfa9fc262",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-07 06:28:49.208810: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Num GPUs Available:  1\n",
      "2023-10-07 06:28:52.253070: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "Found GPU at: /device:GPU:0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miankelk\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "Encoding is: ISO-8859-1\n",
      "Downloading (…)okenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 148kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100% 232k/232k [00:00<00:00, 43.3MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100% 466k/466k [00:00<00:00, 35.2MB/s]\n",
      "Downloading (…)lve/main/config.json: 100% 570/570 [00:00<00:00, 3.37MB/s]\n",
      "Downloading model.safetensors: 100% 440M/440M [00:01<00:00, 371MB/s]\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231007_062910-ijpdym3q\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbert-finetune-run-50agree-10\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/iankelk/bert-sentiment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/iankelk/bert-sentiment/runs/ijpdym3q\u001b[0m\n",
      "Epoch 1/10\n",
      "424/424 [==============================] - ETA: 0s - loss: 0.5049 - accuracy: 0.7963\n",
      "Calculating validation F1 score...\n",
      "Epoch: 1 - validation_data f1_score: 0.8378\n",
      "424/424 [==============================] - 104s 135ms/step - loss: 0.5049 - accuracy: 0.7963 - val_loss: 0.3968 - val_accuracy: 0.8363\n",
      "Epoch 2/10\n",
      "424/424 [==============================] - ETA: 0s - loss: 0.2306 - accuracy: 0.9183\n",
      "Calculating validation F1 score...\n",
      "Epoch: 2 - validation_data f1_score: 0.8163\n",
      "424/424 [==============================] - 31s 74ms/step - loss: 0.2306 - accuracy: 0.9183 - val_loss: 0.5758 - val_accuracy: 0.8129\n",
      "Epoch 3/10\n",
      "424/424 [==============================] - ETA: 0s - loss: 0.1259 - accuracy: 0.9608\n",
      "Calculating validation F1 score...\n",
      "Epoch: 3 - validation_data f1_score: 0.8319\n",
      "424/424 [==============================] - 32s 74ms/step - loss: 0.1259 - accuracy: 0.9608 - val_loss: 0.6491 - val_accuracy: 0.8294\n",
      "Epoch 4/10\n",
      "424/424 [==============================] - ETA: 0s - loss: 0.0916 - accuracy: 0.9685\n",
      "Calculating validation F1 score...\n",
      "Epoch: 4 - validation_data f1_score: 0.8446\n",
      "424/424 [==============================] - 32s 75ms/step - loss: 0.0916 - accuracy: 0.9685 - val_loss: 0.5946 - val_accuracy: 0.8446\n",
      "Epoch 5/10\n",
      "424/424 [==============================] - ETA: 0s - loss: 0.0641 - accuracy: 0.9802\n",
      "Calculating validation F1 score...\n",
      "Epoch: 5 - validation_data f1_score: 0.8443\n",
      "424/424 [==============================] - 31s 74ms/step - loss: 0.0641 - accuracy: 0.9802 - val_loss: 0.6158 - val_accuracy: 0.8459\n",
      "Epoch 6/10\n",
      "424/424 [==============================] - ETA: 0s - loss: 0.0487 - accuracy: 0.9858\n",
      "Calculating validation F1 score...\n",
      "Epoch: 6 - validation_data f1_score: 0.8430\n",
      "424/424 [==============================] - 31s 74ms/step - loss: 0.0487 - accuracy: 0.9858 - val_loss: 0.6791 - val_accuracy: 0.8418\n",
      "Epoch 7/10\n",
      "424/424 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9906\n",
      "Calculating validation F1 score...\n",
      "Epoch: 7 - validation_data f1_score: 0.8534\n",
      "424/424 [==============================] - 32s 75ms/step - loss: 0.0320 - accuracy: 0.9906 - val_loss: 0.7066 - val_accuracy: 0.8528\n",
      "Epoch 8/10\n",
      "424/424 [==============================] - ETA: 0s - loss: 0.0373 - accuracy: 0.9894\n",
      "Calculating validation F1 score...\n",
      "Epoch: 8 - validation_data f1_score: 0.8418\n",
      "424/424 [==============================] - 31s 73ms/step - loss: 0.0373 - accuracy: 0.9894 - val_loss: 0.8494 - val_accuracy: 0.8418\n",
      "Epoch 9/10\n",
      "424/424 [==============================] - ETA: 0s - loss: 0.0436 - accuracy: 0.9873\n",
      "Calculating validation F1 score...\n",
      "Epoch: 9 - validation_data f1_score: 0.8366\n",
      "424/424 [==============================] - 31s 74ms/step - loss: 0.0436 - accuracy: 0.9873 - val_loss: 0.6743 - val_accuracy: 0.8363\n",
      "Epoch 10/10\n",
      "424/424 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9932\n",
      "Calculating validation F1 score...\n",
      "Epoch: 10 - validation_data f1_score: 0.8501\n",
      "424/424 [==============================] - 31s 74ms/step - loss: 0.0258 - accuracy: 0.9932 - val_loss: 0.8328 - val_accuracy: 0.8514\n",
      "91/91 [==============================] - 5s 20ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.80      0.82        91\n",
      "     neutral       0.85      0.91      0.88       432\n",
      "    positive       0.82      0.71      0.76       204\n",
      "\n",
      "    accuracy                           0.84       727\n",
      "   macro avg       0.84      0.81      0.82       727\n",
      "weighted avg       0.84      0.84      0.84       727\n",
      "\n",
      "F1 Score: 0.8392584602932518\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     accuracy ▁▅▇▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     f1_score ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         loss █▄▂▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: val_accuracy ▅▁▄▇▇▆█▆▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ▁▄▅▄▄▅▆█▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      accuracy 0.99322\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    best_epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: best_val_loss 0.39678\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         epoch 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      f1_score 0.83926\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          loss 0.02581\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  val_accuracy 0.85144\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val_loss 0.83284\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbert-finetune-run-50agree-10\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/iankelk/bert-sentiment/runs/ijpdym3q\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231007_062910-ijpdym3q/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# It's ugly but it works. There is no image displayed because in `finetune_bert.py`\n",
    "# it saves the plots to disk instead of displaying them.\n",
    "!python finetune_bert.py"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
