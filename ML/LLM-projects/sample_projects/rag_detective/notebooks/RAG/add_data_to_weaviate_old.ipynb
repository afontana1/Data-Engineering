{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "934520c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'websiteAddress', 'dataType': ['string'], 'description': 'The address of the website.', 'indexInverted': True}, {'name': 'scrapeSessions', 'dataType': ['ScrapeSession'], 'description': 'The scrape sessions associated with the website.'}]\n"
     ]
    }
   ],
   "source": [
    "schema = {\n",
    "    \"classes\": [\n",
    "        {\n",
    "            \"class\": \"TextChunk\",\n",
    "            \"description\": \"A segmented portion of text from a scraped webpage.\",\n",
    "            \"vectorizer\": \"text2vec-openai\",\n",
    "            \"moduleConfig\": {\n",
    "                \"generative-openai\": {\n",
    "                    \"model\": \"gpt-3.5-turbo\"\n",
    "                }\n",
    "            },\n",
    "            \"properties\": [\n",
    "                {\n",
    "                    \"name\": \"key\",\n",
    "                    \"dataType\": [\"string\"],\n",
    "                    \"description\": \"The identifier for the text chunk.\",\n",
    "                    \"indexInverted\": True\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"text\",\n",
    "                    \"dataType\": [\"string\"],\n",
    "                    \"description\": \"The content of the text chunk.\",\n",
    "                    \"indexInverted\": True\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"class\": \"Page\",\n",
    "            \"description\": \"Details of a specific webpage scraped during a session.\",\n",
    "            \"properties\": [\n",
    "                {\n",
    "                    \"name\": \"pageURL\",\n",
    "                    \"dataType\": [\"string\"],\n",
    "                    \"description\": \"The specific URL of the scraped webpage.\",\n",
    "                    \"indexInverted\": True\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"chunks\",\n",
    "                    \"dataType\": [\"TextChunk\"],\n",
    "                    \"description\": \"Segmented content chunks from the scraped page.\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"class\": \"ScrapeSession\",\n",
    "            \"description\": \"Represents a specific scrape event at a certain timestamp.\",\n",
    "            \"invertedIndexConfig\": {\n",
    "                \"indexTimestamps\": True\n",
    "            },\n",
    "            \"properties\": [\n",
    "                {\n",
    "                    \"name\": \"timestamp\",\n",
    "                    \"dataType\": [\"date\"],\n",
    "                    \"description\": \"The date and time when the scrape session occurred.\",\n",
    "                    \"indexInverted\": True\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"pages\",\n",
    "                    \"dataType\": [\"Page\"],\n",
    "                    \"description\": \"Webpages scraped during the session.\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"class\": \"Website\",\n",
    "            \"description\": \"Represents a website which can have multiple scrape sessions.\",\n",
    "            \"properties\": [\n",
    "                {\n",
    "                    \"name\": \"websiteAddress\",\n",
    "                    \"dataType\": [\"string\"],\n",
    "                    \"description\": \"The address of the website.\",\n",
    "                    \"indexInverted\": True\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"scrapeSessions\",\n",
    "                    \"dataType\": [\"ScrapeSession\"],\n",
    "                    \"description\": \"The scrape sessions associated with the website.\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# To print, for example, the properties of the Website class:\n",
    "print(schema[\"classes\"][-1][\"properties\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04fcac3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema was created.\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "import csv\n",
    "from os import listdir\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def create_date(date_string):\n",
    "    # convert it to datetime object\n",
    "    dt_object = datetime.strptime(date_string, \"%Y-%m-%dT%H-%M-%S\")\n",
    "    # convert datetime object to RFC 3339 string (with timezone)\n",
    "    rfc3339_string = dt_object.replace(tzinfo=timezone.utc).isoformat()\n",
    "    return rfc3339_string\n",
    "\n",
    "\n",
    "# Initialize Weaviate client\n",
    "client = weaviate.Client(\n",
    "    url=\"http://34.66.77.236:8080\",\n",
    "    additional_headers={\n",
    "        \"X-OPENAI-Api-Key\": \"sk-OPEN_AI_KEY\",  # Replace with your OpenAI key\n",
    "    }\n",
    ")\n",
    "\n",
    "# Delete existing schema (caution: this deletes the current structure)\n",
    "client.schema.delete_all()\n",
    "\n",
    "# Here we use the schema created in the previous cell.\n",
    "client.schema.create(schema)\n",
    "print(\"Schema was created.\")\n",
    "\n",
    "# Function to load data from CSV and extract website name and timestamp from filename\n",
    "def load_csv_data(directory):\n",
    "    all_data = []\n",
    "    for filename in listdir(directory):\n",
    "        if filename.endswith('.csv'):\n",
    "            website_name, timestamp = filename.rsplit('.', 1)[0].split('_')\n",
    "            timestamp = create_date(timestamp)\n",
    "            with open(f\"{directory}/{filename}\", mode='r') as file:\n",
    "                reader = csv.DictReader(file)\n",
    "                for row in reader:\n",
    "                    row['websiteAddress'] = website_name\n",
    "                    row['timestamp'] = timestamp\n",
    "                    all_data.append(row)\n",
    "    return all_data\n",
    "\n",
    "# Load CSV data. TODO: Scraper needs to be modified to save CSVs here\n",
    "data_directory = '../data'\n",
    "\n",
    "csv_data = load_csv_data(data_directory)\n",
    "\n",
    "text_chunk_size = 500\n",
    "\n",
    "def split_into_chunks(string, text_chunk_size):\n",
    "    words = string.split()\n",
    "    chunks = []\n",
    "    chunk = \"\"\n",
    "    \n",
    "    # Split into chunks\n",
    "    for idx, word in enumerate(words):\n",
    "        if idx % text_chunk_size == 0 and idx > 0:\n",
    "            chunks.append(chunk.strip())\n",
    "            chunk = \"\"\n",
    "        chunk += word + \" \"\n",
    "    \n",
    "    # Add last chunk to list by concatenating with last chunk in the list\n",
    "    if chunk and len(chunks) > 0:\n",
    "        chunks[-1] += chunk.strip()\n",
    "        # chunks.append(chunk.strip())\n",
    "        \n",
    "    # If there's only one chunk, return list with one chunk\n",
    "    elif chunk:\n",
    "        chunks.append(chunk.strip())\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb781fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77144adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "www.chooch.com\n",
      "{'data': {'Get': {'ScrapeSession': []}}}\n"
     ]
    }
   ],
   "source": [
    "from weaviate.batch import Batch \n",
    "from weaviate.util import generate_uuid5 \n",
    "\n",
    "# create a \"Website\" for the website\n",
    "csv_website_address = csv_data[0]['websiteAddress']\n",
    "\n",
    "print(csv_website_address)\n",
    "\n",
    "websites_in_weaviate = client.query.raw(f\"\"\"\n",
    "{{\n",
    "  Get {{\n",
    "    Website(where: {{path: [\"websiteAddress\"], operator: Equal, valueString: \"{csv_website_address}\"}}){{\n",
    "      websiteAddress\n",
    "    }}\n",
    "  }}\n",
    "}}\n",
    "\"\"\")\n",
    "\n",
    "if len(websites_in_weaviate['data']['Get']['Website'])==0:\n",
    "    website_uuid = generate_uuid5(csv_website_address)\n",
    "    client.data_object.create({\"websiteAddress\": csv_website_address}, \"Website\", website_uuid)\n",
    "\n",
    "# Create a \"ScrapeSession\" for the session\n",
    "csv_timestamp = csv_data[0]['timestamp']  # assuming csv_data[0] has the timestamp\n",
    "scrapesessions_in_weaviate = client.query.raw(f\"\"\"\n",
    "{{\n",
    "  Get {{\n",
    "    ScrapeSession(where: {{path: [\"timestamp\"], operator: Equal, valueDate: \"{csv_timestamp}\"}}) {{\n",
    "      timestamp\n",
    "    }}\n",
    "  }}\n",
    "}}\n",
    "\"\"\")\n",
    "\n",
    "print(scrapesessions_in_weaviate)\n",
    "\n",
    "if len(scrapesessions_in_weaviate['data']['Get']['ScrapeSession']) == 0:\n",
    "    scrapesession_uuid = generate_uuid5(csv_timestamp)\n",
    "    client.data_object.create({\"timestamp\": csv_timestamp}, \"ScrapeSession\", scrapesession_uuid)\n",
    "    \n",
    "    # Add to_object_class_name='Website' as an argument\n",
    "    client.batch.add_reference(\n",
    "        from_object_uuid=website_uuid, \n",
    "        from_object_class_name='Website',\n",
    "        from_property_name='scrapeSessions',\n",
    "        to_object_uuid=scrapesession_uuid,\n",
    "        to_object_class_name='ScrapeSession'\n",
    "    )\n",
    "\n",
    "# For each entry in the list csv_data\n",
    "for data in csv_data:\n",
    "    # Create a \"Page\" entry for the webpage\n",
    "    pageAddress = data['page']\n",
    "    page_uuid = generate_uuid5(pageAddress)\n",
    "\n",
    "    client.data_object.create({\"pageURL\": pageAddress}, \"Page\", page_uuid)\n",
    "\n",
    "    # Add 'ScrapeSession' into to_object_class_name\n",
    "    client.batch.add_reference(\n",
    "        from_object_uuid=scrapesession_uuid,\n",
    "        from_object_class_name='ScrapeSession',\n",
    "        from_property_name='pages',\n",
    "        to_object_uuid=page_uuid,\n",
    "        to_object_class_name='Page'\n",
    "    )\n",
    "\n",
    "    # Split the text into chunks and create TextChunks\n",
    "    chunks = split_into_chunks(data['text'], text_chunk_size)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        textchunk_uuid = generate_uuid5(chunk)\n",
    "        client.data_object.create({\"text\": chunk}, \"TextChunk\", textchunk_uuid)\n",
    "\n",
    "        # Add 'Page' into to_object_class_name\n",
    "        client.batch.add_reference(\n",
    "            from_object_uuid=page_uuid,\n",
    "            from_object_class_name='Page',\n",
    "            from_property_name='chunks',\n",
    "            to_object_uuid=textchunk_uuid,\n",
    "            to_object_class_name='TextChunk'\n",
    "        )\n",
    "\n",
    "# Submit the batch to weaviate\n",
    "# status_objects = client.batch.create_objects()\n",
    "# status_references = client.batch.create_references()\n",
    "\n",
    "# Flush to Weaviate\n",
    "client.batch.configure(batch_size=20)\n",
    "client.batch.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "276f0ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classes': [{'class': 'TextChunk',\n",
       "   'description': 'A segmented portion of text from a scraped webpage.',\n",
       "   'invertedIndexConfig': {'bm25': {'b': 0.75, 'k1': 1.2},\n",
       "    'cleanupIntervalSeconds': 60,\n",
       "    'stopwords': {'additions': None, 'preset': 'en', 'removals': None}},\n",
       "   'moduleConfig': {'generative-openai': {'model': 'gpt-3.5-turbo'},\n",
       "    'text2vec-openai': {'model': 'ada',\n",
       "     'modelVersion': '002',\n",
       "     'type': 'text',\n",
       "     'vectorizeClassName': True}},\n",
       "   'multiTenancyConfig': {'enabled': False},\n",
       "   'properties': [{'dataType': ['text'],\n",
       "     'description': 'The identifier for the text chunk.',\n",
       "     'indexFilterable': True,\n",
       "     'indexSearchable': True,\n",
       "     'moduleConfig': {'text2vec-openai': {'skip': False,\n",
       "       'vectorizePropertyName': False}},\n",
       "     'name': 'key',\n",
       "     'tokenization': 'whitespace'},\n",
       "    {'dataType': ['text'],\n",
       "     'description': 'The content of the text chunk.',\n",
       "     'indexFilterable': True,\n",
       "     'indexSearchable': True,\n",
       "     'moduleConfig': {'text2vec-openai': {'skip': False,\n",
       "       'vectorizePropertyName': False}},\n",
       "     'name': 'text',\n",
       "     'tokenization': 'whitespace'}],\n",
       "   'replicationConfig': {'factor': 1},\n",
       "   'shardingConfig': {'virtualPerPhysical': 128,\n",
       "    'desiredCount': 1,\n",
       "    'actualCount': 1,\n",
       "    'desiredVirtualCount': 128,\n",
       "    'actualVirtualCount': 128,\n",
       "    'key': '_id',\n",
       "    'strategy': 'hash',\n",
       "    'function': 'murmur3'},\n",
       "   'vectorIndexConfig': {'skip': False,\n",
       "    'cleanupIntervalSeconds': 300,\n",
       "    'maxConnections': 64,\n",
       "    'efConstruction': 128,\n",
       "    'ef': -1,\n",
       "    'dynamicEfMin': 100,\n",
       "    'dynamicEfMax': 500,\n",
       "    'dynamicEfFactor': 8,\n",
       "    'vectorCacheMaxObjects': 1000000000000,\n",
       "    'flatSearchCutoff': 40000,\n",
       "    'distance': 'cosine',\n",
       "    'pq': {'enabled': False,\n",
       "     'bitCompression': False,\n",
       "     'segments': 0,\n",
       "     'centroids': 256,\n",
       "     'trainingLimit': 100000,\n",
       "     'encoder': {'type': 'kmeans', 'distribution': 'log-normal'}}},\n",
       "   'vectorIndexType': 'hnsw',\n",
       "   'vectorizer': 'text2vec-openai'},\n",
       "  {'class': 'Page',\n",
       "   'description': 'Details of a specific webpage scraped during a session.',\n",
       "   'invertedIndexConfig': {'bm25': {'b': 0.75, 'k1': 1.2},\n",
       "    'cleanupIntervalSeconds': 60,\n",
       "    'stopwords': {'additions': None, 'preset': 'en', 'removals': None}},\n",
       "   'moduleConfig': {'text2vec-openai': {'model': 'ada',\n",
       "     'modelVersion': '002',\n",
       "     'type': 'text',\n",
       "     'vectorizeClassName': True}},\n",
       "   'multiTenancyConfig': {'enabled': False},\n",
       "   'properties': [{'dataType': ['text'],\n",
       "     'description': 'The specific URL of the scraped webpage.',\n",
       "     'indexFilterable': True,\n",
       "     'indexSearchable': True,\n",
       "     'moduleConfig': {'text2vec-openai': {'skip': False,\n",
       "       'vectorizePropertyName': False}},\n",
       "     'name': 'pageURL',\n",
       "     'tokenization': 'whitespace'},\n",
       "    {'dataType': ['TextChunk'],\n",
       "     'description': 'Segmented content chunks from the scraped page.',\n",
       "     'indexFilterable': True,\n",
       "     'indexSearchable': False,\n",
       "     'moduleConfig': {'text2vec-openai': {'skip': False,\n",
       "       'vectorizePropertyName': False}},\n",
       "     'name': 'chunks'}],\n",
       "   'replicationConfig': {'factor': 1},\n",
       "   'shardingConfig': {'virtualPerPhysical': 128,\n",
       "    'desiredCount': 1,\n",
       "    'actualCount': 1,\n",
       "    'desiredVirtualCount': 128,\n",
       "    'actualVirtualCount': 128,\n",
       "    'key': '_id',\n",
       "    'strategy': 'hash',\n",
       "    'function': 'murmur3'},\n",
       "   'vectorIndexConfig': {'skip': False,\n",
       "    'cleanupIntervalSeconds': 300,\n",
       "    'maxConnections': 64,\n",
       "    'efConstruction': 128,\n",
       "    'ef': -1,\n",
       "    'dynamicEfMin': 100,\n",
       "    'dynamicEfMax': 500,\n",
       "    'dynamicEfFactor': 8,\n",
       "    'vectorCacheMaxObjects': 1000000000000,\n",
       "    'flatSearchCutoff': 40000,\n",
       "    'distance': 'cosine',\n",
       "    'pq': {'enabled': False,\n",
       "     'bitCompression': False,\n",
       "     'segments': 0,\n",
       "     'centroids': 256,\n",
       "     'trainingLimit': 100000,\n",
       "     'encoder': {'type': 'kmeans', 'distribution': 'log-normal'}}},\n",
       "   'vectorIndexType': 'hnsw',\n",
       "   'vectorizer': 'text2vec-openai'},\n",
       "  {'class': 'ScrapeSession',\n",
       "   'description': 'Represents a specific scrape event at a certain timestamp.',\n",
       "   'invertedIndexConfig': {'bm25': {'b': 0.75, 'k1': 1.2},\n",
       "    'cleanupIntervalSeconds': 60,\n",
       "    'indexTimestamps': True,\n",
       "    'stopwords': {'additions': None, 'preset': 'en', 'removals': None}},\n",
       "   'moduleConfig': {'text2vec-openai': {'model': 'ada',\n",
       "     'modelVersion': '002',\n",
       "     'type': 'text',\n",
       "     'vectorizeClassName': True}},\n",
       "   'multiTenancyConfig': {'enabled': False},\n",
       "   'properties': [{'dataType': ['date'],\n",
       "     'description': 'The date and time when the scrape session occurred.',\n",
       "     'indexFilterable': True,\n",
       "     'indexSearchable': False,\n",
       "     'moduleConfig': {'text2vec-openai': {'skip': False,\n",
       "       'vectorizePropertyName': False}},\n",
       "     'name': 'timestamp'},\n",
       "    {'dataType': ['Page'],\n",
       "     'description': 'Webpages scraped during the session.',\n",
       "     'indexFilterable': True,\n",
       "     'indexSearchable': False,\n",
       "     'moduleConfig': {'text2vec-openai': {'skip': False,\n",
       "       'vectorizePropertyName': False}},\n",
       "     'name': 'pages'}],\n",
       "   'replicationConfig': {'factor': 1},\n",
       "   'shardingConfig': {'virtualPerPhysical': 128,\n",
       "    'desiredCount': 1,\n",
       "    'actualCount': 1,\n",
       "    'desiredVirtualCount': 128,\n",
       "    'actualVirtualCount': 128,\n",
       "    'key': '_id',\n",
       "    'strategy': 'hash',\n",
       "    'function': 'murmur3'},\n",
       "   'vectorIndexConfig': {'skip': False,\n",
       "    'cleanupIntervalSeconds': 300,\n",
       "    'maxConnections': 64,\n",
       "    'efConstruction': 128,\n",
       "    'ef': -1,\n",
       "    'dynamicEfMin': 100,\n",
       "    'dynamicEfMax': 500,\n",
       "    'dynamicEfFactor': 8,\n",
       "    'vectorCacheMaxObjects': 1000000000000,\n",
       "    'flatSearchCutoff': 40000,\n",
       "    'distance': 'cosine',\n",
       "    'pq': {'enabled': False,\n",
       "     'bitCompression': False,\n",
       "     'segments': 0,\n",
       "     'centroids': 256,\n",
       "     'trainingLimit': 100000,\n",
       "     'encoder': {'type': 'kmeans', 'distribution': 'log-normal'}}},\n",
       "   'vectorIndexType': 'hnsw',\n",
       "   'vectorizer': 'text2vec-openai'},\n",
       "  {'class': 'Website',\n",
       "   'description': 'Represents a website which can have multiple scrape sessions.',\n",
       "   'invertedIndexConfig': {'bm25': {'b': 0.75, 'k1': 1.2},\n",
       "    'cleanupIntervalSeconds': 60,\n",
       "    'stopwords': {'additions': None, 'preset': 'en', 'removals': None}},\n",
       "   'moduleConfig': {'text2vec-openai': {'model': 'ada',\n",
       "     'modelVersion': '002',\n",
       "     'type': 'text',\n",
       "     'vectorizeClassName': True}},\n",
       "   'multiTenancyConfig': {'enabled': False},\n",
       "   'properties': [{'dataType': ['text'],\n",
       "     'description': 'The address of the website.',\n",
       "     'indexFilterable': True,\n",
       "     'indexSearchable': True,\n",
       "     'moduleConfig': {'text2vec-openai': {'skip': False,\n",
       "       'vectorizePropertyName': False}},\n",
       "     'name': 'websiteAddress',\n",
       "     'tokenization': 'whitespace'},\n",
       "    {'dataType': ['ScrapeSession'],\n",
       "     'description': 'The scrape sessions associated with the website.',\n",
       "     'indexFilterable': True,\n",
       "     'indexSearchable': False,\n",
       "     'moduleConfig': {'text2vec-openai': {'skip': False,\n",
       "       'vectorizePropertyName': False}},\n",
       "     'name': 'scrapeSessions'}],\n",
       "   'replicationConfig': {'factor': 1},\n",
       "   'shardingConfig': {'virtualPerPhysical': 128,\n",
       "    'desiredCount': 1,\n",
       "    'actualCount': 1,\n",
       "    'desiredVirtualCount': 128,\n",
       "    'actualVirtualCount': 128,\n",
       "    'key': '_id',\n",
       "    'strategy': 'hash',\n",
       "    'function': 'murmur3'},\n",
       "   'vectorIndexConfig': {'skip': False,\n",
       "    'cleanupIntervalSeconds': 300,\n",
       "    'maxConnections': 64,\n",
       "    'efConstruction': 128,\n",
       "    'ef': -1,\n",
       "    'dynamicEfMin': 100,\n",
       "    'dynamicEfMax': 500,\n",
       "    'dynamicEfFactor': 8,\n",
       "    'vectorCacheMaxObjects': 1000000000000,\n",
       "    'flatSearchCutoff': 40000,\n",
       "    'distance': 'cosine',\n",
       "    'pq': {'enabled': False,\n",
       "     'bitCompression': False,\n",
       "     'segments': 0,\n",
       "     'centroids': 256,\n",
       "     'trainingLimit': 100000,\n",
       "     'encoder': {'type': 'kmeans', 'distribution': 'log-normal'}}},\n",
       "   'vectorIndexType': 'hnsw',\n",
       "   'vectorizer': 'text2vec-openai'}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.schema.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a7e962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm data was properly loaded\n",
    "query = \"\"\"\n",
    "{\n",
    "  Get {\n",
    "    Website {\n",
    "      websiteAddress\n",
    "      scrapeSessions {\n",
    "        ... on ScrapeSession {\n",
    "          pages {\n",
    "            ... on Page {\n",
    "              pageURL\n",
    "              chunks {\n",
    "                ... on TextChunk {\n",
    "                  text\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "result = client.query.raw(query)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7ed9fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setup the query for the Website class and run it\n",
    "# query_result = (\n",
    "#     client.query\n",
    "#     .get(\"Website\", [\"websiteAddress\", \"scrapeSessions { timestamp, pages { pageURL, chunks { key, text } } }\"])  # specify the properties you're interested in here\n",
    "#     .with_where(main_filter)\n",
    "#     .with_near_text({\"concepts\": [search_text]})\n",
    "#     .do()\n",
    "# )\n",
    "\n",
    "# print(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3e1124b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'errors': [{'locations': [{'column': 320, 'line': 1}], 'message': 'Cannot query field \"timestamp\" on type \"WebsiteScrapeSessionsObj\". Did you mean to use an inline fragment on \"ScrapeSession\"?', 'path': None}, {'locations': [{'column': 331, 'line': 1}], 'message': 'Cannot query field \"pages\" on type \"WebsiteScrapeSessionsObj\". Did you mean to use an inline fragment on \"ScrapeSession\"?', 'path': None}]}\n"
     ]
    }
   ],
   "source": [
    "search_text = \"What is computer vision?\"\n",
    "\n",
    "# Define the where filter for websiteAddress\n",
    "website_addr_filter = {\n",
    "    \"path\": [\"websiteAddress\"],\n",
    "    \"operator\": \"Equal\",\n",
    "    \"valueString\": \"www.chooch.com\"   # Assuming websiteAddress is type string\n",
    "}\n",
    "\n",
    "# Define the where filter for timestamp\n",
    "timestamp_filter = {\n",
    "    \"path\": [\"scrapeSessions\", \"ScrapeSession\", \"timestamp\"], # Following the property path\n",
    "    \"operator\": \"Equal\",\n",
    "    \"valueDate\": \"2023-10-03T15:30:00+00:00\"   # Assuming timestamp is type date\n",
    "}\n",
    "\n",
    "# Define the main where filter object\n",
    "main_filter = {\n",
    "    \"operator\": \"And\",\n",
    "    \"operands\": [\n",
    "        website_addr_filter,\n",
    "        timestamp_filter\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Setup the query for the Website class and run it\n",
    "query_result = (\n",
    "    client.query\n",
    "    .get(\"Website\", [\"websiteAddress\", \"scrapeSessions { timestamp, pages { pageURL, chunks { key, text } } }\"])  # specify the properties you're interested in here\n",
    "    .with_where(main_filter)\n",
    "    .with_near_text({\"concepts\": [search_text]})\n",
    "    .do()\n",
    ")\n",
    "\n",
    "print(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57efd47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
