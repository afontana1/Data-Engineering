{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e220e70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01ff872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import ChromiumOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0d4c326",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cffc56cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_chrome_options() -> ChromiumOptions:\n",
    "    \"\"\"Sets chrome options for Selenium.Chrome options for headless browser is enabled.\n",
    "    Args: None\n",
    "    \n",
    "    returns:\n",
    "        Chrome options that can work headless i.e. without actually launching the browser.\n",
    "    \"\"\"\n",
    "    chrome_options = ChromiumOptions()\n",
    "    chrome_options.add_argument(\"--headless=new\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_prefs = {}\n",
    "    chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "    chrome_prefs[\"profile.default_content_settings\"] = {\"images\": 2}\n",
    "    return chrome_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e9c37fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_sitemap(url):\n",
    "    \"\"\"\n",
    "    Extracts all the links from a company's sitemap.xml.\n",
    "    \n",
    "    Args:\n",
    "    url (str): The URL for a company sitemap.xml.\n",
    "    \n",
    "    Returns:\n",
    "    pd.Series: A pandas Series with all the links, or None if no links found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with requests.get(url, headers=headers) as response:\n",
    "            response.raise_for_status()  # Check if the request was successful\n",
    "            soup = BeautifulSoup(response.text, 'lxml-xml')\n",
    "            urls = [link.text.strip() for link in soup.find_all('loc') if link]\n",
    "            \n",
    "            if not urls:\n",
    "                return None  # Return None if no URLs found\n",
    "            \n",
    "            extended_urls = []\n",
    "            for link in urls:\n",
    "                if link.endswith('xml'):\n",
    "                    try:\n",
    "                        with requests.get(link, headers=headers) as response:\n",
    "                            response.raise_for_status()  # Check if the request was successful\n",
    "                            nested_soup = BeautifulSoup(response.text, 'lxml')\n",
    "                            nested_urls = [url.text.strip() for url in nested_soup.find_all('loc') if url]\n",
    "                            extended_urls.extend(nested_urls)\n",
    "                    except requests.RequestException as e:\n",
    "                        print(f\"Error occurred while processing {link}: {e}\")\n",
    "                else:\n",
    "                    extended_urls.append(link)\n",
    "            \n",
    "            if not extended_urls:\n",
    "                return None  # Return None if no extended URLs found\n",
    "            \n",
    "            return pd.Series(extended_urls).drop_duplicates().str.strip()\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        return None  # Return None if the initial request fails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9fe28c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_website(all_links, options):\n",
    "    \"\"\"\n",
    "    Extracts all the text data from the webpages of a company.\n",
    "\n",
    "    Args:\n",
    "    all_links (pd.Series): A pandas Series with all the links in the company's website.\n",
    "    options: Chrome options to apply to the browser\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A pandas DataFrame with the columns 'key' (webpage link), 'text' (includes \n",
    "                  the text of the webpage), and 'timestamp' (when the data was scraped).\n",
    "    \n",
    "    pd.DataFrame : A pandas dataframe with columns 'key' (webpage link), 'error with timestamp' .\n",
    "    \"\"\"\n",
    "    log_dict = {}\n",
    "    text_dict = {}\n",
    "    wait_condition = (By.TAG_NAME,['html','div','body'])\n",
    "\n",
    "    for link in all_links.to_list():\n",
    "        try:\n",
    "            # First, scrape the page using requests\n",
    "            with requests.get(link, headers=headers) as response:\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.text, 'lxml')\n",
    "                    \n",
    "                    [tag.decompose() for tag in soup.find_all(['header', 'nav', 'footer'])]\n",
    "                    text_only_requests = soup.get_text(separator=' ', strip=True)\n",
    "                    print(link)\n",
    "                \n",
    "                # If content seems too short or response code is not 200, use Selenium\n",
    "                if response.status_code != 200 or len(text_only_requests.split()) < 50:\n",
    "                    print(\"using selenium to scrape..\\n\")\n",
    "                    try:\n",
    "                        browser = webdriver.Chrome(options)\n",
    "                        browser.get(link)\n",
    "                        wait = WebDriverWait(browser, timeout=30)\n",
    "\n",
    "                        wait.until(lambda d: browser.find_element(By.TAG_NAME, 'html').is_displayed() \\\n",
    "                                   if browser.find_element(By.TAG_NAME, 'html') else True &\\\n",
    "                                   \n",
    "                                  np.all(np.array([i.is_displayed() \\\n",
    "                                                       for i in browser.find_elements(By.TAG_NAME, 'div')])) \\\n",
    "                                   \n",
    "                                   if browser.find_elements(By.TAG_NAME, 'div') else True)\n",
    "                                   \n",
    "                               \n",
    "                                  \n",
    "                        soup_selenium = BeautifulSoup(browser.page_source, 'lxml')\n",
    "\n",
    "                        [tag.decompose() for tag in soup_selenium.find_all(['header', 'nav', 'footer'])]\n",
    "                        text_only_selenium = soup_selenium.get_text(separator=' ', strip=True).lower()\n",
    "\n",
    "                        text_dict[link] = text_only_selenium\n",
    "                        \n",
    "                        if len(text_only_selenium.lower().split()) < 20:\n",
    "                            log_dict[link] = str(pd.to_datetime(datetime.today().date())) + \\\n",
    "                            \" \"+text_only_selenium\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error occurred while processing {link} in selenium: {e.with_traceback}\")\n",
    "                        log_dict[link] = f'{pd.to_datetime(datetime.today().date())}  {e.with_traceback}'\n",
    "                       \n",
    "                    finally:\n",
    "                        browser.close()\n",
    "                                \n",
    "                else:\n",
    "                    text_dict[link] = text_only_requests.lower()\n",
    "        \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error occurred while processing {link}: {e}\")\n",
    "            log_dict[link] = f'pd.to_datetime(datetime.today().date())  {e.with_traceback}'\n",
    "    if not log_dict:\n",
    "        df_log = pd.DataFrame()\n",
    "    else:\n",
    "        \n",
    "        df_log = pd.DataFrame(list(log_dict.items()), columns=['key', 'error'])\n",
    "    \n",
    "    if not text_dict:\n",
    "        return pd.DataFrame(), df_log   # return empty DataFrame if no text is extracted\n",
    "\n",
    "    df = pd.DataFrame(list(text_dict.items()), columns=['key', 'text'])    \n",
    "\n",
    "    return df, df_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "59c79437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Runs the scraping engine, extracts data from the specified sitemaps, and saves\n",
    "    the extracted data to a CSV file.\n",
    "    \n",
    "    The CSV file contains the scraped data from the first 10 webpages of each sitemap.\n",
    "    \"\"\"\n",
    "    p =Path('../../data')\n",
    "    path = str(p)\n",
    "    options = set_chrome_options()\n",
    "    try:\n",
    "        sitemap_df = pd.read_csv(\"sitemap.csv\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: The file 'sitemap.csv' was not found.\")\n",
    "        return\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"Error: The file 'sitemap.csv' is empty.\")\n",
    "        return\n",
    "\n",
    "    if 'sitemap' not in sitemap_df.columns:\n",
    "        print(\"Error: The expected 'sitemap' column is missing in 'sitemap.csv'.\")\n",
    "        return\n",
    "\n",
    "    print(\"Starting scraping:\\n\")\n",
    "    for index, item in enumerate(sitemap_df['sitemap'], start=1):\n",
    "        print(f\"Working on {item} ...\")\n",
    "        links = scrape_sitemap(item)\n",
    "        if links is None or links.empty:\n",
    "            print(f\"No links were found in {item}.\\n\")\n",
    "            continue\n",
    "\n",
    "        link_split = item.split('/')\n",
    "        if link_split:\n",
    "            website_name = link_split[2]\n",
    "        print(f\"{links.shape[0]} webpages found for scraping. For demo, scraping only the first 10 webpages.\\n\")\n",
    "        \n",
    "        scraped_df, log_df = scrape_website(links[:3], options)\n",
    "        if scraped_df.empty:\n",
    "            print(f\"No data was scraped from the first 10 links of {item}.\\n\")\n",
    "            continue\n",
    "            \n",
    "        split=str(datetime.now()).split()\n",
    "        date = str(split[0])\n",
    "        ms= str(split[1].split('.')[1])\n",
    "        timestamp = date+\"-\"+ms\n",
    "     \n",
    "        output_file = path+'/'+website_name+'_'+timestamp+'.csv'\n",
    "        print(output_file)\n",
    "        scraped_df.to_csv(output_file, index=False)\n",
    "        if not log_df.empty:\n",
    "            log_file = f'log/{website_name}_{timestamp}.csv'\n",
    "            log_df.to_csv(log_file, index=False)\n",
    "        \n",
    "        \n",
    "        print(f\"Finished scraping. Data stored in {output_file}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d0a4059f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scraping:\n",
      "\n",
      "Working on https://bland.ai/sitemap.xml ...\n",
      "12 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "http://www.bland.ai/\n",
      "http://www.bland.ai/blog\n",
      "https://www.bland.ai/blog/enterprise-ai-phone-call-use-cases\n",
      "../../data/bland.ai_2023-10-05-242917.csv\n",
      "Finished scraping. Data stored in ../../data/bland.ai_2023-10-05-242917.csv\n",
      "\n",
      "Working on https://cohere.com/sitemap.xml ...\n",
      "30 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://cohere.com/responsibility\n",
      "https://cohere.com/deployment-options/oracle\n",
      "https://cohere.com/news\n",
      "../../data/cohere.com_2023-10-05-839812.csv\n",
      "Finished scraping. Data stored in ../../data/cohere.com_2023-10-05-839812.csv\n",
      "\n",
      "Working on https://ai21.com/sitemap.xml ...\n",
      "91 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://www.ai21.com/\n",
      "https://www.ai21.com/about\n",
      "https://www.ai21.com/ai-co-writing\n",
      "../../data/ai21.com_2023-10-05-189942.csv\n",
      "Finished scraping. Data stored in ../../data/ai21.com_2023-10-05-189942.csv\n",
      "\n",
      "Working on https://descript.com/sitemap.xml ...\n",
      "187 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://www.descript.com/about\n",
      "https://www.descript.com/affiliate\n",
      "https://www.descript.com/affiliate-terms\n",
      "../../data/descript.com_2023-10-05-219526.csv\n",
      "Finished scraping. Data stored in ../../data/descript.com_2023-10-05-219526.csv\n",
      "\n",
      "Working on https://weaviate.io/sitemap.xml ...\n",
      "335 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://weaviate.io/blog\n",
      "https://weaviate.io/blog/ann-algorithms-hnsw-pq\n",
      "https://weaviate.io/blog/ann-algorithms-tiles-enocoder\n",
      "../../data/weaviate.io_2023-10-05-114024.csv\n",
      "Finished scraping. Data stored in ../../data/weaviate.io_2023-10-05-114024.csv\n",
      "\n",
      "Working on https://assemblyai.com/sitemap.xml ...\n",
      "17 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://www.assemblyai.com/\n",
      "https://www.assemblyai.com/models/core-transcription\n",
      "https://www.assemblyai.com/models/audio-intelligence\n",
      "../../data/assemblyai.com_2023-10-05-622865.csv\n",
      "Finished scraping. Data stored in ../../data/assemblyai.com_2023-10-05-622865.csv\n",
      "\n",
      "Working on https://anthropic.com/sitemap.xml ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riku/miniconda3/envs/cs115/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://www.anthropic.com/amazon-bedrock\n",
      "https://www.anthropic.com/claude-in-slack\n",
      "https://www.anthropic.com/claude-in-slack/support\n",
      "../../data/anthropic.com_2023-10-05-373333.csv\n",
      "Finished scraping. Data stored in ../../data/anthropic.com_2023-10-05-373333.csv\n",
      "\n",
      "Working on https://inflection.ai/sitemap.xml ...\n",
      "15 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://inflection.ai\n",
      "https://inflection.ai/about\n",
      "https://inflection.ai/safety\n",
      "../../data/inflection.ai_2023-10-05-219168.csv\n",
      "Finished scraping. Data stored in ../../data/inflection.ai_2023-10-05-219168.csv\n",
      "\n",
      "Working on https://h2o.ai/sitemap.xml ) ...\n",
      "983 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://h2o.ai/\n",
      "https://h2o.ai/br/\n",
      "https://h2o.ai/br/agradecemos-seu-contato/\n",
      "../../data/h2o.ai_2023-10-05-725298.csv\n",
      "Finished scraping. Data stored in ../../data/h2o.ai_2023-10-05-725298.csv\n",
      "\n",
      "Working on https://harver.com/sitemap_index.xml  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riku/miniconda3/envs/cs115/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1004 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://harver.com/blog/\n",
      "https://harver.com/blog/hr-tech-5-skills-modern-day-hr-professionals-need/\n",
      "https://harver.com/blog/hr-tech-5-recruitment-hacks/\n",
      "../../data/harver.com_2023-10-05-178292.csv\n",
      "Finished scraping. Data stored in ../../data/harver.com_2023-10-05-178292.csv\n",
      "\n",
      "Working on https://dataminr.com/sitemap.xml ...\n",
      "451 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://www.dataminr.com/blog/dataminr-celebrates-pride-month\n",
      "https://www.dataminr.com/resources/storms-sweep-the-southeast\n",
      "https://www.dataminr.com/press/embracing-broad-data-sets-protecting-brand-reputation-at-the-world-cup\n",
      "../../data/dataminr.com_2023-10-05-411142.csv\n",
      "Finished scraping. Data stored in ../../data/dataminr.com_2023-10-05-411142.csv\n",
      "\n",
      "Working on https://shield.ai/sitemap_index.xml ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riku/miniconda3/envs/cs115/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://shield.ai/blog/\n",
      "https://shield.ai/america-must-accelerate-scale-and-innovate-win-future-wars/\n",
      "https://shield.ai/on-overcoming-obstacles/\n",
      "../../data/shield.ai_2023-10-05-657874.csv\n",
      "Finished scraping. Data stored in ../../data/shield.ai_2023-10-05-657874.csv\n",
      "\n",
      "Working on https://kymeratx.com/sitemap.xml ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riku/miniconda3/envs/cs115/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://www.kymeratx.com/\n",
      "https://www.kymeratx.com/terms/\n",
      "https://www.kymeratx.com/privacy-policy/\n",
      "../../data/kymeratx.com_2023-10-05-299174.csv\n",
      "Finished scraping. Data stored in ../../data/kymeratx.com_2023-10-05-299174.csv\n",
      "\n",
      "Working on https://arvinas.com/sitemap.xml ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riku/miniconda3/envs/cs115/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://www.arvinas.com/\n",
      "https://www.arvinas.com/terms-of-use/\n",
      "https://www.arvinas.com/patients/\n",
      "../../data/arvinas.com_2023-10-05-949037.csv\n",
      "Finished scraping. Data stored in ../../data/arvinas.com_2023-10-05-949037.csv\n",
      "\n",
      "Working on https://ardelyx.com/sitemap.xml ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riku/miniconda3/envs/cs115/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://ardelyx.com/\n",
      "https://ardelyx.com/our-pipeline/\n",
      "https://ardelyx.com/products/\n",
      "../../data/ardelyx.com_2023-10-05-267887.csv\n",
      "Finished scraping. Data stored in ../../data/ardelyx.com_2023-10-05-267887.csv\n",
      "\n",
      "Working on https://monterosatx.com/sitemap.xml ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riku/miniconda3/envs/cs115/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://www.monterosatx.com/news/monte-rosa-therapeutics-strengthens-leadership-with-key-executive-and-board-appointments/\n",
      "https://www.monterosatx.com/news/monte-rosa-therapeutics-to-participate-in-upcoming-investor-conferences/\n",
      "https://www.monterosatx.com/news/monte-rosa-therapeutics-expands-senior-management-team/\n",
      "../../data/monterosatx.com_2023-10-05-817657.csv\n",
      "Finished scraping. Data stored in ../../data/monterosatx.com_2023-10-05-817657.csv\n",
      "\n",
      "Working on https://trianabio.com/sitemap.xml ...\n",
      "31 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://trianabio.com/press-release-4-6-2022\n",
      "https://trianabio.com/job-posting-senior-scientist-lead-discovery\n",
      "https://images.squarespace-cdn.com/content/v1/61d5e7afb1ff8b1bd358de61/5855e10d-77f0-42cc-aa0f-5d4792c9244a/TRIANA_ID_Logomark_72.png\n",
      "../../data/trianabio.com_2023-10-05-639980.csv\n",
      "Finished scraping. Data stored in ../../data/trianabio.com_2023-10-05-639980.csv\n",
      "\n",
      "Working on https://tangotx.com/sitemap.xml ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riku/miniconda3/envs/cs115/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://www.tangotx.com/\n",
      "https://www.tangotx.com/science/our-approach/\n",
      "https://www.tangotx.com/pipeline/\n",
      "../../data/tangotx.com_2023-10-05-413127.csv\n",
      "Finished scraping. Data stored in ../../data/tangotx.com_2023-10-05-413127.csv\n",
      "\n",
      "Working on https://vertex.com/sitemap.xml ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riku/miniconda3/envs/cs115/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://www.vertex.com/our-blog-news-updates-trends-vertex-software/\n",
      "https://www.vertex.com/american-companies-saving-10m-annually-process-automation/\n",
      "https://www.vertex.com/hr-management-in-the-age-of-technology/\n",
      "../../data/vertex.com_2023-10-05-312111.csv\n",
      "Finished scraping. Data stored in ../../data/vertex.com_2023-10-05-312111.csv\n",
      "\n",
      "Working on https://vervetx.com/sitemap.xml ...\n",
      "28 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://www.vervetx.com/about-us/our-story\n",
      "https://www.vervetx.com/about-us/why-cardiovascular-disease\n",
      "https://www.vervetx.com/careers\n",
      "../../data/vervetx.com_2023-10-05-853328.csv\n",
      "Finished scraping. Data stored in ../../data/vervetx.com_2023-10-05-853328.csv\n",
      "\n",
      "Working on https://novonordisk.com/sitemap.xml ...\n",
      "318 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://www.novonordisk.com/\n",
      "https://www.novonordisk.com/disease-areas.html\n",
      "https://www.novonordisk.com/disease-areas/type-1-diabetes.html\n",
      "../../data/novonordisk.com_2023-10-05-636779.csv\n",
      "Finished scraping. Data stored in ../../data/novonordisk.com_2023-10-05-636779.csv\n",
      "\n",
      "Working on https://shionogi.com/us/en/sitemap.xml ...\n",
      "113 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "http://www.shionogi.com/us/en/\n",
      "http://www.shionogi.com/us/en/about.html\n",
      "http://www.shionogi.com/us/en/about/philosophy.html\n",
      "../../data/shionogi.com_2023-10-05-930947.csv\n",
      "Finished scraping. Data stored in ../../data/shionogi.com_2023-10-05-930947.csv\n",
      "\n",
      "Working on https://alexion.com/sitemap.xml ...\n",
      "Error occurred: 404 Client Error: Not Found for url: https://alexion.com/sitemap.xml\n",
      "No links were found in https://alexion.com/sitemap.xml.\n",
      "\n",
      "Working on https://relaytx.com/sitemap.xml  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riku/miniconda3/envs/cs115/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://relaytx.com/\n",
      "https://relaytx.com/terms-of-use/\n",
      "https://relaytx.com/privacy-policy/\n",
      "../../data/relaytx.com_2023-10-05-520388.csv\n",
      "Finished scraping. Data stored in ../../data/relaytx.com_2023-10-05-520388.csv\n",
      "\n",
      "Working on https://neumoratx.com/sitemap.xml ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riku/miniconda3/envs/cs115/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[102], line 27\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sitemap_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msitemap\u001b[39m\u001b[38;5;124m'\u001b[39m], start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorking on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m     links \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_sitemap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m links \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m links\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo links were found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[22], line 24\u001b[0m, in \u001b[0;36mscrape_sitemap\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m link\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxml\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[1;32m     25\u001b[0m             response\u001b[38;5;241m.\u001b[39mraise_for_status()  \u001b[38;5;66;03m# Check if the request was successful\u001b[39;00m\n\u001b[1;32m     26\u001b[0m             nested_soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlxml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs115/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs115/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs115/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/envs/cs115/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/envs/cs115/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs115/lib/python3.10/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs115/lib/python3.10/site-packages/urllib3/connectionpool.py:403\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs115/lib/python3.10/site-packages/urllib3/connectionpool.py:1053\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1053\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n\u001b[1;32m   1056\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1057\u001b[0m         (\n\u001b[1;32m   1058\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnverified HTTPS request is being made to host \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1063\u001b[0m         InsecureRequestWarning,\n\u001b[1;32m   1064\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/cs115/lib/python3.10/site-packages/urllib3/connection.py:419\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_certs\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_cert_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(context, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_default_certs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    416\u001b[0m ):\n\u001b[1;32m    417\u001b[0m     context\u001b[38;5;241m.\u001b[39mload_default_certs()\n\u001b[0;32m--> 419\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# If we're using all defaults and the connection\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# is TLSv1 or TLSv1.1 we throw a DeprecationWarning\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# for the host.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    436\u001b[0m     default_ssl_context\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock\u001b[38;5;241m.\u001b[39mversion() \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTLSv1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTLSv1.1\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m    440\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/cs115/lib/python3.10/site-packages/urllib3/util/ssl_.py:402\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ca_certs \u001b[38;5;129;01mor\u001b[39;00m ca_cert_dir \u001b[38;5;129;01mor\u001b[39;00m ca_cert_data:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 402\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_verify_locations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIOError\u001b[39;00m, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e87a1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2023, 10, 5, 18, 59, 24, 957549)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "861e4ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "www.chooch.com_2023-10-06T11-15-17.csv\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Get the timestamp string in the desired format\n",
    "timestamp = datetime.now().strftime('%Y-%m-%dT%H-%M-%S')\n",
    "\n",
    "website_name = 'www.chooch.com'\n",
    "\n",
    "# Create the filename\n",
    "filename = f\"{website_name}_{timestamp}.csv\"\n",
    "\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e298c37e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-10-05T19-00-37'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72ebfa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd6a9074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data\n"
     ]
    }
   ],
   "source": [
    "p = Path('../../data/')\n",
    "if  os.path.exists(str(p)):\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ab01d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   key\n",
       "0    1\n",
       "1    2\n",
       "2    3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =pd.DataFrame([1,2,3], columns=['key'])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4f48fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = a.to_csv(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "445d4bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'key\\n1\\n2\\n3\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b5f7cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9681602c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = str(Path('../../data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f81e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename =\"abc.com_111\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c298637c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../data/abc.com_111'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{path}/{filename}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0907c3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/abc.com_111'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'data/{filename}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6da84083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = str(Path('data/'))\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fff6056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('rag-detective-2ed9f2d52fde.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(\"./rag-detective-2ed9f2d52fde.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a701589d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/abc.com_111'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{path}/{filename}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cd84baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9e9a4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6090f40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export GOOGLE_APPLICATION_CREDENTIALS=\"./rag-detective-2ed9f2d52fde.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e8708ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    storage_client = storage.Client(project='rag-detective')\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d48563bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000</td>\n",
       "      <td>2000</td>\n",
       "      <td>3000</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      a     b     c     d\n",
       "0  <NA>     2     3     4\n",
       "1   100   200   300   400\n",
       "2  1000  2000  3000  4000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict = [{'a': pd.NA, 'b': 2, 'c': 3, 'd': 4},\n",
    "          {'a': 100, 'b': 200, 'c': 300, 'd': 400},\n",
    "          {'a': 1000, 'b': 2000, 'c': 3000, 'd': 4000 }]\n",
    "df = pd.DataFrame(mydict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b552ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a    <NA>\n",
       "b       2\n",
       "c       3\n",
       "d       4\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70d51365",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "889424df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([1, 2], dtype='int64')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
