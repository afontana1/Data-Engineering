{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t-bag/miniconda3/envs/rag_trustii/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from typing import Dict\n",
    "from initialize_rag import RAGInitializer\n",
    "\n",
    "def process_query(query: str,\n",
    "                 retriever,\n",
    "                 reranker,\n",
    "                 response_generator,\n",
    "                 process_config: Dict,\n",
    "                 send_nb_chunks_to_llm=2) -> Dict:\n",
    "    \"\"\"\n",
    "    Process a single query through the complete RAG pipeline.\n",
    "    \n",
    "    This function orchestrates the query processing workflow:\n",
    "    1. Optional query expansion\n",
    "    2. Document retrieval\n",
    "    3. Optional result reranking\n",
    "    4. Response generation\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user's query\n",
    "        retriever: Document retrieval component\n",
    "        reranker: Result reranking component\n",
    "        response_generator: Response generation component\n",
    "        process_config (Dict): Processing configuration\n",
    "        send_nb_chunks_to_llm (int): Number of chunks to send to LLM\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Processing results containing:\n",
    "            - Query: Original query\n",
    "            - Response: Generated response\n",
    "            - Score: Best retrieval/reranking score\n",
    "            \n",
    "    Note:\n",
    "        The function handles errors gracefully, returning an error message\n",
    "        in the response if any step fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Expand query if configured\n",
    "        if process_config['retrieval']['use_query_expansion']:\n",
    "            expanded_query = response_generator.expand_query(query)\n",
    "            logging.info(f\"Expanded query: {expanded_query}\")\n",
    "        else:\n",
    "            expanded_query = query\n",
    "            \n",
    "        # Retrieve relevant documents using expanded or original query\n",
    "        if process_config['retrieval']['use_bm25']:\n",
    "            retrieved_results = retriever.retrieve_with_method(\n",
    "                expanded_query,\n",
    "                method=\"hybrid\",\n",
    "                top_k=process_config['retrieval']['top_k']\n",
    "            )\n",
    "        else:\n",
    "            retrieved_results = retriever.retrieve_with_method(\n",
    "                expanded_query,\n",
    "                method=\"vector\",\n",
    "                top_k=process_config['retrieval']['top_k']\n",
    "            )\n",
    "        logging.info(f\"Retrieved {len(retrieved_results)} documents\")\n",
    "        \n",
    "        # Apply reranking if configured\n",
    "        if process_config['retrieval']['use_reranking']:\n",
    "            reranked_results = reranker.rerank(\n",
    "                query,\n",
    "                [r.document for r in retrieved_results],\n",
    "                top_k=send_nb_chunks_to_llm\n",
    "            )\n",
    "            relevant_docs = [r.document for r in reranked_results]\n",
    "            best_score = reranked_results[0].score if reranked_results else 0.0\n",
    "            logging.info(f\"Reranked results. Best score: {best_score}\")\n",
    "        else:\n",
    "            relevant_docs = [r.document for r in retrieved_results]\n",
    "            best_score = retrieved_results[0].score if retrieved_results else 0.0\n",
    "            logging.info(f\"Using retrieval scores. Best score: {best_score}\")\n",
    "        \n",
    "        # Generate final response using selected documents\n",
    "        response_data = response_generator.generate_answer(\n",
    "            query,\n",
    "            relevant_docs,\n",
    "            metadata={'retrieval_score': best_score}\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'Query': query,\n",
    "            'Response': response_data['response'],\n",
    "            'Score': best_score\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing query: {str(e)}\")\n",
    "        return {\n",
    "            'Query': query,\n",
    "            'Response': \"An error occurred processing your query.\",\n",
    "            'Score': 0.0\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_config_path = 'config/init_config.yaml'\n",
    "process_config_path = 'config/process_config.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 02:47:21,541 - root - INFO - Initializing components...\n",
      "2024-11-10 02:47:21,868 - datasets - INFO - PyTorch version 2.5.1+cu121 available.\n",
      "2024-11-10 02:47:21,929 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: nomic-ai/nomic-embed-text-v1.5\n",
      "2024-11-10 02:47:24,399 - transformers_modules.nomic-ai.nomic-bert-2048.c1b1fd7a715b8eb2e232d34593154ac782c98ac9.modeling_hf_nomic_bert - WARNING - <All keys matched successfully>\n",
      "2024-11-10 02:47:26,272 - src.models.reranker - INFO - Initialized reranker model: BAAI/bge-reranker-v2-m3\n",
      "2024-11-10 02:47:26,273 - root - INFO - Loading and preprocessing data...\n",
      "2024-11-10 02:47:26,273 - src.data.data_ingestion - INFO - Loading CSV data...\n",
      "2024-11-10 02:47:26,282 - src.data.data_ingestion - INFO - Loaded 2424 training samples and 1040 test samples\n",
      "2024-11-10 02:47:26,282 - src.data.data_ingestion - INFO - Found 1 documents for processing\n",
      "Loading documents: 100%|██████████| 1/1 [00:00<00:00, 222.98it/s]\n",
      "2024-11-10 02:47:26,288 - root - INFO - Processing documents...\n",
      "2024-11-10 02:47:26,288 - src.data.data_preprocessing - INFO - Processing documents into chunks...\n",
      "Chunking documents: 100%|██████████| 1/1 [00:00<00:00, 124.03it/s]\n",
      "2024-11-10 02:47:26,552 - src.data.data_preprocessing - INFO - Created 177 chunks from 1 documents\n",
      "2024-11-10 02:47:26,553 - src.retrieval.hybrid_retriever - INFO - Initializing BM25 index...\n",
      "2024-11-10 02:47:26,561 - src.retrieval.bm25_retriever - INFO - Loaded BM25 index from cache\n",
      "2024-11-10 02:47:26,561 - src.retrieval.hybrid_retriever - INFO - Initializing vector store...\n",
      "2024-11-10 02:47:26,567 - faiss.loader - INFO - Loading faiss with AVX512 support.\n",
      "2024-11-10 02:47:26,578 - faiss.loader - INFO - Successfully loaded faiss with AVX512 support.\n",
      "2024-11-10 02:47:26,585 - src.retrieval.vector_retriever - INFO - Vector store created successfully\n",
      "2024-11-10 02:47:26,585 - src.retrieval.hybrid_retriever - INFO - Hybrid retriever initialization complete\n"
     ]
    }
   ],
   "source": [
    "# Initialize system with both configs\n",
    "initializer = RAGInitializer(init_config_path, process_config_path)\n",
    "components = initializer.initialize()\n",
    "components.retriever.initialize(components.original_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --> IMPORTANT : You will need to restart the notebook each time in case of CUDA Out Of Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.36it/s]\n",
      "2024-11-10 02:47:28,315 - root - INFO - Retrieved 50 documents\n",
      "2024-11-10 02:47:28,315 - src.models.reranker - INFO - Reranking 50 documents for query: search_query: What is the subject key identifier of the DAC?\n",
      "Reranking documents: 100%|██████████| 7/7 [00:03<00:00,  2.28it/s]\n",
      "2024-11-10 02:47:31,380 - root - INFO - Reranked results. Best score: 4.782618522644043\n"
     ]
    }
   ],
   "source": [
    "query = 'search_query: What is the subject key identifier of the DAC?'\n",
    "# Process queries\n",
    "result = process_query(query=query,\n",
    "            retriever = components.retriever,\n",
    "            reranker = components.reranker,\n",
    "            response_generator = components.response_generator,\n",
    "            process_config = components.process_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The subject key identifier of the DAC is 96:C2:D9:24:94:EA:97:85:C0:D1:67:08:E3:88:F1:C0:91:EA:0F:D5.</s>\n"
     ]
    }
   ],
   "source": [
    "print(result[\"Response\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_trustii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
