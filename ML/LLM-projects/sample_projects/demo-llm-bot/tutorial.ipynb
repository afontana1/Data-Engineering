{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Bot Demo with MLRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-01-04 23:15:30,122 [info] Created and saved project: {'name': 'llmbot', 'from_template': None, 'overwrite': False, 'context': './', 'save': True}\n",
      "Project Source: v3io:///bigdata/demo-llm-bot.zip\n",
      "Exporting project as zip archive to v3io:///bigdata/demo-llm-bot.zip...\n",
      "> 2024-01-04 23:15:30,873 [info] Project created successfully: {'project_name': 'llmbot', 'stored_in_db': True}\n"
     ]
    }
   ],
   "source": [
    "project = mlrun.get_or_create_project(\n",
    "    name=\"llmbot\",\n",
    "    parameters={\n",
    "        \"source\" : \"v3io:///bigdata/demo-llm-bot.zip\",\n",
    "        # \"source\" : \"git://github.com/mlrun/demo-llm-bot#main\",\n",
    "        \"secrets_file\" : \"secrets.env\",\n",
    "        \"image\" : \"nschenone/llmbot:1.4.1\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Data and Deploy LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>Pipeline running (id=7da0c1da-b6b9-4625-a054-dc7314898ac4), <a href=\"https://dashboard.default-tenant.app.cst-354.iguazio-cd2.com/mlprojects/llmbot/jobs/monitor-workflows/workflow/7da0c1da-b6b9-4625-a054-dc7314898ac4\" target=\"_blank\"><b>click here</b></a> to view the details in MLRun UI</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 9.0.0 (20231125.0833)\n",
       " -->\n",
       "<!-- Title: kfp Pages: 1 -->\n",
       "<svg width=\"522pt\" height=\"116pt\"\n",
       " viewBox=\"0.00 0.00 522.15 116.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 112)\">\n",
       "<title>kfp</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-112 518.15,-112 518.15,4 -4,4\"/>\n",
       "<!-- llm&#45;pipeline&#45;kd2rb&#45;4025891677 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>llm&#45;pipeline&#45;kd2rb&#45;4025891677</title>\n",
       "<ellipse fill=\"green\" stroke=\"black\" cx=\"141.7\" cy=\"-90\" rx=\"141.7\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"141.7\" y=\"-85.33\" font-family=\"Times,serif\" font-size=\"14.00\">ingest&#45;documents&#45;handler</text>\n",
       "</g>\n",
       "<!-- llm&#45;pipeline&#45;kd2rb&#45;937889504 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>llm&#45;pipeline&#45;kd2rb&#45;937889504</title>\n",
       "<polygon fill=\"green\" stroke=\"black\" points=\"341.57,-36 211.82,-36 207.82,-32 207.82,0 337.57,0 341.57,-4 341.57,-36\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"337.57,-32 207.82,-32\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"337.57,-32 337.57,0\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"337.57,-32 341.57,-36\"/>\n",
       "<text text-anchor=\"middle\" x=\"274.7\" y=\"-13.32\" font-family=\"Times,serif\" font-size=\"14.00\">deploy&#45;serve&#45;llm</text>\n",
       "</g>\n",
       "<!-- llm&#45;pipeline&#45;kd2rb&#45;4025891677&#45;&gt;llm&#45;pipeline&#45;kd2rb&#45;937889504 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>llm&#45;pipeline&#45;kd2rb&#45;4025891677&#45;&gt;llm&#45;pipeline&#45;kd2rb&#45;937889504</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M173.89,-72.05C191.28,-62.91 212.97,-51.49 231.77,-41.59\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"233.32,-44.73 240.54,-36.98 230.06,-38.54 233.32,-44.73\"/>\n",
       "</g>\n",
       "<!-- llm&#45;pipeline&#45;kd2rb&#45;451092921 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>llm&#45;pipeline&#45;kd2rb&#45;451092921</title>\n",
       "<ellipse fill=\"green\" stroke=\"black\" cx=\"407.7\" cy=\"-90\" rx=\"106.45\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"407.7\" y=\"-85.33\" font-family=\"Times,serif\" font-size=\"14.00\">ingest&#45;urls&#45;handler</text>\n",
       "</g>\n",
       "<!-- llm&#45;pipeline&#45;kd2rb&#45;451092921&#45;&gt;llm&#45;pipeline&#45;kd2rb&#45;937889504 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>llm&#45;pipeline&#45;kd2rb&#45;451092921&#45;&gt;llm&#45;pipeline&#45;kd2rb&#45;937889504</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M376.18,-72.41C358.81,-63.27 336.99,-51.78 318.04,-41.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"319.67,-38.71 309.19,-37.15 316.41,-44.91 319.67,-38.71\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f54e584df40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>Run Results</h2><h3>[info] Workflow 7da0c1da-b6b9-4625-a054-dc7314898ac4 finished, state=Succeeded</h3><br>click the hyper links below to see detailed results<br><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>uid</th>\n",
       "      <th>start</th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td><div title=\"fca3c37921944922bf968d996bfdfb05\"><a href=\"https://dashboard.default-tenant.app.cst-354.iguazio-cd2.com/mlprojects/llmbot/jobs/monitor/fca3c37921944922bf968d996bfdfb05/overview\" target=\"_blank\" >...6bfdfb05</a></div></td>\n",
       "      <td>Jan 04 23:15:53</td>\n",
       "      <td>completed</td>\n",
       "      <td>ingest-urls-handler</td>\n",
       "      <td><div class=\"dictlist\">urls_file=data/urls/mlops_blogs.txt</div></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td><div title=\"9c9eacead6824982b5f85fc4b839088d\"><a href=\"https://dashboard.default-tenant.app.cst-354.iguazio-cd2.com/mlprojects/llmbot/jobs/monitor/9c9eacead6824982b5f85fc4b839088d/overview\" target=\"_blank\" >...b839088d</a></div></td>\n",
       "      <td>Jan 04 23:15:53</td>\n",
       "      <td>completed</td>\n",
       "      <td>ingest-documents-handler</td>\n",
       "      <td><div class=\"dictlist\">source_directory=data/mlrun_docs_md</div></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "7da0c1da-b6b9-4625-a054-dc7314898ac4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project.run(\n",
    "    name=\"main\",\n",
    "    arguments={\n",
    "        \"source_directory\" : \"data/mlrun_docs_md\",\n",
    "        \"urls_file\" : \"data/urls/mlops_blogs.txt\"\n",
    "    },\n",
    "    watch=True,\n",
    "    dirty=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query LLM via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model endpoint: http://nuclio-llmbot-serve-llm.default-tenant.svc.cluster.local:8080\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Get model serving endpoint port\n",
    "serving_fn = project.get_function(\"serve-llm\", sync=True)\n",
    "\n",
    "MODEL_ENDPOINT_URL = serving_fn.get_url()\n",
    "print(f\"Model endpoint: {MODEL_ENDPOINT_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'how I deploy an sklearn model with mlrun?',\n",
       " 'chat_history': [],\n",
       " 'output': 'To deploy an sklearn model with mlrun, you can use the `mlrun.deploy_function` function. Here is an example code snippet:\\n\\n```\\nserving_fn = mlrun.new_function(\"serving\", image=\"mlrun/mlrun\", kind=\"serving\")\\nmlrun.deploy_function(\\n  serving_fn,\\n  models=[\\n      {\\n          \"key\": model_name,\\n          \"model_path\": train.outputs[\"model\"],\\n          \"class_name\": \\'mlrun.frameworks.sklearn.SklearnModelServer\\',\\n      }\\n  ],\\n)\\n```\\n\\nYou can also use the `apply_mlrun` function from the `mlrun.frameworks.sklearn` module to apply mlrun to your sklearn model. Here is an example code snippet:\\n\\n```\\nfrom mlrun.frameworks.sklearn import apply_mlrun\\n\\napply_mlrun(model=model, model_name=\"my_model\", x_test=X_test, y_test=y_test)\\nmodel.fit(X_train, y_train)\\n```\\n\\nFor more information and examples, you can refer to the mlrun documentation sources provided below.\\n\\nSOURCES:\\n- data/mlrun_docs_md/projects/run-build-deploy.md\\n- data/mlrun_docs_md/cheat-sheet.md\\n- data/mlrun_docs_md/projects/build-run-workflows-pipelines.md'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = requests.post(url=MODEL_ENDPOINT_URL, json={\"question\" : \"how I deploy an sklearn model with mlrun?\", \"chat_history\" : []})\n",
    "resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the average weight of Adelie penguins?',\n",
       " 'chat_history': [],\n",
       " 'output': '3706.16 grams'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = requests.post(url=MODEL_ENDPOINT_URL, json={\"question\" : \"What is the average weight of Adelie penguins?\", \"chat_history\" : []})\n",
    "resp.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query LLM via Chat Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:7860\n",
      "Running on public URL: https://65c537de6121616438.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://65c537de6121616438.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import gradio as gr\n",
    "import requests\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import messages_to_dict\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "\n",
    "def enrich_docs_url(ai_message: str) -> str:\n",
    "    pattern = r\"data/mlrun_docs_md/(.*?)\\.md\"\n",
    "    new_url = r\"https://docs.mlrun.org/en/latest/\\1.html\"\n",
    "    return re.sub(pattern, new_url, ai_message)\n",
    "\n",
    "\n",
    "def query_llm(message: str) -> str:\n",
    "    resp = requests.post(\n",
    "        url=MODEL_ENDPOINT_URL,\n",
    "        json={\n",
    "            \"question\": message,\n",
    "            \"chat_history\": messages_to_dict(memory.chat_memory.messages),\n",
    "        },\n",
    "        verify=False,\n",
    "    )\n",
    "    resp_json = resp.json()\n",
    "    ai_message = resp_json[\"output\"]\n",
    "    memory.save_context({\"input\": message}, {\"output\": ai_message})\n",
    "    ai_message = enrich_docs_url(ai_message=ai_message)\n",
    "    return ai_message\n",
    "\n",
    "\n",
    "def reset_memory() -> None:\n",
    "    memory.clear()\n",
    "    return None\n",
    "\n",
    "\n",
    "with gr.Blocks(analytics_enabled=False, theme=gr.themes.Soft()) as chat:\n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot()\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=6):\n",
    "            message = gr.Textbox(label=\"Q:\", placeholder=\"Type a question and Enter\")\n",
    "        with gr.Column(scale=3):\n",
    "            clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def respond(message, chat_history):\n",
    "        bot_message = query_llm(message=message)\n",
    "        chat_history.append((message, bot_message))\n",
    "        return \"\", chat_history\n",
    "\n",
    "    message.submit(respond, [message, chatbot], [message, chatbot])\n",
    "    clear.click(reset_memory, None, chatbot, queue=False)\n",
    "\n",
    "chat.launch(server_name=\"0.0.0.0\", share=True, ssl_verify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "chat.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmbot",
   "language": "python",
   "name": "conda-env-.conda-llmbot-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
