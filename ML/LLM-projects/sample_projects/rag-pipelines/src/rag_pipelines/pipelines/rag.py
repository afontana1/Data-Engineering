import os
from typing import Any, Optional

import weave
from langchain_community.retrievers import PineconeHybridSearchRetriever
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts.chat import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_groq import ChatGroq
from weave import Model
from weave.integrations.langchain import WeaveTracer

# Disable global tracing explicitly
os.environ["WEAVE_TRACE_LANGCHAIN"] = "false"


class RAGPipeline(Model):
    """A hybrid retrieval-augmented generation (RAG) pipeline using Weave for tracing and LangChain components.

    This pipeline integrates a retriever, prompt template, and language model (LLM) to implement a retrieval-augmented
    generation system, where the LLM generates answers based on both retrieved documents and a prompt template.
    Weave is used for tracing to monitor the pipeline's execution.

    Attributes:
        retriever: The retrieval model used to fetch relevant documents based on a query.
        prompt: The prompt template to generate questions for the LLM.
        llm: The language model used to generate responses.
        tracer: The tracer used to record the execution details with Weave.
        tracing_project_name: The name of the Weave project for tracing.
    """

    retriever: Optional[PineconeHybridSearchRetriever] = None
    prompt: Optional[ChatPromptTemplate] = None
    llm: Optional[ChatGroq] = None
    tracing_project_name: str
    weave_params: dict[str, Any]
    tracer: Optional[WeaveTracer] = None

    def __init__(self, retriever, prompt, llm, tracing_project_name="hybrid_rag", weave_params=None):
        """Initialize the HybridRAGPipeline.

        This constructor sets up the retriever, prompt, LLM, and integrates Weave tracing if specified.

        Args:
            retriever: The retrieval model used to fetch documents for the RAG pipeline.
            prompt: The prompt template used to create questions for the LLM.
            llm: The language model used for response generation based on retrieved documents and prompt.
            tracing_project_name (str): The name of the Weave project for tracing. Defaults to "hybrid_rag".
            weave_params (dict): Additional parameters for initializing Weave. This can include configuration
                                    details or authentication settings for the Weave service.
        """
        super().__init__(
            retriever=retriever,
            prompt=prompt,
            llm=llm,
            tracing_project_name=tracing_project_name,
            weave_params=weave_params,
        )

        if weave_params is None:
            weave_params = {}

        self.retriever = retriever
        self.prompt = prompt
        self.llm = llm
        self.tracing_project_name = tracing_project_name

        # Initialize Weave tracing if parameters are provided, otherwise default initialization.
        if weave_params:
            self._initialize_weave(**weave_params)
        else:
            self._initialize_weave()

    def _initialize_weave(self, **weave_params):
        """Initialize Weave with the specified tracing project name.

        This method sets up the Weave environment and creates an instance of the WeaveTracer.
        The tracer records the execution of each step in the RAG pipeline for monitoring and debugging purposes.
        """
        # Initialize the Weave project
        weave.init(self.tracing_project_name, **weave_params)
        # Set up the tracer for tracking pipeline execution
        self.tracer = WeaveTracer()

    @weave.op()
    def predict(self, question: str) -> str:
        """Execute the Hybrid RAG pipeline with the given question.

        This method orchestrates the entire RAG pipeline. It first retrieves documents using the retriever,
        formats them, generates a question using the prompt template, and then processes the final response
        using the LLM. The process is traced using Weave for debugging and monitoring.

        Args:
            question (str): The input question to be answered by the pipeline.

        Returns:
            str: The answer generated by the LLM based on the retrieved documents and the question prompt.
        """
        # Configuration for trace callbacks to record the execution process
        config = {"callbacks": [self.tracer]}

        # Set up the RAG pipeline chain with document retrieval, formatting, prompting, LLM, and output parsing
        rag_chain = (
            {
                "context": self.retriever | self.format_docs,
                "question": RunnablePassthrough(),
            }
            | self.prompt
            | self.llm
            | StrOutputParser()
        )

        # Invoke the pipeline with the specified question and configuration
        return rag_chain.invoke(question, config=config)

    def format_docs(self, docs):
        """Format retrieved documents into a string for input to the LLM.

        The documents are formatted with information such as filing date, accession number, summary, and image
        descriptions.
        This string will be passed as the context for the LLM to generate a response.

        Args:
            docs (list): A list of document objects that have been retrieved based on the input question.

        Returns:
            str: A formatted string of document contents, joined by newline characters.
        """
        context = ""
        for doc in docs:
            date = doc.metadata["filing_date"]
            accession_no = doc.metadata["accession_no"]
            summary = doc.metadata["summary"]
            image_descriptions = doc.metadata["image_descriptions"]
            context += (
                f"""# Report {accession_no} filed on {date}:\n\n## An excerpt from the report"""
                f"""\n\n{doc.page_content}\n\n"""
            )
            if len(image_descriptions) > 0:
                context += f"""## Image descriptions\n\n{image_descriptions}\n\n"""
            context += (
                f"""## Summary of the report\n\nHere's a summary of the report along with the some """
                f"""important keywords and phrases present in the report:\n\n{summary}\n\n"""
            )

        return context
