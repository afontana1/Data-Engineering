{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowflake RAG demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Cortex Search Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download your search database\n",
    "\n",
    "For example, you can use [KILT knowledge source](https://github.com/facebookresearch/KILT) to download the [preprocessed 2019/08/01 Wikipedia dump](http://dl.fbaipublicfiles.com/KILT/kilt_knowledgesource.json)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Apply truncation to the knowledge dataset\n",
    "\n",
    "Since the wikipedia passages are too long to achieve good retrieval quality. We need to apply truncation for the whole dataset. Here is an example doing the truncation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "input_file = \"kilt_knowledgesource.json\"\n",
    "output_file_base = \"wiki2019_part\"  # Base name for output files\n",
    "max_file_size = 240 * 1024 * 1024  # 250 MB in bytes\n",
    "\n",
    "\n",
    "def split_into_chunks(text, max_sentences=4):\n",
    "    \"\"\"\n",
    "    Split text into chunks of at most max_sentences sentences.\n",
    "    \"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    return [\" \".join(sentences[i:i + max_sentences]) for i in range(0, len(sentences), max_sentences)]\n",
    "\n",
    "\n",
    "# Initialize variables\n",
    "current_file_index = 1\n",
    "current_file_size = 0\n",
    "output_file = open(f\"{output_file_base}_{current_file_index}.json\", 'w', encoding='utf-8')\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        if not line.strip():  # Skip empty lines\n",
    "            continue\n",
    "\n",
    "        json_data = json.loads(line)\n",
    "        wikipedia_title = json_data.get(\"wikipedia_title\")\n",
    "        text_list = json_data.get(\"text\", [])\n",
    "\n",
    "        first_content = []\n",
    "        current_section = None\n",
    "        section_content = []\n",
    "        stop_processing = False\n",
    "\n",
    "        # Process initial content before first \"Section::::\"\n",
    "        for text in text_list:\n",
    "            if text.startswith(\"Section::::\"):\n",
    "                break\n",
    "            first_content.append(text.strip())\n",
    "\n",
    "        if first_content:\n",
    "            full_text = \" \".join(first_content).replace(\"BULLET::::\", \"\").strip()\n",
    "            entry_text = f\"{wikipedia_title}: {full_text}\"\n",
    "            new_entry = {\"text\": entry_text}\n",
    "            serialized_entry = json.dumps(new_entry) + '\\n'\n",
    "            entry_size = len(serialized_entry.encode('utf-8'))\n",
    "\n",
    "            if current_file_size + entry_size > max_file_size:\n",
    "                output_file.close()\n",
    "                current_file_index += 1\n",
    "                output_file = open(f\"{output_file_base}_{current_file_index}.json\", 'w', encoding='utf-8')\n",
    "                current_file_size = 0\n",
    "\n",
    "            output_file.write(serialized_entry)\n",
    "            current_file_size += entry_size\n",
    "\n",
    "        # Process sections normally\n",
    "        for text in text_list:\n",
    "            if text.startswith(\"Section::::\"):\n",
    "                if current_section and section_content and not stop_processing:\n",
    "                    full_text = \" \".join(section_content).replace(\"BULLET::::\", \"\").strip()\n",
    "                    chunks = split_into_chunks(full_text, max_sentences=4)\n",
    "\n",
    "                    for chunk in chunks:\n",
    "                        entry_text = f\"{wikipedia_title} - {current_section}: {chunk}\"\n",
    "                        new_entry = {\"text\": entry_text}\n",
    "                        serialized_entry = json.dumps(new_entry) + '\\n'\n",
    "                        entry_size = len(serialized_entry.encode('utf-8'))\n",
    "\n",
    "                        if current_file_size + entry_size > max_file_size:\n",
    "                            output_file.close()\n",
    "                            current_file_index += 1\n",
    "                            output_file = open(f\"{output_file_base}_{current_file_index}.json\", 'w', encoding='utf-8')\n",
    "                            current_file_size = 0\n",
    "\n",
    "                        output_file.write(serialized_entry)\n",
    "                        current_file_size += entry_size\n",
    "\n",
    "                current_section = text.replace(\"Section::::\", \"\").split('.')[0].strip()\n",
    "                section_content = []\n",
    "                stop_processing = current_section.startswith((\"External links\", \"See also\"))\n",
    "            else:\n",
    "                if not stop_processing:\n",
    "                    section_content.append(text.strip())\n",
    "\n",
    "        if current_section and section_content and not stop_processing:\n",
    "            full_text = \" \".join(section_content).replace(\"BULLET::::\", \"\").strip()\n",
    "            chunks = split_into_chunks(full_text, max_sentences=4)\n",
    "\n",
    "            for chunk in chunks:\n",
    "                entry_text = f\"{wikipedia_title} - {current_section}: {chunk}\"\n",
    "                new_entry = {\"text\": entry_text}\n",
    "                serialized_entry = json.dumps(new_entry) + '\\n'\n",
    "                entry_size = len(serialized_entry.encode('utf-8'))\n",
    "\n",
    "                if current_file_size + entry_size > max_file_size:\n",
    "                    output_file.close()\n",
    "                    current_file_index += 1\n",
    "                    output_file = open(f\"{output_file_base}_{current_file_index}.json\", 'w', encoding='utf-8')\n",
    "                    current_file_size = 0\n",
    "\n",
    "                output_file.write(serialized_entry)\n",
    "                current_file_size += entry_size\n",
    "\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Upload the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please see [Cortex Search tutorials](https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-search/overview-tutorials) for more instructions.\n",
    "\n",
    "#### Option 1: Use Snowsight to load data into table\n",
    "\n",
    "In a supported web browser, navigate to [Snowsight](https://app.snowflake.com). \n",
    "\n",
    "After login, click on the \"Create\" on the upper left, choose \"Table: From File\". \n",
    "\n",
    "Upload your files for the dataset, create your own database and schema (You may need to create a new warehouse if you don't have one). \n",
    "\n",
    "Click \"Next\", you can review the data columns. \n",
    "\n",
    "If you are uploading using the json files, better to uncheck \"Load as a single variant column?\". \n",
    "\n",
    "Also, you can edit your column name for the key retrieval content as \"text\". \n",
    "\n",
    "Then you can then click \"load\" and the loading will be started.\n",
    "\n",
    "#### Option 2: Use snowflake's python library to load data into table (see below for full code to insert data and create Search Service programmatically):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Build the Cortex Search Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: Use Snowsight to create the service\n",
    "\n",
    "Click on \"AI & ML\" on the left menu, go to \"Studio\", and choose \"Create a Cortex Search Service\" in the studio.\n",
    "\n",
    "Choose your own \"Database and Schema\" and set your service name in \"Name\".\n",
    "\n",
    "In the following steps, you need to go through the following steps:\n",
    "\n",
    "a. Select data to be indexed \\\n",
    "b. Select columns to include in the service \\\n",
    "c. Select a column to search \\\n",
    "d. Select attribute column(s) (This one can be skipped) \\\n",
    "e. Configure your Search Service (Usually use the default one)\n",
    "\n",
    "Then you just need to wait after the service creation is done.\n",
    "\n",
    "#### Option 2: Use snowflake's python library to create the service\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"SNOWFLAKE_ACCOUNT\"] = \"your_account\"\n",
    "os.environ[\"SNOWFLAKE_USER\"] = \"your_username\"\n",
    "os.environ[\"SNOWFLAKE_ROLE\"] = \"your_role\"\n",
    "os.environ[\"SNOWFLAKE_WAREHOUSE\"] = \"your_warehouse\"\n",
    "os.environ[\"SNOWFLAKE_DATABASE\"] = \"your_database\"\n",
    "os.environ[\"SNOWFLAKE_SCHEMA\"] = \"your_schema\"\n",
    "os.environ[\"CORTEX_SERVICE_NAME\"] = \"your_service_name\"\n",
    "os.environ[\"SNOWFLAKE_PASSWORD\"] = \"your_password\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def load_json_files(directory_path):\n",
    "    \"\"\"Load all JSON files from the specified directory into a pandas DataFrame.\"\"\"\n",
    "    # Get all JSON files in the directory\n",
    "    json_files = glob.glob(os.path.join(directory_path, \"*.json\"))\n",
    "    print(f\"Found {len(json_files)} JSON files\")\n",
    "    \n",
    "    # List to store all records\n",
    "    all_records = []\n",
    "    \n",
    "    # Process each JSON file\n",
    "    for file_path in json_files:\n",
    "        print(f\"Processing {file_path}\")\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                try:\n",
    "                    record = json.loads(line)\n",
    "                    all_records.append(record)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error parsing JSON in {file_path}: {e}\")\n",
    "                    continue\n",
    "        break # Remove this line to process all files\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_records)\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    # Snowflake connection parameters\n",
    "    snowflake_connection_params = {\n",
    "        \"account\": os.environ.get(\"SNOWFLAKE_ACCOUNT\"),\n",
    "        \"user\": os.environ.get(\"SNOWFLAKE_USER\"),\n",
    "        \"password\": os.environ.get(\"SNOWFLAKE_PASSWORD\"),  \n",
    "        \"warehouse\": os.environ.get(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "        \"database\": os.environ.get(\"SNOWFLAKE_DATABASE\"),\n",
    "        \"schema\": os.environ.get(\"SNOWFLAKE_SCHEMA\"),\n",
    "    }\n",
    "\n",
    "    # Use the specified directory path\n",
    "    directory_path = \"./simple_RAG\"\n",
    "    \n",
    "    # Load JSON files\n",
    "    print(\"Loading JSON files...\")\n",
    "    df = load_json_files(directory_path)\n",
    "    print(\"JSON data preview:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # Connect to Snowflake\n",
    "    print(\"Connecting to Snowflake...\")\n",
    "    conn = snowflake.connector.connect(**snowflake_connection_params)\n",
    "    print(\"Connected to Snowflake.\")\n",
    "\n",
    "    # Create and populate the WIKI_TEXT table\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(\"DROP TABLE IF EXISTS WIKI_TEXT\")\n",
    "        conn.commit()\n",
    "\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE WIKI_TEXT (\n",
    "                CONTENT_TEXT STRING\n",
    "            )\n",
    "        \"\"\")\n",
    "        conn.commit()\n",
    "    print(\"Table 'WIKI_TEXT' created successfully.\")\n",
    "\n",
    "    # Rename the 'text' column to 'CONTENT_TEXT' before writing to Snowflake\n",
    "    df = df.rename(columns={'text': 'CONTENT_TEXT'})\n",
    "\n",
    "    # Write the data to Snowflake\n",
    "    success, nchunks, nrows, _ = write_pandas(conn, df, 'WIKI_TEXT')\n",
    "    print(f\"Data load into 'WIKI_TEXT' successful: {success}\")\n",
    "    print(f\"Number of rows inserted: {nrows}\")\n",
    "\n",
    "    # Create the warehouse for Cortex Search if it doesn't exist\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(\"CREATE WAREHOUSE IF NOT EXISTS CORTEX_SEARCH_WH WAREHOUSE_SIZE = 'XSMALL'\")\n",
    "        conn.commit()\n",
    "    print(\"Warehouse 'CORTEX_SEARCH_WH' is available.\")\n",
    "\n",
    "    # Create the Cortex Search Service\n",
    "    create_cortex_search_service = \"\"\"\n",
    "    CREATE OR REPLACE CORTEX SEARCH SERVICE wiki_search_service\n",
    "      ON CONTENT_TEXT\n",
    "      WAREHOUSE = CORTEX_SEARCH_WH\n",
    "      TARGET_LAG = '1 day'\n",
    "      EMBEDDING_MODEL = 'snowflake-arctic-embed-l-v2.0'\n",
    "      AS (\n",
    "        SELECT\n",
    "            CONTENT_TEXT\n",
    "        FROM WIKI_TEXT\n",
    "    );\n",
    "    \"\"\"\n",
    "\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(create_cortex_search_service)\n",
    "        conn.commit()\n",
    "    print(\"Cortex Search Service 'wiki_search_service' created successfully.\")\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "    print(\"Snowflake connection closed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Build your RAG framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arctic_agentic_rag.template import Field, Template\n",
    "from arctic_agentic_rag.template.template import set_instructions\n",
    "\n",
    "# Create your prompt\n",
    "generate_short_answer_prompt = (\n",
    "    \"Answer questions with concise, fact-based answers using only the information provided in the context. \\n\"\n",
    "    \"Include a citation for the source of the information if available in the context, \\n\"\n",
    "    \"Cite the source of the information by including the context number in brackets, immediately after the relevant part of the answer. \\n\"\n",
    "    \"You should include the citations for all the information you use. \\n\"\n",
    "    \"Format the answer section as: \\\"[content] [number]\\\", where [number] is the context number. \\n\"\n",
    "    \"Examples: \\n\"\n",
    "    \"- \\\"He went to school at 8 am [1].\\\" \\n\"\n",
    "    \"- \\\"She wrote the book A [2] and book B [4].\\\" \\n\"\n",
    "    \"- \\\"They are shopping for food [1][3].\\\" \\n\"\n",
    "    \"The answer should be simple, clear, concise, and correct, often between 2-5 words. \\n\"\n",
    "    \"If the context lacks sufficient information to answer, \\n\"\n",
    "    \"Directly Format the answer section as: \\\"I don't know.\\\" \\n\"\n",
    ")\n",
    "\n",
    "# Set the input and output template\n",
    "@set_instructions(generate_short_answer_prompt)\n",
    "class GenerateAnswer(Template):\n",
    "    question = Field(desc=\"the input question\", mode=\"input\")\n",
    "    context = Field(desc=\"may contain relevant information\", mode=\"input\")\n",
    "    answer = Field(desc=\"the output answer, often between 2-5 words\", mode=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arctic_agentic_rag.agent import TemplateAgent\n",
    "from arctic_agentic_rag.retrieval import CortexRetrievalAgent\n",
    "from arctic_agentic_rag.llm import BaseLLM\n",
    "from arctic_agentic_rag.logging import Logger\n",
    "import re\n",
    "\n",
    "class GenerateAnswerAgent(TemplateAgent):\n",
    "    def __init__(self, backbone: BaseLLM, uid: str, logger: Logger, **llm_kwargs):\n",
    "        super(GenerateAnswerAgent, self).__init__(\n",
    "            backbone=backbone, \n",
    "            uid=uid,\n",
    "            logger=logger,\n",
    "            template=GenerateAnswer, \n",
    "            **llm_kwargs)\n",
    "        \n",
    "class RAG(TemplateAgent):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: BaseLLM,\n",
    "        uid: str,\n",
    "        logger: Logger,\n",
    "        retrieval_config,\n",
    "        **llm_kwargs,\n",
    "    ):\n",
    "\n",
    "        super(RAG, self).__init__(\n",
    "            backbone=backbone, \n",
    "            uid=uid,\n",
    "            logger=logger,\n",
    "            retrieval_config=retrieval_config, \n",
    "            template=GenerateAnswer, \n",
    "            action_space=[], \n",
    "            **llm_kwargs)\n",
    "        \n",
    "        self.retrieval = CortexRetrievalAgent(\n",
    "            connection_config = retrieval_config[0],\n",
    "            service_config = retrieval_config[1]\n",
    "        )\n",
    "\n",
    "        self.generate = GenerateAnswerAgent(\n",
    "            backbone,\n",
    "            uid=None,\n",
    "            logger=logger,\n",
    "        )\n",
    "        \n",
    "    # convert list to string\n",
    "    def convert_context(self, context):\n",
    "        return \"\\n\".join([f\"[{i+1}] {item}\" for i, item in enumerate(context)])\n",
    "    \n",
    "    # clean the answer and get the source\n",
    "    def process_source(self, answer_string, context_string):\n",
    "        indices = [int(num) for num in re.findall(r'\\[(\\d+)\\]', answer_string)]\n",
    "        cleaned_answer_string = re.sub(r'\\[\\d+\\]', '', answer_string).strip()\n",
    "\n",
    "        if not indices:\n",
    "            processed_source = \"None\"\n",
    "        else:\n",
    "            # Use regex to find all context entries in the context_string\n",
    "            pattern = r'\\[(\\d+)\\](.*?)(?=\\n\\[\\d+\\]|$)'\n",
    "            matches = re.findall(pattern, context_string, re.DOTALL)\n",
    "\n",
    "            # Create a dictionary mapping each index to its context entry\n",
    "            context_dict = {int(idx): text.strip() for idx, text in matches}\n",
    "\n",
    "            # Extract entries corresponding to the provided indices\n",
    "            extracted_entries = {idx: context_dict.get(idx, '') for idx in indices}\n",
    "\n",
    "            processed_source = \"\\n\".join(f\"[{i + 1}] {extracted_entries[k]}\" for i, k in enumerate(sorted(extracted_entries)))\n",
    "\n",
    "        return cleaned_answer_string, processed_source\n",
    "\n",
    "    def get_result(self, question: str):\n",
    "\n",
    "        # number of passages\n",
    "        k = 5\n",
    "\n",
    "        # question answering\n",
    "        retrieval_context = self.retrieval.get_retrieval(question, k)\n",
    "        context_string = self.convert_context(retrieval_context)\n",
    "        origin_answer_input = {\"question\": question, \"context\": context_string}\n",
    "        origin_answer_output = self.generate.get_result(origin_answer_input)\n",
    "        origin_answer = origin_answer_output[\"answer\"]\n",
    "\n",
    "        cleaned_origin_answer, source = self.process_source(origin_answer, context_string)\n",
    "\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"answer\": cleaned_origin_answer,\n",
    "            \"source\": source\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    def parse_response(self, response: str):\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arctic_agentic_rag.utils import get_snowflake_credentials\n",
    "\n",
    "# set connection for cortex search\n",
    "connection_config=get_snowflake_credentials(authentication=\"sso\")\n",
    "\n",
    "service_config={\n",
    "    \"database\": os.environ.get(\"SNOWFLAKE_DATABASE\"),\n",
    "    \"schema\": os.environ.get(\"SNOWFLAKE_SCHEMA\"),\n",
    "    \"service_name\": os.environ.get(\"CORTEX_SERVICE_NAME\")\n",
    "}\n",
    "\n",
    "retrieval_config = [connection_config, service_config]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from arctic_agentic_rag.llm import CortexComplete\n",
    "\n",
    "# the run log and result file will be saved here\n",
    "logger = Logger(base_path=\"Result\")\n",
    "\n",
    "# the identity verification will be twice, one for retrieval and one for generation\n",
    "llm = CortexComplete(uid=\"rag-test.generator\", model=\"llama3.1-405b\", authentication=\"sso\", logger=logger, max_retries=1)\n",
    "\n",
    "agent = RAG(backbone=llm, uid=\"rag test\", logger=logger, retrieval_config=retrieval_config)\n",
    "\n",
    "# test single question\n",
    "result = agent.get_result(\"when did the song holiday road come out\")\n",
    "\n",
    "logger.log_final_results(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
