{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMOX2aHdRjVy/l90VRoPqu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trancethehuman/ai-workshop-code/blob/main/Long_term_memory_%26_personalized_LLM_responses.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Long-term memory and personalized LLM responses (5 tiers)"
      ],
      "metadata": {
        "id": "stIgQAOWwOyE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we start, make sure you have an [OpenAI API key](https://platform.openai.com/docs/overview) and a [Pinecone API key](https://www.pinecone.io/). Pinecone should be free, and OpenAI will cost you very very little."
      ],
      "metadata": {
        "id": "PiaH3UnRiHIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The setup\n",
        "First, you'll need to install some dependencies."
      ],
      "metadata": {
        "id": "HSApoWrayqQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGFM2ztrx3l1",
        "outputId": "29d52d42-6cc9-4302-f37f-c1dcf00b9b51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, enter your OpenAI API key."
      ],
      "metadata": {
        "id": "_8qtOgZzyx19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "\n",
        "OPENAI_API_KEY = getpass.getpass('Enter your OpenAI API key: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6NuEckyy11Z",
        "outputId": "93505c62-0318-4d00-83d3-533b08cfbb89"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, set up an OpenAI client to interact with GPT."
      ],
      "metadata": {
        "id": "3c5e7DOmzbiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "hc6dGxLAzf8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's declare a `system` message to our AI assistant. The system message is supposedly there to help ground the conversation."
      ],
      "metadata": {
        "id": "lb69MBAU13lt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = {\"role\": \"system\", \"content\": \"You are a helpful personal assistant. Your main goal is to take into account what you know about the user and answer them in pirate speak.\"}\n",
        "\n",
        "messages_history = [system_message]"
      ],
      "metadata": {
        "id": "SkK4IXTD2FKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we create a handy function to receive user's request and send back an answer using `gpt-4-turbo`.\n",
        "\n",
        "In Python, to capture the content as it's streamed in from OpenAI, we'll have to use `yield` instead of `return` statement.\n",
        "\n",
        "Notice here that I'm appending new user queries directly into the messages_history list outside of this function to keep a list of previous messages, so we can better simulate a conversation."
      ],
      "metadata": {
        "id": "DXGMGlZBz2Ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ai_answer(new_question: str, messages: list):\n",
        "    messages.append({\"role\": \"user\", \"content\": new_question})\n",
        "\n",
        "    stream = client.chat.completions.create(\n",
        "        model=\"gpt-4-turbo\",\n",
        "        messages=messages,\n",
        "        stream=True,\n",
        "    )\n",
        "\n",
        "    for chunk in stream:\n",
        "        if chunk.choices[0].delta.content is not None:\n",
        "            yield chunk.choices[0].delta.content"
      ],
      "metadata": {
        "id": "P9C6D2YLz_RH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additionally, here's a handy function to just print the streaming content to our terminal."
      ],
      "metadata": {
        "id": "weiIPh263FA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_ai_answer(user_input: str):\n",
        "    for chunk in get_ai_answer(user_input, messages_history):\n",
        "        print(chunk, end=\"\")"
      ],
      "metadata": {
        "id": "u6SrmqSY3K1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's test and see if our new function works. Let's pass in a question and see the response streamed back."
      ],
      "metadata": {
        "id": "gOrx9pjV0x-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_ai_answer(\"Tell me a joke\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4ZTLDwQ02sr",
        "outputId": "5929f671-3715-46d1-b29f-c2473c6d3c7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arrr, why did the pirate go to school?\n",
            "\n",
            "To improve his \"Arrrrrrrrrr-ticulation\"! Avast!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# F-tier: No personalization\n",
        "To test the first tier of personalization, which is no personalization, ask the AI what it knows about you."
      ],
      "metadata": {
        "id": "Y9szEzLY2vuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_ai_answer(\"What do you know about me?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqanetO13BZo",
        "outputId": "ef92ef47-99fb-4033-9510-66c2435c0bb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arrr, me heartie! So far, I've naught but the basic bearings o' our conversation. If ye be willin' t' share more 'bout yer preferences or hobbies, I can tailor me responses better t' yer liking! So, what sparks yer interest, matey?"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, it doesn't know anything. This is the F-tier (the worst tier)"
      ],
      "metadata": {
        "id": "pHsKGiSn3Y-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# D-tier: System prompt personalization\n",
        "To put a tiny bit of personalization in, we can change the system message so it'll know a few things about us up-front.\n",
        "\n",
        "Here I'm just adding an extra sentence to the content of the system message, which is the first message that the LLM will see in the list."
      ],
      "metadata": {
        "id": "37qMrZIB4zu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_message[\"content\"] += \" My name is Hai and I'm currently an unemployed washed up full stack developer. I don't like chocolate.\"\n",
        "\n",
        "print_ai_answer(\"What do you know about me?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjlcSfIE6VOL",
        "outputId": "077da522-b1c3-4698-8263-b241149522c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ahoy, Hai! I know ye be a former full stack developer who's currently not working, and ye be no friend of chocolate!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only one of those things is true about me (guess which one). This method of personalization is rigid, and we can't ask it about anything else other than this."
      ],
      "metadata": {
        "id": "vik4TfoG6ty2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_ai_answer(\"Do I like baseball?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfuo8TGX6-qR",
        "outputId": "18c108c2-c354-4ecc-ad6d-42f4e6a8f084"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ahoy, Hai! Ye've not mentioned yer likin' or dislikin' for baseball to me, so I be not knowin' for sure, matey!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# C-tier: Personalization through messages history\n",
        "This takes us to the next tier: personalization through messages history. This means you let the LLM see a history of our conversation, and as long as you've mentioned a fact about yourself, then it can refer to it and answer your newest query.\n",
        "\n",
        "Here, I'm just simulating past messages by passing into our messages_history list a previous message we've sent about my dancing habit."
      ],
      "metadata": {
        "id": "KovbZUBg7KWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages_history.append({\"role\": \"user\", \"content\": \"I like to Boogie all night long.\"})"
      ],
      "metadata": {
        "id": "evp2hsYR7wmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, before asking what the assistant knows about my dancing habits, let's look at the messages we've already sent.\n",
        "\n",
        "Here I'm just using pprint to display the list of messages a bit nicer."
      ],
      "metadata": {
        "id": "JTIIfsmu8MWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "pprint(messages_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fS3N5y5A76AV",
        "outputId": "f7f011b6-35fe-4c1a-8824-18356719044d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'content': 'You are a helpful personal assistant. Your main goal is to take '\n",
            "             'into account what you know about the user and answer them in '\n",
            "             \"pirate speak. My name is Hai and I'm currently an unemployed \"\n",
            "             \"washed up full stack developer. I don't like chocolate.\",\n",
            "  'role': 'system'},\n",
            " {'content': 'Tell me a joke', 'role': 'user'},\n",
            " {'content': 'What do you know about me?', 'role': 'user'},\n",
            " {'content': 'What do you know about me?', 'role': 'user'},\n",
            " {'content': 'Do I like baseball?', 'role': 'user'},\n",
            " {'content': 'I like to Boogie all night long.', 'role': 'user'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's ask it if it knows what type of dancing I like to do."
      ],
      "metadata": {
        "id": "T8FCvcpI-oBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_ai_answer(\"What kind of dancing do I like?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gNq4CLK-snC",
        "outputId": "dd0d86a3-4502-4434-b6a6-026e1b61ed0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ahoy, Hai! Ye be likin' to Boogie all night long, that be the dancin' ye fancy!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One silly downside to keeping things in messages history is long-running conversations will result in the LLM forgetting (or technically not paying attention to) key details about the user. It can also become costly since LLMs like the ones behind an API from OpenAI are stateless (meaning it needs the full messages history sent alongside the latest message) every time you make a request. Not efficient.\n",
        "\n",
        "# B-tier: Entity memory\n",
        "Ok, on to the next tier: entity memory. This one combines LLM text generation with an old NLP technique: entity extraction.\n",
        "\n",
        "LLMs are great at entity extraction, meaning it can classify and selectively save certain words, phrases or key details from a text (in this case, a message from the user) based on a set of criteria. Imagine putting a filter on a column in your Excel spreadsheet and then save that as a worksheet.\n",
        "\n",
        "We can first look at the user's message, see if there's any juicy details we want to remember, and then pass those juicy details into the `system` message so it's always there for the LLM to refer back to.\n",
        "\n",
        "Things I'd like my LLM to remember about me: my list of likes and dislikes.\n",
        "\n",
        "What I'm gonna do is:\n",
        "- Define a new function for the app to run just before answering my question to extract out the juicy details about me.\n",
        "- Update the `system` prompt.\n",
        "- Then with the juicy details updated, answer my question.\n",
        "- **I'll go an extra step** and wipe the `messages_history` just so we can test whether or not it was able to extract key details and save them or not. Plus, this saves me some coins.\n",
        "\n",
        "First, let's declare a make a function to extract goodies from my query. I want to keep what I like and dislike. This will be it's own LLM call, with it's own `system` message.\n",
        "\n",
        "However, I don't want to stream here because I need the full output before jumping into answering. I also use `gpt-3.5-turbo` instead of `gpt-4-turbo` here for faster inference speed (it's smart enough for this task). As our pipeline gets more complicated, we're both getting better results but higher latency due to the number of hops."
      ],
      "metadata": {
        "id": "W6Yl-kEv-57P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_personality(user_input: str):\n",
        "  entity_extraction_system_message = {\"role\": \"system\", \"content\": \"Looking at the user's message, you must extract their likes and dislikes and put them as strings into lists, and respond only in JSON in this format: {{\"\"likes\"\": \"\"[<things_user_likes>]\"\", \"\"dislikes\"\": \"\"[<things_user_dislikes>]\"\"}}\"}\n",
        "\n",
        "  messages = [entity_extraction_system_message]\n",
        "  messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "  response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        stream=False,\n",
        "        response_format={\"type\": \"json_object\"}\n",
        "    )\n",
        "\n",
        "  return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "SOKn0t8CBHtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the system message here for entity extraction, I'm just letting it know that it has one job: to get the user's likes and dislikes, and respond in JSON format (I've also specified `response_format` for the OpenAI's client since their SDK supports JSON mode, which sends back (most of the time) valid JSON for you to then parse into a Python dict and have more flexibility in using it.\n",
        "\n",
        "Also, this part,`{{\"\"likes\"\": \"\"[<things_user_likes>]\"\", \"\"dislikes\"\": \"\"[<things_user_dislikes>]\"\"}}` is just a JSON object with two keys: likes and dislikes. The extra double quotes and curly brackets are there to keep them all inside one string.\n",
        "\n",
        "Let's test it."
      ],
      "metadata": {
        "id": "99Sfj1DmDXhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_preferences = extract_personality(\"Sometimes when I'm really sad, I go on long walks alone. Then I come home and eat a bowl of ramen. I usually don't watch TVs at night when I'm tired, but often stay up late using my phone.\")\n",
        "\n",
        "import json\n",
        "\n",
        "pprint(json.loads(my_preferences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnGGgzxpD-ci",
        "outputId": "ff00ab28-80fd-4239-f72f-2c923c80c1d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'dislikes': ['watching TV at night when tired',\n",
            "              'staying up late using the phone'],\n",
            " 'likes': ['long walks alone', 'eating a bowl of ramen']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfect. Now we can modify the answer pipeline so the assistant first take the user's query, extract info, and then come up with an informed answer.\n",
        "\n",
        "We do this by:\n",
        "- Declare a dictionary that will store our personality data: `extracted_personality`. This will be persistent even when our `messages_history` changes. This is by design so that the key memories about the user is independent of the conversation's context.\n",
        "- re-writing our `print_ai_answer()` function to first extract, then add this to the messages_history list (it's just convenient to do it like this; you can put this in the `system` prompt, an `assistant` message or `user` message. Doesn't really matter.\n",
        "\n",
        "I like to add HTML-like tags (or XML tags) when injecting large chunks of string into an LLM's prompt like this because it's shown (anecdotally) that this helps the LLM pay attention and differentiate different sections of content in it's prompt. It could be placebo but it makes it easier to read for me."
      ],
      "metadata": {
        "id": "Oh04iHt4E9LQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_personality = {\"likes\": [], \"dislikes\": []}\n",
        "\n",
        "def print_ai_answer(user_input: str):\n",
        "  # Clear the current messages history for fair testing :)\n",
        "  messages_history = []\n",
        "  print(\"Messages history cleared.\")\n",
        "\n",
        "  # Extract relevant personality info and update personality dictionary\n",
        "  new_personalities = json.loads(extract_personality(user_input))\n",
        "  print(\"Done extracting personality.\")\n",
        "  pprint(new_personalities)\n",
        "  extracted_personality[\"likes\"].extend(new_personalities[\"likes\"])\n",
        "  extracted_personality[\"dislikes\"].extend(new_personalities[\"dislikes\"])\n",
        "\n",
        "  # Insert into the messages_history list\n",
        "  messages_history.append({\"role\": \"assistant\", \"content\": f\"The user's preferences are as follows: <user_preferences>{extracted_personality}</user_preferences>\"})\n",
        "\n",
        "  # Generate a final answer\n",
        "  for chunk in get_ai_answer(user_input, messages_history):\n",
        "      print(chunk, end=\"\")"
      ],
      "metadata": {
        "id": "PS33Wa-CGXEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run our first test."
      ],
      "metadata": {
        "id": "aupfksWMISc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_ai_answer(\"I've been craving avocado toasts recently but I can't afford it. Give me one bougie thing for a millenial like me to have for breakfast.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8G7TdpIaIWVQ",
        "outputId": "84886e49-25c3-45cd-d616-4d1c9e093869"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Messages history cleared.\n",
            "Done extracting personality.\n",
            "{'dislikes': ['not being able to afford it'], 'likes': ['avocado toasts']}\n",
            "If you're looking for something that feels special and is budget-friendly, consider making a sweet potato toast. It's a delightful alternative to avocado toast and also caters to a trendy, wholesome breakfast option. Simply slice a sweet potato lengthwise into 1/4-inch thick slices, toast them in your toaster or oven until they are firm but pierce-able with a fork, and then top them with ingredients you already have at home. Good topping options include peanut butter and banana, almond butter and apple slices, or even a savory option like hummus and sliced radishes. This can give you that gourmet feel without breaking the bank!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, it was able to extract my preference: `{'dislikes': [], 'likes': ['avocado toasts']}`\n",
        "\n",
        "With no persistent messages history, let's see if I keep telling it stuff and it'll remember.\n"
      ],
      "metadata": {
        "id": "v-AwSHdNKeoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_ai_answer(\"When I'm sad, I go on long walks and it helps me relax a lot. I don't eat too much too often because I'm trying to be in shape. I spend a lot of time at the gym.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyUYE7huKr8_",
        "outputId": "b105408c-0c1f-4b49-da8b-dded24715cd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Messages history cleared.\n",
            "Done extracting personality.\n",
            "{'dislikes': ['eating too much'],\n",
            " 'likes': ['going on long walks', 'relaxing', 'spending time at the gym']}\n",
            "It's great that you have found healthy ways to manage your emotions and maintain your physical fitness. Going on long walks not only helps you relax but also has the added benefit of physical exercise, which can improve your overall mood and health. Spending time at the gym is another excellent way to stay in shape and release endorphins, which can naturally help lift your spirits. Keep up the good work maintaining a balanced lifestyle! If you ever feel like your routine is becoming too repetitive or you need new ideas to stay motivated, consider trying different walking routes or new fitness classes at the gym."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's ask for recommendations. This is where personalization is critical.\n",
        "\n",
        "Remember that there's no prior messages to this. Let's see how our assistant handles this query."
      ],
      "metadata": {
        "id": "LrIm0sOQLJyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_ai_answer(\"I'm bored. What do you think I should do?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7xyk5jYLO3K",
        "outputId": "ca156d17-3b1b-449f-dc96-282f8e22e493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Messages history cleared.\n",
            "Done extracting personality.\n",
            "{'dislikes': [], 'likes': []}\n",
            "Since you enjoy relaxing and going on long walks, why not take a leisurely stroll in a nearby park or nature reserve? This could be a great way to clear your mind and enjoy some fresh air. If the weather isn't cooperative, perhaps you could spend some time at the gym, which is another activity you like. It's a productive way to beat boredom and stay active."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Small example, but imagine your user starting a new chat thread and able to instantly get personalized recommendations. Pretty magical I'd say.\n",
        "\n",
        "Plus, our personality bank is memory efficient. Let's take a look."
      ],
      "metadata": {
        "id": "cucbKfXKLdua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(extracted_personality)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdBRmQupLBQp",
        "outputId": "ee571c9b-3e8c-44bb-d1f7-849ce4d396a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'dislikes': ['eating too much'],\n",
            " 'likes': ['avocado toasts',\n",
            "           'long walks',\n",
            "           'relaxing',\n",
            "           'spending time at the gym']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seems like it knows quite a lot about me."
      ],
      "metadata": {
        "id": "pqPDFJ-5LGn9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now this tier is a lot closer to what you'd have in a production environment. As your user uses the app, more data will start to pile up. After a year of usage, you can't just stuff a list of 500 things the user likes into an LLM's prompt for every request.\n",
        "\n",
        "So again we be more selective in what preferences to pull in.\n",
        "\n",
        "# A-tier: Vectorstore-backed memory\n",
        "This means storing the personality preferences in a data structure that's easy to look up. In the case of LLMs, since Q&A is unstructured data and queries, a vector database might come to mind. This is not the only option you have (you can always convert unstructured queries to structured, and query your data in SQL or other ways).\n",
        "\n",
        "But, vector databases are the hot new thing and I don't think I can convince you otherwise, so we'll be using a lightweight Pinecone setup to store our user's personality bank, and only query the relevant parts when the assistant looks at the user's query"
      ],
      "metadata": {
        "id": "kPJbiz1uL0rI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we set up Pinecone as our vector database."
      ],
      "metadata": {
        "id": "pvsn-rCXaS_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step one is to install Pinecone."
      ],
      "metadata": {
        "id": "bY0YE52ciBJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pinecone-client --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1p84v5zaSTf",
        "outputId": "02f4ff8b-2a8c-4e39-ac02-2077ad7fd203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/214.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m153.6/214.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.5/214.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, save our Pinecone API key."
      ],
      "metadata": {
        "id": "QC0TB9Y2iqv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PINECONE_API_KEY = getpass.getpass('Enter your Pinecone API key: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PibTMQ7TivHO",
        "outputId": "788d0ea3-cba2-4f8c-c621-b85efde2cbbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Pinecone API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's setup our Pinecone SDK client. We'll be using Pinecone's new serverless architecture (because it's cheap and free lol)"
      ],
      "metadata": {
        "id": "3EqOXUP-iifw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)"
      ],
      "metadata": {
        "id": "dOpZZffOik3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we have to create an index. An index for a vector database is a way to organize embeddings in an efficient manner for search. Popular indexing algorithms are: HNSW, IVF, etc.. I personally use HNSW with Supabase (which is just a Postgres table and pg-vector extension underneath) It's great for my usecase."
      ],
      "metadata": {
        "id": "_ZQ5QE8-mGbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = \"user-preferences\"\n",
        "\n",
        "EMBEDDINGS_DIMENSIONS = 512\n",
        "\n",
        "# Just checking to see if this index already exist\n",
        "existing_indexes = pc.list_indexes().names()\n",
        "\n",
        "# If index doesn't exist yet, then delete it and create one (we're starting from scratch)\n",
        "if index_name in existing_indexes:\n",
        "  pc.delete_index(index_name)\n",
        "  print(\"Deleted index.\")\n",
        "\n",
        "pc.create_index(\n",
        "    name=index_name,\n",
        "    dimension=EMBEDDINGS_DIMENSIONS,\n",
        "    metric=\"cosine\",\n",
        "    spec=ServerlessSpec(\n",
        "        cloud='aws',\n",
        "        region='us-east-1'\n",
        "    )\n",
        ")\n",
        "print(\"Created index.\")\n",
        "\n",
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qomzuZ88mvrJ",
        "outputId": "033a151f-8134-4177-c897-bd039b481459"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted index.\n",
            "Created index.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice a few things about this setup:\n",
        "- \"cosine\" stands for cosine similarity, which is a way to calculate distance between vectors in 3D space. It's just one way to do vector search.\n",
        "- dimension: How many dimensions does your embedding model create for each piece of text? Heavily fine-tuned models with high quality data can get away with lower numbers (like OpenAI's recent `text-embedding-3-large` can throw away half the dimensions and still perform as well as dumber models that require more dimensions to represent the same concept ([read more here](https://openai.com/index/new-embedding-models-and-api-updates)))\n",
        "- We went with 512 because that's an optimal number for our chosen embeddings model. Lower dimensions also saves your database storage and potentially save you lots of money."
      ],
      "metadata": {
        "id": "p-q6zMZQnA0k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to setup our embedding model so we can easily convert text to numbers for easy vector search.\n",
        "\n",
        "Here I'm going to use LangChain because there's some math involved with cutting down number of dimensions for OpenAI's embeddings that I don't remember lol. LangChain takes care of that for me.\n",
        "\n",
        "They recently separated a lot of the 3rd party abstractions into their own Python packages, so I'll just use the OpenAI wrapper for this one."
      ],
      "metadata": {
        "id": "1SD4371MoBaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain-openai --quiet"
      ],
      "metadata": {
        "id": "T-cc8XJZoVr2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7f85d9d-bc25-46ef-a6ff-376560d1b6dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.8/120.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embedding_client = OpenAIEmbeddings(api_key=OPENAI_API_KEY,\n",
        "    model=\"text-embedding-3-small\", dimensions=EMBEDDINGS_DIMENSIONS)"
      ],
      "metadata": {
        "id": "KgSmQ5YpoTPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some people like to shit on LangChain but it's great for the majority of use cases and prototyping. I don't want to write all the utility functions from scratch every time. Here, every time I need to generate embeddings, I can just call `embedding_client.embed_query(\"MY QUERY\")` and it's done. Boom."
      ],
      "metadata": {
        "id": "XmDLmSpBooAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, now we update our chat pipeline to do a search of the user's existing preferences in the vector index to get similar things. This will help our LLM make a better decision when recommending things.\n",
        "\n",
        "We'll need to:\n",
        "- Embed the latest user query. This allows us to use the embeddings to find similar data in user's preferences.\n",
        "- Find the list of existing preferences.\n",
        "- Let the LLM know that these are the preferences that are relevant to the latest query."
      ],
      "metadata": {
        "id": "3ItS8iWPaaC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "\n",
        "# Declare this here just to wipe the slate clean again.\n",
        "extracted_personality = {\"likes\": [], \"dislikes\": []}\n",
        "\n",
        "def print_ai_answer(user_input: str):\n",
        "  # Clear the current messages history for fair testing :)\n",
        "  messages_history = []\n",
        "  print(\"Messages history cleared.\")\n",
        "\n",
        "  # Extract relevant personality info and update personality dictionary\n",
        "  new_personalities = json.loads(extract_personality(user_input))\n",
        "  print(\"Done extracting personality.\")\n",
        "  extracted_personality[\"likes\"].extend(new_personalities[\"likes\"])\n",
        "  extracted_personality[\"dislikes\"].extend(new_personalities[\"dislikes\"])\n",
        "\n",
        "  # Embed each new personality item, give them metadata so we can filter later, and upsert them to our vector database\n",
        "  embeddings = []\n",
        "  for dislike in extracted_personality[\"dislikes\"]:\n",
        "    text_to_embed = f\"The user dislikes {dislike}\"\n",
        "    current_embeddings = embedding_client.embed_query(text_to_embed)\n",
        "\n",
        "    dislike_with_metadata = {\n",
        "        \"id\": str(uuid.uuid4()), \"values\": current_embeddings, \"metadata\": {\"type\": \"dislikes\", \"content\": dislike}\n",
        "    }\n",
        "    embeddings.append(dislike_with_metadata)\n",
        "\n",
        "  for like in extracted_personality[\"likes\"]:\n",
        "    text_to_embed = f\"The user likes {like}\"\n",
        "    current_embeddings = embedding_client.embed_query(text_to_embed)\n",
        "\n",
        "    dislike_with_metadata = {\n",
        "        \"id\": str(uuid.uuid4()), \"values\": current_embeddings, \"metadata\": {\"type\": \"likes\", \"content\": like}\n",
        "    }\n",
        "    embeddings.append(dislike_with_metadata)\n",
        "\n",
        "  # Push all of our embeddings (likes and dislikes) to vector database (Pinecone)\n",
        "  index.upsert(vectors=embeddings)\n",
        "\n",
        "  # Embed the user's question so we can compare to our embedded personality items\n",
        "  user_query_embedded = embedding_client.embed_query(user_input)\n",
        "\n",
        "  # Search for relevant personalities. Here, to make my life easier, we just look up things the person likes\n",
        "  likes_filter={\n",
        "        \"type\": {\"$eq\": \"likes\"}\n",
        "    }\n",
        "\n",
        "  found_likes = index.query(\n",
        "      vector=user_query_embedded,\n",
        "      filter=likes_filter,\n",
        "      top_k=1, # we only want one piece of personality trait from our bank\n",
        "      include_values=True,\n",
        "      include_metadata=True\n",
        "  )\n",
        "\n",
        "\n",
        "  def get_content_out_of_pinecone_query_result():\n",
        "    content_list = []\n",
        "    # Loop through each match in the query result\n",
        "    for match in found_likes.get('matches', []):\n",
        "        # Get the 'metadata' dictionary from the match\n",
        "        metadata = match.get('metadata', {})\n",
        "        # Extract the 'content' from the 'metadata'\n",
        "        if 'content' in metadata:\n",
        "            content_list.append(metadata['content'])\n",
        "\n",
        "    return content_list\n",
        "\n",
        "  found_likes_formatted = get_content_out_of_pinecone_query_result()\n",
        "\n",
        "  print(\"Found the following relevant things that the user liked in the past:\")\n",
        "  pprint(found_likes_formatted)\n",
        "\n",
        "  # Insert our user's likes into the messages_history list\n",
        "  messages_history.append({\"role\": \"assistant\", \"content\": f\"The user really likes: <user_preferences>{found_likes_formatted}</user_preferences>. This is what you know about the user. Use it in your answer.\"})\n",
        "\n",
        "  # Generate a final answer\n",
        "  for chunk in get_ai_answer(user_input, messages_history):\n",
        "      print(chunk, end=\"\")"
      ],
      "metadata": {
        "id": "ZoalM0fcayfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how I added more information to each piece of personality before embedding? This is me enriching the meanings of each one so that we get a better accuracy rate when searching for them. It's better to embed \"skiing\" in the context of \"He likes this\" and not on it's own.\n",
        "\n",
        "Also note that I save the exact text in each lists (likes and dislikes) again in my vector database. In a production environment, this is costly and not necessary because you can always query by ID and get the original source. But here, it makes my life easier."
      ],
      "metadata": {
        "id": "nMNKoldbtXXv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's just ramble on and on about ourselves so that the LLM has something to extract and store."
      ],
      "metadata": {
        "id": "Vazq-3gBbTo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_ai_answer(\"I just got back from a run and man it was exciting. Stopped by a donut shop. Ever heard of Cops donuts? It's incredible. I didn't like how my shoes fell off at the end, but the scenery was spectacular. I could go for hours if only I didn't eat too much cheese. I never eat cheese.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdY_pRzqcUTH",
        "outputId": "2d9fff50-d848-4ffd-c0bc-32cb84fdc9d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Messages history cleared.\n",
            "Done extracting personality.\n",
            "Found the following relevant things that the user liked in the past:\n",
            "[]\n",
            "It sounds like you had quite an eventful run! Cops donuts is a unique name for a donut shop—its charm is definitely enticing! It's unfortunate about your shoes, though; maybe it's time to look into a pair that fits better or has more secure fastenings, especially if you're planning on enjoying more long scenic runs.\n",
            "\n",
            "Since you mentioned not eating cheese, steering clear of heavy foods before a run might be a good idea to keep you light and energized. How often do you go running, and do you have any particular routes or routines you prefer?"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's tell it even more about us. Remember, every chat session is new with no prior messages. And notice how our array when queried is currently empty? That's becase our upsert operation was async, and we didn't wait for it to complete before querying.\n",
        "\n",
        "That's fine. Next example is we're telling our assistant more information about ourselves. Notice how there's some things about us that it found by similarity searching in vector database."
      ],
      "metadata": {
        "id": "AsPE7VXJ0gnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_ai_answer(\"I miss traveling. I think New York city is beautiful. I just enjoy being in big cities\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93r4Mep40lWv",
        "outputId": "5bd4057d-42ba-4d66-87f2-4c0f4a059dba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Messages history cleared.\n",
            "Done extracting personality.\n",
            "Found the following relevant things that the user liked in the past:\n",
            "['New York city']\n",
            "That's completely understandable! There's something truly magical about the energy and vibrancy of New York City. Each borough offers a unique flavor, and the never-ending opportunities to explore arts, culture, and culinary delights make it a dream destination for many. If you're missing the experience of traveling, perhaps you could revisit some of your favorite spots through movies, books, or even planning a future trip. Delving into some New York-themed novels or films might help recreate that special big-city ambiance at home. Is there a particular part of the city or type of activity in New York that you enjoy the most?"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it has a bank of our preferences, we then ask for recommendations."
      ],
      "metadata": {
        "id": "GZozkJ62cUmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_ai_answer(\"I need to eat something\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stBw3AAjcXTC",
        "outputId": "9428733c-2c4f-4175-d01e-4014aba16dbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Messages history cleared.\n",
            "Done extracting personality.\n",
            "Found the following relevant things that the user liked in the past:\n",
            "['Cops donuts']\n",
            "Since you enjoy cop donuts, maybe you could grab a donut or two! They make for a quick and satisfying snack. If you're looking for something more substantial, how about pairing it with a cup of coffee or hot chocolate? Enjoy your treat!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how the preferences that are pulled in are only the ones relevant to the conversation? There are a million things to a person, but at any given time, you only need to know enough to give them an answer.\n",
        "\n",
        "Now our LLM can be fed just the right data without being overwhelmed, so you can get the best performance out of each user's question."
      ],
      "metadata": {
        "id": "ckOvt1VScXtX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's all for today. Let me know if you have any questions about this topic.\n",
        "\n",
        "If you'd like to connect with me, here are my details:\n",
        "\n",
        "- LinkedIn: https://www.linkedin.com/in/haiphunghiem\n",
        "- GitHub: https://github.com/trancethehuman\n",
        "- Twitter / X: https://twitter.com/haithehuman\n",
        "- YouTube: https://www.youtube.com/@devlearnllm/videos"
      ],
      "metadata": {
        "id": "Y67bVYYC5ftR"
      }
    }
  ]
}
