{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keys import OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, OpenAI, LLMChain\n",
    "\n",
    "prompt_template = \"What is a word to replace the following: {word}?\"\n",
    "\n",
    "# Set the \"OPENAI_API_KEY\" environment variable before running following line.\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate.from_template(prompt_template)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'artificial', 'text': '\\n\\nSynthetic'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain(\"artificial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '\\n\\nSynthetic'}, {'text': '\\n\\nWisdom'}, {'text': '\\n\\nAutomaton'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_list = [\n",
    "    {\"word\": \"artificial\"},\n",
    "    {\"word\": \"intelligence\"},\n",
    "    {\"word\": \"robot\"}\n",
    "]\n",
    "\n",
    "llm_chain.apply(input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='\\n\\nSynthetic', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWisdom', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nAutomaton', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'prompt_tokens': 33, 'total_tokens': 46, 'completion_tokens': 13}, 'model_name': 'text-davinci-003'}, run=RunInfo(run_id=UUID('05b75909-1786-46a8-9b11-06480b90e204')))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.generate(input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nventilator'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"Looking at the context of '{context}'. What is an appropriate word to replace the following: {word}?\"\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=prompt_template, input_variables=[\"word\", \"context\"]))\n",
    "\n",
    "llm_chain.predict(word=\"fan\", context=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nAdmirer'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(word=\"fan\", context=\"humans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Synthetic',\n",
       " 'Manufactured',\n",
       " 'Imitation',\n",
       " 'Fabricated',\n",
       " 'Fake',\n",
       " 'Mechanical',\n",
       " 'Computerized',\n",
       " 'Automated',\n",
       " 'Simulated',\n",
       " 'Artificial Intelligence',\n",
       " 'Constructed',\n",
       " 'Programmed',\n",
       " 'Processed',\n",
       " 'Algorithmic.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "template = \"\"\"List all possible words as substitute for 'artificial' as comma separated.\"\"\"\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=template, output_parser=output_parser, input_variables=[]),\n",
    "    output_parser=output_parser)\n",
    "\n",
    "llm_chain.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversational Chain (Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Synthetic, robotic, manufactured, simulated, computerized, programmed, man-made, fabricated, contrived, and artificial.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "chain_one = conversation.predict(input=\"List all possible words as substitute for 'artificial' as comma separated.\")\n",
    "chain_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Imitation, virtual, constructed, and artificial intelligence.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_two = conversation.predict(input=\"And the next 4?\")\n",
    "chain_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'chains'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m \u001b[39mimport\u001b[39;00m SimpleSequentialChain\n\u001b[0;32m----> 3\u001b[0m overall_chain \u001b[39m=\u001b[39m SimpleSequentialChain(chains\u001b[39m=\u001b[39;49m[chain_one, chain_two])\n",
      "File \u001b[0;32m~/mambaforge/envs/llm-env/lib/python3.11/site-packages/langchain/load/serializable.py:64\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/mambaforge/envs/llm-env/lib/python3.11/site-packages/pydantic/main.py:339\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/mambaforge/envs/llm-env/lib/python3.11/site-packages/pydantic/main.py:1102\u001b[0m, in \u001b[0;36mpydantic.main.validate_model\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/mambaforge/envs/llm-env/lib/python3.11/site-packages/langchain/chains/sequential.py:155\u001b[0m, in \u001b[0;36mSimpleSequentialChain.validate_chains\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39m@root_validator\u001b[39m()\n\u001b[1;32m    153\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidate_chains\u001b[39m(\u001b[39mcls\u001b[39m, values: Dict) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict:\n\u001b[1;32m    154\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Validate that chains are all single input/output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m     \u001b[39mfor\u001b[39;00m chain \u001b[39min\u001b[39;00m values[\u001b[39m\"\u001b[39;49m\u001b[39mchains\u001b[39;49m\u001b[39m\"\u001b[39;49m]:\n\u001b[1;32m    156\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(chain\u001b[39m.\u001b[39minput_keys) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    157\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    158\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mChains used in SimplePipeline should all have one input, got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mchain\u001b[39m}\u001b[39;00m\u001b[39m with \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(chain\u001b[39m.\u001b[39minput_keys)\u001b[39m}\u001b[39;00m\u001b[39m inputs.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m             )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'chains'"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[chain_one, chain_two])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Debug Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mList all possible words as substitute for 'artificial' as comma separated.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "\n",
      "Answer briefly. write the first 3 options.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nSynthetic, Manufactured, Imitation'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set `verbose=True` to debug chains\n",
    "template = \"\"\"List all possible words as substitute for 'artificial' as comma separated.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "\n",
    "{input}\"\"\"\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=template, input_variables=[\"history\", \"input\"], output_parser=output_parser),\n",
    "    memory=ConversationBufferMemory(),\n",
    "    verbose=True)\n",
    "\n",
    "conversation.predict(input=\"Answer briefly. write the first 3 options.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.base import Chain\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "class ConcatenateChain(Chain):\n",
    "    chain_1: LLMChain\n",
    "    chain_2: LLMChain\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        # Union of the input keys of the two chains.\n",
    "        all_input_vars = set(self.chain_1.input_keys).union(set(self.chain_2.input_keys))\n",
    "        return list(all_input_vars)\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return ['concat_output']\n",
    "\n",
    "    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n",
    "        output_1 = self.chain_1.run(inputs)\n",
    "        output_2 = self.chain_2.run(inputs)\n",
    "        return {'concat_output': output_1 + output_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated output:\n",
      "\n",
      "\n",
      "Artificial means something that is not natural or made by humans, but rather created or produced by artificial means.\n",
      "\n",
      "Synthetic\n"
     ]
    }
   ],
   "source": [
    "prompt_1 = PromptTemplate(\n",
    "    input_variables=[\"word\"],\n",
    "    template=\"What is the meaning of the following word '{word}'?\",\n",
    ")\n",
    "chain_1 = LLMChain(llm=llm, prompt=prompt_1)\n",
    "\n",
    "prompt_2 = PromptTemplate(\n",
    "    input_variables=[\"word\"],\n",
    "    template=\"What is a word to replace the following: {word}?\",\n",
    ")\n",
    "chain_2 = LLMChain(llm=llm, prompt=prompt_2)\n",
    "\n",
    "concat_chain = ConcatenateChain(chain_1=chain_1, chain_2=chain_2)\n",
    "concat_output = concat_chain.run(\"artificial\")\n",
    "print(f\"Concatenated output:\\n{concat_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
