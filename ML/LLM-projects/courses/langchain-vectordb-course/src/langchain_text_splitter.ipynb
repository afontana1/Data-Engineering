{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keys import OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"The One Page Linux Manual.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='THE ONE     PAGE LINUX MANUALA summary of useful Linux commands\\nVersion 3.0 May 1999 squadron@powerup.com.au\\nStarting & Stopping\\nshutdown -h now Shutdown the system now and do not\\nreboot\\nhalt Stop all processes - same as above\\nshutdown -r 5 Shutdown the system in 5 minutes and\\nreboot\\nshutdown -r now Shutdown the system now and reboot\\nreboot Stop all processes and then reboot - same\\nas above\\nstartx Start the X system\\nAccessing & mounting file systems\\nmount -t iso9660 /dev/cdrom\\n/mnt/cdromMount the device cdrom\\nand call it cdrom under the\\n/mnt directory\\nmount -t msdos /dev/hdd\\n/mnt/ddriveMount hard disk “d” as a\\nmsdos file system and call\\nit ddrive under the /mnt\\ndirectory\\nmount -t vfat /dev/hda1\\n/mnt/cdriveMount hard disk “a” as a\\nVFAT file system and call it\\ncdrive under the /mnt\\ndirectory\\numount /mnt/cdrom Unmount the cdrom\\nFinding files and text within files\\nfind / -name  fname Starting with the root directory, look\\nfor the file called fname\\nfind / -name ”*fname* ” Starting with the root directory, look\\nfor the file containing the string fname\\nlocate missingfilename Find a file called missingfilename\\nusing the locate command - this\\nassumes you have already used the\\ncommand updatedb (see next)\\nupdatedb Create or update the database of files\\non all file systems attached to the linux\\nroot directory\\nwhich missingfilename Show the subdirectory containing the\\nexecutable file  called missingfilename\\ngrep textstringtofind\\n/dirStarting with the directory called dir ,\\nlook for and list all files containing\\ntextstringtofind\\nThe X Window System\\nxvidtune Run the X graphics tuning utility\\nXF86Setup Run the X configuration menu with\\nautomatic probing of graphics cards\\nXconfigurator Run another X configuration menu with\\nautomatic probing of graphics cards\\nxf86config Run a text based X configuration menu\\nMoving, copying, deleting & viewing files\\nls -l List files in current directory using\\nlong format\\nls -F List files in current directory and\\nindicate the file type\\nls -laC List all files in current directory in\\nlong format and display in columnsrm name Remove a file or directory called\\nname\\nrm -rf name Kill off an entire directory and all it’s\\nincludes files and subdirectories\\ncp filename\\n/home/dirnameCopy the file called filename to the\\n/home/dirname directory\\nmv filename\\n/home/dirnameMove the file called filename to the\\n/home/dirname directory\\ncat filetoview Display the file called filetoview\\nman -k keyword Display man pages containing\\nkeyword\\nmore filetoview Display the file called filetoview one\\npage at a time, proceed to next page\\nusing the spacebar\\nhead filetoview Display the first 10 lines of the file\\ncalled filetoview\\nhead -20 filetoview Display the first 20 lines of the file\\ncalled filetoview\\ntail filetoview Display the last 10 lines of the file\\ncalled filetoview\\ntail -20 filetoview Display the last 20 lines of the file\\ncalled filetoview\\nInstalling software for Linux\\nrpm -ihv name.rpm Install the rpm package called name\\nrpm -Uhv name.rpm Upgrade the rpm package called\\nname\\nrpm -e package Delete the rpm package called\\npackage\\nrpm -l package List the files in the package called\\npackage\\nrpm -ql package List the files and state the installed\\nversion of the package called\\npackage\\nrpm -i --force package Reinstall the rpm package called\\nname having deleted parts of it (not\\ndeleting using rpm -e)\\ntar -zxvf archive.tar.gz or\\ntar -zxvf archive.tgzDecompress the files contained in\\nthe zipped and tarred archive called\\narchive\\n./configure Execute the script preparing the\\ninstalled files for compiling\\nUser Administration\\nadduser accountname Create a new user call accountname\\npasswd accountname Give accountname a new password\\nsu Log in as superuser from current login\\nexit Stop being superuser and revert to\\nnormal user\\nLittle known tips and tricks\\nifconfig List ip addresses for all devices on\\nthe machine\\napropos subject List manual pages for subject\\nusermount Executes graphical application for\\nmounting and unmounting file\\nsystems' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "You have 2 documents\n",
      "Preview:\n",
      "THE ONE     PAGE LINUX MANUALA summary of useful Linux commands\n",
      "Version 3.0 May 1999 squadron@powerup.com.au\n",
      "Starting & Stopping\n",
      "shutdown -h now Shutdown the system now and do not\n",
      "reboot\n",
      "halt Stop all processes - same as above\n",
      "shutdown -r 5 Shutdown the system in 5 minutes and\n",
      "reboot\n",
      "shutdown -r now Shutdown the system now and reboot\n",
      "reboot Stop all processes and then reboot - same\n",
      "as above\n",
      "startx Start the X system\n",
      "Accessing & mounting file systems\n",
      "mount -t iso9660 /dev/cdrom\n",
      "/mnt/cdromMount the device cdrom\n",
      "and call it cdrom under the\n",
      "/mnt directory\n",
      "mount -t msdos /dev/hdd\n",
      "/mnt/ddriveMount hard disk “d” as a\n",
      "msdos file system and call\n",
      "it ddrive under the /mnt\n",
      "directory\n",
      "mount -t vfat /dev/hda1\n",
      "/mnt/cdriveMount hard disk “a” as a\n",
      "VFAT file system and call it\n",
      "cdrive under the /mnt\n",
      "directory\n",
      "umount /mnt/cdrom Unmount the cdrom\n",
      "Finding files and text within files\n",
      "find / -name  fname Starting with the root directory, look\n",
      "for the file called fname\n",
      "find / -name ”*fname* ” Starting with the root directory, look\n",
      "for the file containing the string fname\n",
      "locate missingfilename Find a file called missingfilename\n",
      "using the locate command - this\n",
      "assumes you have already used the\n",
      "command updatedb (see next)\n",
      "updatedb Create or update the database of files\n",
      "on all file systems attached to the linux\n",
      "root directory\n",
      "which missingfilename Show the subdirectory containing the\n",
      "executable file  called missingfilename\n",
      "grep textstringtofind\n",
      "/dirStarting with the directory called dir ,\n",
      "look for and list all files containing\n",
      "textstringtofind\n",
      "The X Window System\n",
      "xvidtune Run the X graphics tuning utility\n",
      "XF86Setup Run the X configuration menu with\n",
      "automatic probing of graphics cards\n",
      "Xconfigurator Run another X configuration menu with\n",
      "automatic probing of graphics cards\n",
      "xf86config Run a text based X configuration menu\n",
      "Moving, copying, deleting & viewing files\n",
      "ls -l List files in current directory using\n",
      "long format\n",
      "ls -F List files in current directory and\n",
      "indicate the file type\n",
      "ls -laC List all files in current directory in\n",
      "long format and display in columnsrm name Remove a file or directory called\n",
      "name\n",
      "rm -rf name Kill off an entire directory and all it’s\n",
      "includes files and subdirectories\n",
      "cp filename\n",
      "/home/dirnameCopy the file called filename to the\n",
      "/home/dirname directory\n",
      "mv filename\n",
      "/home/dirnameMove the file called filename to the\n",
      "/home/dirname directory\n",
      "cat filetoview Display the file called filetoview\n",
      "man -k keyword Display man pages containing\n",
      "keyword\n",
      "more filetoview Display the file called filetoview one\n",
      "page at a time, proceed to next page\n",
      "using the spacebar\n",
      "head filetoview Display the first 10 lines of the file\n",
      "called filetoview\n",
      "head -20 filetoview Display the first 20 lines of the file\n",
      "called filetoview\n",
      "tail filetoview Display the last 10 lines of the file\n",
      "called filetoview\n",
      "tail -20 filetoview Display the last 20 lines of the file\n",
      "called filetoview\n",
      "Installing software for Linux\n",
      "rpm -ihv name.rpm Install the rpm package called name\n",
      "rpm -Uhv name.rpm Upgrade the rpm package called\n",
      "name\n",
      "rpm -e package Delete the rpm package called\n",
      "package\n",
      "rpm -l package List the files in the package called\n",
      "package\n",
      "rpm -ql package List the files and state the installed\n",
      "version of the package called\n",
      "package\n",
      "rpm -i --force package Reinstall the rpm package called\n",
      "name having deleted parts of it (not\n",
      "deleting using rpm -e)\n",
      "tar -zxvf archive.tar.gz or\n",
      "tar -zxvf archive.tgzDecompress the files contained in\n",
      "the zipped and tarred archive called\n",
      "archive\n",
      "./configure Execute the script preparing the\n",
      "installed files for compiling\n",
      "User Administration\n",
      "adduser accountname Create a new user call accountname\n",
      "passwd accountname Give accountname a new password\n",
      "su Log in as superuser from current login\n",
      "exit Stop being superuser and revert to\n",
      "normal user\n",
      "Little known tips and tricks\n",
      "ifconfig List ip addresses for all devices on\n",
      "the machine\n",
      "apropos subject List manual pages for subject\n",
      "usermount Executes graphical application for\n",
      "mounting and unmounting file\n",
      "systems\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "texts = text_splitter.split_documents(pages)\n",
    "\n",
    "print(texts[0])\n",
    "\n",
    "print (f\"You have {len(texts)} documents\")\n",
    "\n",
    "print (\"Preview:\")\n",
    "print (texts[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pypdf import PdfReader\n",
    "# Load a long document\n",
    "reader = PdfReader(\"The One Page Linux Manual.pdf\")\n",
    "sample_text = ''\n",
    "for page in reader.pages:\n",
    "    sample_text += page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='THE ONE     PAGE LINUX MANUALA summary of useful', metadata={}), Document(page_content='of useful Linux commands', metadata={}), Document(page_content='Version 3.0 May 1999 squadron@powerup.com.au', metadata={}), Document(page_content='Starting & Stopping', metadata={}), Document(page_content='shutdown -h now Shutdown the system now and do', metadata={}), Document(page_content='and do not', metadata={}), Document(page_content='reboot\\nhalt Stop all processes - same as above', metadata={}), Document(page_content='shutdown -r 5 Shutdown the system in 5 minutes', metadata={}), Document(page_content='5 minutes and', metadata={}), Document(page_content='reboot', metadata={}), Document(page_content='shutdown -r now Shutdown the system now and', metadata={}), Document(page_content='now and reboot', metadata={}), Document(page_content='reboot Stop all processes and then reboot - same', metadata={}), Document(page_content='as above\\nstartx Start the X system', metadata={}), Document(page_content='Accessing & mounting file systems', metadata={}), Document(page_content='mount -t iso9660 /dev/cdrom', metadata={}), Document(page_content='/mnt/cdromMount the device cdrom', metadata={}), Document(page_content='and call it cdrom under the\\n/mnt directory', metadata={}), Document(page_content='mount -t msdos /dev/hdd', metadata={}), Document(page_content='/mnt/ddriveMount hard disk “d” as a', metadata={}), Document(page_content='msdos file system and call', metadata={}), Document(page_content='it ddrive under the /mnt\\ndirectory', metadata={}), Document(page_content='directory\\nmount -t vfat /dev/hda1', metadata={}), Document(page_content='/mnt/cdriveMount hard disk “a” as a', metadata={}), Document(page_content='VFAT file system and call it', metadata={}), Document(page_content='cdrive under the /mnt\\ndirectory', metadata={}), Document(page_content='directory\\numount /mnt/cdrom Unmount the cdrom', metadata={}), Document(page_content='Finding files and text within files', metadata={}), Document(page_content='find / -name  fname Starting with the root', metadata={}), Document(page_content='the root directory, look', metadata={}), Document(page_content='for the file called fname', metadata={}), Document(page_content='find / -name ”*fname* ” Starting with the root', metadata={}), Document(page_content='the root directory, look', metadata={}), Document(page_content='for the file containing the string fname', metadata={}), Document(page_content='locate missingfilename Find a file called', metadata={}), Document(page_content='called missingfilename', metadata={}), Document(page_content='using the locate command - this', metadata={}), Document(page_content='assumes you have already used the', metadata={}), Document(page_content='command updatedb (see next)', metadata={}), Document(page_content='updatedb Create or update the database of files', metadata={}), Document(page_content='on all file systems attached to the linux', metadata={}), Document(page_content='root directory', metadata={}), Document(page_content='which missingfilename Show the subdirectory', metadata={}), Document(page_content='containing the', metadata={}), Document(page_content='executable file  called missingfilename', metadata={}), Document(page_content='grep textstringtofind', metadata={}), Document(page_content='/dirStarting with the directory called dir ,', metadata={}), Document(page_content='look for and list all files containing', metadata={}), Document(page_content='textstringtofind\\nThe X Window System', metadata={}), Document(page_content='xvidtune Run the X graphics tuning utility', metadata={}), Document(page_content='XF86Setup Run the X configuration menu with', metadata={}), Document(page_content='automatic probing of graphics cards', metadata={}), Document(page_content='Xconfigurator Run another X configuration menu', metadata={}), Document(page_content='menu with', metadata={}), Document(page_content='automatic probing of graphics cards', metadata={}), Document(page_content='xf86config Run a text based X configuration menu', metadata={}), Document(page_content='Moving, copying, deleting & viewing files', metadata={}), Document(page_content='ls -l List files in current directory using', metadata={}), Document(page_content='long format', metadata={}), Document(page_content='ls -F List files in current directory and', metadata={}), Document(page_content='indicate the file type', metadata={}), Document(page_content='ls -laC List all files in current directory in', metadata={}), Document(page_content='long format and display in columnsrm name Remove', metadata={}), Document(page_content='Remove a file or directory called', metadata={}), Document(page_content='name', metadata={}), Document(page_content='rm -rf name Kill off an entire directory and all', metadata={}), Document(page_content='and all it’s', metadata={}), Document(page_content='includes files and subdirectories\\ncp filename', metadata={}), Document(page_content='/home/dirnameCopy the file called filename to the', metadata={}), Document(page_content='/home/dirname directory\\nmv filename', metadata={}), Document(page_content='/home/dirnameMove the file called filename to the', metadata={}), Document(page_content='/home/dirname directory', metadata={}), Document(page_content='cat filetoview Display the file called filetoview', metadata={}), Document(page_content='man -k keyword Display man pages containing', metadata={}), Document(page_content='keyword', metadata={}), Document(page_content='more filetoview Display the file called', metadata={}), Document(page_content='called filetoview one', metadata={}), Document(page_content='page at a time, proceed to next page', metadata={}), Document(page_content='using the spacebar', metadata={}), Document(page_content='head filetoview Display the first 10 lines of the', metadata={}), Document(page_content='of the file', metadata={}), Document(page_content='called filetoview', metadata={}), Document(page_content='head -20 filetoview Display the first 20 lines of', metadata={}), Document(page_content='lines of the file', metadata={}), Document(page_content='called filetoview', metadata={}), Document(page_content='tail filetoview Display the last 10 lines of the', metadata={}), Document(page_content='of the file', metadata={}), Document(page_content='called filetoview', metadata={}), Document(page_content='tail -20 filetoview Display the last 20 lines of', metadata={}), Document(page_content='lines of the file', metadata={}), Document(page_content='called filetoview\\nInstalling software for Linux', metadata={}), Document(page_content='rpm -ihv name.rpm Install the rpm package called', metadata={}), Document(page_content='called name', metadata={}), Document(page_content='rpm -Uhv name.rpm Upgrade the rpm package called', metadata={}), Document(page_content='name\\nrpm -e package Delete the rpm package called', metadata={}), Document(page_content='package', metadata={}), Document(page_content='rpm -l package List the files in the package', metadata={}), Document(page_content='package called', metadata={}), Document(page_content='package', metadata={}), Document(page_content='rpm -ql package List the files and state the', metadata={}), Document(page_content='state the installed', metadata={}), Document(page_content='version of the package called\\npackage', metadata={}), Document(page_content='rpm -i --force package Reinstall the rpm package', metadata={}), Document(page_content='package called', metadata={}), Document(page_content='name having deleted parts of it (not', metadata={}), Document(page_content='deleting using rpm -e)', metadata={}), Document(page_content='tar -zxvf archive.tar.gz or', metadata={}), Document(page_content='tar -zxvf archive.tgzDecompress the files', metadata={}), Document(page_content='the files contained in', metadata={}), Document(page_content='the zipped and tarred archive called\\narchive', metadata={}), Document(page_content='./configure Execute the script preparing the', metadata={}), Document(page_content='installed files for compiling\\nUser Administration', metadata={}), Document(page_content='adduser accountname Create a new user call', metadata={}), Document(page_content='user call accountname', metadata={}), Document(page_content='passwd accountname Give accountname a new', metadata={}), Document(page_content='a new password', metadata={}), Document(page_content='su Log in as superuser from current login', metadata={}), Document(page_content='exit Stop being superuser and revert to', metadata={}), Document(page_content='normal user\\nLittle known tips and tricks', metadata={}), Document(page_content='ifconfig List ip addresses for all devices on', metadata={}), Document(page_content='the machine', metadata={}), Document(page_content='apropos subject List manual pages for subject', metadata={}), Document(page_content='usermount Executes graphical application for', metadata={}), Document(page_content='mounting and unmounting file', metadata={}), Document(page_content='systems/sbin/e2fsck hda5 Execute the filesystem', metadata={}), Document(page_content='check utility', metadata={}), Document(page_content='on partition hda5', metadata={}), Document(page_content='fdformat /dev/fd0H1440 Format the floppy disk in', metadata={}), Document(page_content='disk in device fd0', metadata={}), Document(page_content='tar -cMf /dev/fd0 Backup the contents of the', metadata={}), Document(page_content='of the current', metadata={}), Document(page_content='directory and subdirectories to', metadata={}), Document(page_content='multiple floppy disks', metadata={}), Document(page_content='tail -f /var/log/messages Display the last 10', metadata={}), Document(page_content='last 10 lines of the system', metadata={}), Document(page_content='log.', metadata={}), Document(page_content='cat /var/log/dmesg Display the file containing', metadata={}), Document(page_content='the boot', metadata={}), Document(page_content='time messages - useful for locating', metadata={}), Document(page_content='problems. Alternatively, use the\\ndmesg  command.', metadata={}), Document(page_content='* wildcard - represents everything. eg.', metadata={}), Document(page_content='cp from/* to  will copy all files in the', metadata={}), Document(page_content='from directory to the to directory', metadata={}), Document(page_content='? Single character wildcard. eg.', metadata={}), Document(page_content='cp config.? /configs will copy all files', metadata={}), Document(page_content='beginning with the name config. in', metadata={}), Document(page_content='the current directory to the directory', metadata={}), Document(page_content='named configs.', metadata={}), Document(page_content='[xyz] Choice of character wildcards. eg.', metadata={}), Document(page_content='ls [xyz]* will list all files in the current', metadata={}), Document(page_content='directory starting with the letter x, y,\\nor z.', metadata={}), Document(page_content='linux single At the lilo prompt, start in single', metadata={}), Document(page_content='in single user', metadata={}), Document(page_content='mode. This is useful if you have', metadata={}), Document(page_content='forgotten your password. Boot in', metadata={}), Document(page_content='single user mode, then run the\\npasswd  command.', metadata={}), Document(page_content='ps List current processes', metadata={}), Document(page_content='kill 123 Kill a specific process eg. kill 123', metadata={}), Document(page_content='Configuration files and what they do', metadata={}), Document(page_content='/etc/profile System wide environment variables', metadata={}), Document(page_content='variables for', metadata={}), Document(page_content='all users.', metadata={}), Document(page_content='/etc/fstab List of devices and their associated', metadata={}), Document(page_content='mount', metadata={}), Document(page_content='points. Edit this file to add cdroms, DOS', metadata={}), Document(page_content='partitions and floppy drives at startup.', metadata={}), Document(page_content='/etc/motd Message of the day broadcast to all', metadata={}), Document(page_content='to all users', metadata={}), Document(page_content='at login.', metadata={}), Document(page_content='etc/rc.d/rc.local Bash script that is executed at', metadata={}), Document(page_content='at the end of', metadata={}), Document(page_content='login process. Similar to autoexec.bat in\\nDOS.', metadata={}), Document(page_content='/etc/HOSTNAME Conatins full hostname including', metadata={}), Document(page_content='including domain.', metadata={}), Document(page_content='/etc/cron.* There are 4 directories that', metadata={}), Document(page_content='that automatically', metadata={}), Document(page_content='execute all scripts within the directory at', metadata={}), Document(page_content='intervals of hour, day, week or month.', metadata={}), Document(page_content='/etc/hosts A list of all know host names and IP', metadata={}), Document(page_content='addresses on the machine.', metadata={}), Document(page_content='/etc/httpd/conf Paramters for the Apache web', metadata={}), Document(page_content='web server', metadata={}), Document(page_content='/etc/inittab Specifies the run level that the', metadata={}), Document(page_content='that the machine', metadata={}), Document(page_content='should boot into.', metadata={}), Document(page_content='/etc/resolv.conf Defines IP addresses of DNS', metadata={}), Document(page_content='of DNS servers.', metadata={}), Document(page_content='/etc/smb.conf Config file for the SAMBA server.', metadata={}), Document(page_content='server. Allows', metadata={}), Document(page_content='file and print sharing with Microsoft\\nclients.', metadata={}), Document(page_content='clients.\\n/etc/X11/XF86Confi', metadata={}), Document(page_content='gConfig file for X -Windows.', metadata={}), Document(page_content='~/.xinitrc Defines the windows manager loaded by', metadata={}), Document(page_content='X. ~ refers to user’s home directory.File', metadata={}), Document(page_content='permissions', metadata={}), Document(page_content='If the command ls -l is given, a long list of', metadata={}), Document(page_content='list of file names is', metadata={}), Document(page_content='displayed. The first column in this list details', metadata={}), Document(page_content='details the permissions', metadata={}), Document(page_content='applying to the file. If a permission is missing', metadata={}), Document(page_content='missing for a owner,', metadata={}), Document(page_content='group of other, it is represented by - eg.  drwxr', metadata={}), Document(page_content='drwxr -x—x', metadata={}), Document(page_content='Read = 4\\nWrite = 2', metadata={}), Document(page_content='Execute = 1File permissions are altered by giving', metadata={}), Document(page_content='by giving the', metadata={}), Document(page_content='chmod command and the appropriate', metadata={}), Document(page_content='octal code for each user type. eg', metadata={}), Document(page_content='chmod 7 6 4 filename will make the file', metadata={}), Document(page_content='called filename R+W+X for the owner,', metadata={}), Document(page_content='R+W for the group and R for others.', metadata={}), Document(page_content='chmod 7 5 5 Full permission for the owner, read', metadata={}), Document(page_content='read and', metadata={}), Document(page_content='execute access for the group and others.', metadata={}), Document(page_content='chmod +x filename Make the file called filename', metadata={}), Document(page_content='filename executable', metadata={}), Document(page_content='to all users.\\nX Shortcuts - (mainly for Redhat)', metadata={}), Document(page_content='Control|Alt  + or - Increase or decrease the', metadata={}), Document(page_content='the screen', metadata={}), Document(page_content='resolution. eg. from 640x480 to\\n800x600', metadata={}), Document(page_content='Alt | escape Display list of active windows', metadata={}), Document(page_content='Shift|Control F8 Resize the selected window', metadata={}), Document(page_content='Right click on desktop\\nbackgroundDisplay menu', metadata={}), Document(page_content='Shift|Control Altr Refresh the screen', metadata={}), Document(page_content='Shift|Control Altx Start an xterm session', metadata={}), Document(page_content='Printing', metadata={}), Document(page_content='/etc/rc.d/init.d/lpd start Start the print daemon', metadata={}), Document(page_content='/etc/rc.d/init.d/lpd stop Stop the print daemon', metadata={}), Document(page_content='/etc/rc.d/init.d/lpd', metadata={}), Document(page_content='statusDisplay status of the print daemon', metadata={}), Document(page_content='lpq Display jobs in print queue', metadata={}), Document(page_content='lprm Remove jobs from queue\\nlpr Print a file', metadata={}), Document(page_content='lpc Printer control tool', metadata={}), Document(page_content='man subject | lpr Print the manual page called', metadata={}), Document(page_content='called subject', metadata={}), Document(page_content='as plain text', metadata={}), Document(page_content='man -t subject | lpr Print the manual page called', metadata={}), Document(page_content='called subject', metadata={}), Document(page_content='as Postscript output', metadata={}), Document(page_content='printtool Start X printer setup', metadata={}), Document(page_content='setup interface~/.Xdefaults Define configuration', metadata={}), Document(page_content='for some X -', metadata={}), Document(page_content='applications. ~ refers to user’s home\\ndirectory.', metadata={}), Document(page_content='Get your own Official Linux Pocket Protector -', metadata={}), Document(page_content='- includes', metadata={}), Document(page_content='handy command summary. Visit:', metadata={}), Document(page_content='www.powerup.com.au/~squadron', metadata={})]\n"
     ]
    }
   ],
   "source": [
    "texts = text_splitter.create_documents([sample_text])\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Building LLM applications for productionBuilding LLM applications for production | Chip HuyenChip HuyenBooksBlogList 100MLOps GuideTiếng ViệtBuilding LLM applications for productionApr 11, 2023•Chip Huyen[Hacker News discussion,LinkedIn discussion,Twitter thread]A question that I’ve been asked a lot recently is how large language models (LLMs) will change machine learning workflows. After working with several companies who are working with LLM applications and personally going down a rabbit hole building my applications, I realized two things:It’s easy to make something cool with LLMs, but very hard to make something production-ready with them.LLM limitations are exacerbated by a lack of engineering rigor in prompt engineering, partially due to the ambiguous nature of natural languages, and partially due to the nascent nature of the field.This post consists of three parts.Part 1 discusses the key challenges of productionizing LLM applications and the solutions that I’ve seen.Part 2 discusses how to compose multiple tasks with control flows (e.g. if statement, for loop) and incorporate tools (e.g. SQL executor, bash, web browsers, third-party APIs) for more complex and powerful applications.Part 3 covers some of the promising use cases that I’ve seen companies building on top of LLMs and how to construct them from smaller tasks.There has been so much written about LLMs, so feel free to skip any section you’re already familiar with.Table of contentsPart I. Challenges of productionizing prompt engineering……..The ambiguity of natural languages…………Prompt evaluation…………Prompt versioning…………Prompt optimization……..Cost and latency…………Cost…………Latency…………The impossibility of cost + latency analysis for LLMs……..Prompting vs. finetuning vs. alternatives…………Prompt tuning…………Finetuning with distillation……..Embeddings + vector databases……..Backward and forward compatibilityPart 2. Task composability……..Applications that consist of multiple tasks……..Agents, tools, and control flows…………Tools vs. plugins…………Control flows: sequential, parallel, if, for loop…………Control flow with LLM agents…………Testing an agentPart 3. Promising use cases……..AI assistant……..Chatbot……..Programming and gaming……..Learning……..Talk-to-your-data…………Can LLMs do data analysis for me?……..Search and recommendation……..Sales……..SEOConclusionPart I. Challenges of productionizing prompt engineeringThe ambiguity of natural languagesFor most of the history of computers, engineers have written instructions in programming languages. Programming languages are “mostly” exact. Ambiguity causes frustration and even passionate hatred in developers (think dynamic typing in Python or JavaScript).In prompt engineering, instructions are written in natural languages, which are a lot more flexible than programming languages. This can make for a great user experience, but can lead to a pretty bad developer experience.The flexibility comes from two directions: how users define instructions, and how LLMs respond to these instructions.First, the flexibility in user-defined prompts leads to silent failures. If someone accidentally makes some changes in code, like adding a random character or removing a line, it’ll likely throw an error. However, if someone accidentally changes a prompt, it will still run but give very different outputs.While the flexibility in user-defined prompts is just an annoyance, the ambiguity in LLMs’ generated responses can be a dealbreaker. It leads to two problems:Ambiguous output format: downstream applications on top of LLMs expect outputs in a certain format so that they can parse. We can craft our prompts to be explicit about the output format, but there’s no guarantee that the outputs willalwaysfollow this format.Inconsistency in user experience: when using an application, users expect certain consistency. Imagine an insurance company giving you a different quote every time you check on their website. LLMs are stochastic – there’s no guarantee that an LLM will give you the same output for the same input every time.You can force an LLM to give the same response by settingtemperature = 0, which is, in general, a good practice. While itmostly solves the consistency problem, it doesn’t inspire trust in the system. Imagine a teacher who gives you consistent scores only if that teacher sits in one particular room. If that teacher sits in different rooms, that teacher’s scores for you will be wild.How to solve this ambiguity problem?This seems to be a problem that OpenAI is actively trying to mitigate. They have a notebook with tips on how to increase their models’ reliability.A couple of people who’ve worked with LLMs for years told me that they just accepted this ambiguity and built their workflows around that. It’s a different mindset compared to developing deterministic programs, but not something impossible to get used to.This ambiguity can be mitigated by applying as much engineering rigor as possible. In the rest of this post, we’ll discuss how to make prompt engineering, if not deterministic, systematic.Prompt evaluationA common technique for prompt engineering is to provide in the prompt a few examples and hope that the LLM will generalize from these examples (fewshot learners).As an example, consider trying to give a text a controversy score – it was a fun project that I did to find the correlation between a tweet’s popularity and its controversialness. Here is the shortened prompt with 4 fewshot examples:Example: controversy scorerGiven a text, give it a controversy score from 0 to 10.\\n\\nExamples:\\n\\n1 + 1 = 2\\nControversy score: 0\\n\\nStarting April 15th, only verified accounts on Twitter will be eligible to be in For You recommendations\\nControversy score: 5\\n\\nEveryone has the right to own and use guns\\nControversy score: 9\\n\\nImmigration should be completely banned to protect our country\\nControversy score: 10\\n\\nThe response should follow the format:\\n\\nControversy score: { score }\\nReason: { reason }\\n\\nHere is the text.When doing fewshot learning, two questions to keep in mind:Whether the LLM understands the examples given in the prompt. One way to evaluate this is to input the same examples and see if the model outputs the expected scores. If the model doesn’t perform well on the same examples given in the prompt, it is likely because the prompt isn’t clear – you might want to rewrite the prompt or break the task into smaller tasks (and combine them together, discussed in detail in Part II of this post).Whether the LLM overfits to these fewshot examples.You can evaluate your model on separate examples.One thing I’ve also found useful is to ask models to give examples for which it would give a certain label. For example, I can ask the model to give me examples of texts for which it’d give a score of 4. Then I’d input these examples into the LLM to see if it’ll indeed output 4.from llm import OpenAILLM\\n\\ndef eval_prompt(examples_file, eval_file):\\n    prompt = get_prompt(examples_file)\\n    model = OpenAILLM(prompt=prompt, temperature=0)\\n    compute_rmse(model, examples_file)\\n    compute_rmse(model, eval_file)\\neval_prompt(\"fewshot_examples.txt\", \"eval_examples.txt\")Prompt versioningSmall changes to a prompt can lead to very different results. It’s essential to version and track the performance of each prompt. You can use git to version each prompt and its performance, but I wouldn’t be surprised if there will be tools like MLflow or Weights & Biases for prompt experiments.Prompt optimizationThere have been many papers + blog posts written on how to optimize prompts. I agree with Lilian Weng inher helpful blog postthat most papers on prompt engineering are tricks that can be explained in a few sentences. OpenAI has a great notebook that explains manytips with examples. Here are some of them:Prompt the model to explain or explain step-by-step how it arrives at an answer, a technique known asChain-of-Thoughtor COT (Wei et al., 2022).Tradeoff: COT can increase both latency and cost due to the increased number of output tokens [seeCost and latencysection]Generate many outputs for the same input. Pick the final output by either the majority vote  (also known asself-consistency techniqueby Wang et al., 2023) or you can ask your LLM to pick the best one. In OpenAI API, you can generate multiple responses for the same input by passing in the argumentn(not an ideal API design if you ask me).Break one big prompt into smaller, simpler prompts.Many tools promise to auto-optimize your prompts – they are quite expensive and usually just apply these tricks. One nice thing about these tools is that they’re no code, which makes them appealing to non-coders.Cost and latencyCostThe more explicit detail and examples you put into the prompt, the better the model performance (hopefully), and the more expensive your inference will cost.OpenAI API charges for both the input and output tokens. Depending on the task, a simple prompt might be anything between 300 - 1000 tokens. If you want to include more context, e.g. adding your own documents or info retrieved from the Internet to the prompt, it can easily go up to 10k tokens for the prompt alone.The cost with long prompts isn’t in experimentation but in inference.Experimentation-wise,prompt engineering is a cheap and fast way get something up and running. For example, even if you use GPT-4 with the following setting, your experimentation cost will still be just over $300. The traditional ML cost of collecting data and training models is usually much higher and takes much longer.Prompt: 10k tokens ($0.06/1k tokens)Output: 200 tokens ($0.12/1k tokens)Evaluate on 20 examplesExperiment with 25 different versions of promptsThe cost of LLMOps is in inference.If you use GPT-4 with 10k tokens in input and 200 tokens in output, it’ll be $0.624 / prediction.If you use GPT-3.5-turbo with 4k tokens for both input and output, it’ll be $0.004 / prediction or $4 / 1k predictions.As a thought exercise, in 2021, DoorDash ML models made10 billion predictions a day. If each prediction costs $0.004, that’d be $40 million a day!By comparison, AWS personalization costs about$0.0417 / 1k predictionsand AWS fraud detection costs about$7.5 / 1k predictions[for over 100,000 predictions a month]. AWS services are usually considered prohibitively expensive (and less flexible) for any company of a moderate scale.LatencyInput tokens can be processed in parallel, which means that input length shouldn’t affect the latency that much.However, output length significantly affects latency, which is likely due to output tokens being generated sequentially.Even for extremely short input (51 tokens) and output (1 token), the latency forgpt-3.5-turbois around 500ms. If the output token increases to over 20 tokens, the latency is over 1 second.Here’s an experiment I ran, each setting is run 20 times. All runs happen within 2 minutes. If I do the experiment again, the latency will be very different, but the relationship between the 3 settings should be similar.This is another challenge of productionizing LLM applications using APIs like OpenAI:APIs are very unreliable, and no commitment yet on when SLAs will be provided.# tokensp50 latency (sec)p75 latencyp90 latencyinput: 51 tokens, output: 1 token0.580.630.75input: 232 tokens, output: 1 token0.530.580.64input: 228 tokens, output: 26 tokens1.431.491.62It is, unclear, how much of the latency is due to model, networking (which I imagine is huge due to high variance across runs), or some just inefficient engineering overhead. It’s very possible that the latency will reduce significantly in a near future.While half a second seems high for many use cases, this number is incredibly impressive given how big the model is and the scale at which the API is being used. The number of parameters for gpt-3.5-turbo isn’t public but is guesstimated to be around 150B. As of writing, no open-source model is that big. Google’s T5 is 11B parameters and Facebook’s largest LLaMA model is 65B parameters. People discussed onthis GitHub threadwhat configuration they needed to make LLaMA models work, and it seemed like getting the 30B parameter model to work is hard enough. The most successful one seemed to berandallerwho was able to get the30B parameter model work on 128 GB of RAM, which takes a few seconds just to generate one token.The impossibility of cost + latency analysis for LLMsThe LLM application world is moving so fast that any cost + latency analysis is bound to go outdated quickly.Matt Ross, a senior manager of applied research at Scribd, told me that the estimated API cost for his use cases has gone down two orders of magnitude over the last year. Latency has significantly decreased as well. Similarly, many teams have told me they feel like they have to redo the feasibility estimation and buy (using paid APIs) vs. build (using open source models) decision every week.Prompting vs. finetuning vs. alternativesPrompting: for each sample, explicitly tell your model how it should respond.Finetuning: train a model on how to respond, so you don’t have to specify that in your prompt.There are 3 main factors when considering prompting vs. finetuning: data availability, performance, and cost.If you have only a few examples, prompting is quick and easy to get started.There’s a limit to how many examples you can include in your prompt due to the maximum input token length.The number of examples you need to finetune a model to your task, of course, depends on the task and the model. In my experience, however, you can expect a noticeable change in your model performance if you finetune on 100s examples. However, the result might not be much better than prompting.InHow Many Data Points is a Prompt Worth?(2021), \\u200b\\u200bScao and Rush found that a prompt is worth approximately 100 examples (caveat: variance across tasks and models is high – see image below). The general trend is thatas you increase the number of examples, finetuning will give better model performance than prompting. There’s no limit to how many examples you can use to finetune a model.The benefit of finetuning is two folds:You can get better model performance: can use more examples, examples becoming part of the model’s internal knowledge.You can reduce the cost of prediction. The more instruction you can bake into your model, the less instruction you have to put into your prompt. Say, if you can reduce 1k tokens in your prompt for each prediction, for 1M predictions ongpt-3.5-turbo, you’d save $2000.Prompt tuningA cool idea that is between prompting and finetuning isprompt tuning, introduced by Leister et al. in 2021. Starting with a prompt, instead of changing this prompt, you programmatically change the embedding of this prompt. For prompt tuning to work, you need to be able to input prompts’ embeddings into your LLM model and generate tokens from these embeddings, which currently, can only be done with open-source LLMs and not in OpenAI API. On T5, prompt tuning appears to perform much better than prompt engineering and can catch up with model tuning (see image below).Finetuning with distillationIn March 2023, a group of Stanford students released a promising idea: finetune a smaller open-source language model (LLaMA-7B, the 7 billion parameter version of LLaMA) on examples generated by a larger language model (text-davinci-003– 175 billion parameters). This technique of training a small model to imitate the behavior of a larger model is called distillation. The resulting finetuned model behaves similarly totext-davinci-003, while being a lot smaller and cheaper to run.For finetuning, they used 52k instructions, which they inputted intotext-davinci-003to obtain outputs, which are then used to finetune LLaMa-7B. This costs under $500 to generate. The training process for finetuning costs under $100. SeeStanford Alpaca: An Instruction-following LLaMA Model(Taori et al., 2023).The appeal of this approach is obvious. After 3 weeks, their GitHub repo got almost 20K stars!! By comparison,HuggingFace’s transformersrepo took over a year to achieve a similar number of stars, andTensorFlowrepo took 4 months.Embeddings + vector databasesOne direction that I find very promising is to use LLMs to generate embeddings and then build your ML applications on top of these embeddings, e.g. for search and recsys. As of April 2023, the cost for embeddings using the smaller modeltext-embedding-ada-002is $0.0004/1k tokens. If each item averages 250 tokens (187 words), this pricing means $1 for every 10k items or $100 for 1 million items.While this still costs more than some existing open-source models, this is still very affordable, given that:You usually only have to generate the embedding for each item once.With OpenAI API, it’s easy to generate embeddings for queries and new items in real-time.To learn more about using GPT embeddings, check outSGPT(Niklas Muennighoff, 2022) or this analysison the performance and cost GPT-3 embeddings(Nils Reimers, 2022). Some of the numbers in Nils’ post are already outdated (the field is moving so fast!!), but the method is great!The main cost of embedding models for real-time use cases is loading these embeddings into a vector database for low-latency retrieval. However, you’ll have this cost regardless of which embeddings you use. It’s exciting to see so many vector databases blossoming – the new ones such as Pinecone, Qdrant, Weaviate, Chroma as well as the incumbents Faiss, Redis, Milvus, ScaNN.If 2021 was the year of graph databases, 2023 is the year of vector databases.Backward and forward compatibilityHacker News discussion:Who is working on forward and backward compatibility for LLMs?Foundational models can work out of the box for many tasks without us having to retrain them as much. However, they do need to be retrained or finetuned from time to time as they go outdated. According to Lilian Weng’sPrompt Engineering post:One observation withSituatedQAdataset for questions grounded in different dates is that despite LM (pretraining cutoff is year 2020) has access to latest information via Google Search, its performance on post-2020 questions are still a lot worse than on pre-2020 questions. This suggests the existence of some discrepencies or conflicting parametric between contextual information and model internal knowledge.In traditional software, when software gets an update, ideally it should still work with the code written for its older version. However, with prompt engineering, if you want to use a newer model, there’s no way to guarantee that all your prompts will still work as intended with the newer model, so you’ll likely have to rewrite your prompts again.If you expect the models you use to change at all, it’s important to unit-test all your prompts using evaluation examples.One argument I often hear is that prompt rewriting shouldn’t be a problem because:Newer models shouldonlywork better than existing models. I’m not convinced about this. Newer models might, overall, be better, but there will be use cases for which newer models are worse.Experiments with prompts are fast and cheap, as we discussed in the sectionCost. While I agree with this argument, a big challenge I see in MLOps today is that there’s a lack of centralized knowledge for model logic, feature logic, prompts, etc. An application might contain multiple prompts with complex logic (discussed in Part 2. Task composability). If the person who wrote the original prompt leaves, it might be hard to understand the intention behind the original prompt to update it. This can become similar to the situation when someone leaves behind a 700-line SQL query that nobody dares to touch.Another challenge is that prompt patterns are not robust to changes. For example, many of the published prompts I’ve seen start with “I want you to act as XYZ”. If OpenAI one day decides to print something like: “I’m an AI assistant and I can’t act like XYZ”, all these prompts will need to be updated.Part 2. Task composabilityApplications that consist of multiple tasksThe example controversy scorer above consists of one single task: given an input, output a controversy score. Most applications, however, are more complex. Consider the “talk-to-your-data” use case where we want to connect to a database and query this database in natural language. Imagine a credit card transaction table. You want to ask things like:\"How many unique merchants are there in Phoenix and what are their names?\"and your database will return:\"There are 9 unique merchants in Phoenix and they are …\".One way to do this is to write a program that performs the following sequence of tasks:Task 1: convert natural language input from user to SQL query [LLM]Task 2: execute SQL query in the SQL database [SQL executor]Task 3: convert the SQL result into a natural language response to show user [LLM]Agents, tools, and control flowsI did a small survey among people in my network and there doesn’t seem to be any consensus on terminologies, yet.The word agent is being thrown around a lot to refer to an application that can execute multiple tasks according to a givencontrol flow(see Control flows section). A task can leverage one or moretools. In the example above, SQL executor is an example of a tool.Note: some people in my network resist using the term agent in this context as it is already overused in other contexts (e.g. agent to refer to a policy inreinforcement learning).Tools vs. pluginsOther than SQL executor, here are more examples of tools:search (e.g. by using Google Search API or Bing API)web browser (e.g. given a URL, fetch its content)bash executorcalculatorTools and plugins are basically the same things. You can think of plugins as tools contributed to the OpenAI plugin store. As of writing, OpenAI plugins aren’t open to the public yet, but anyone can create and use tools.Control flows: sequential, parallel, if, for loopIn the example above, sequential is an example of a control flow in which one task is executed after another. There are other types of control flows such as parallel, if statement, for loop.Sequential: executing task B after task A completes, likely because task B depends on Task A. For example, the SQL query can only be executed after it’s been translated from the user input.Parallel: executing tasks A and B at the same time.If statement: executing task A or task B depending on the input.For loop: repeat executing task A until a certain condition is met. For example, imagine you use browser action to get the content of a webpage and keep on using browser action to get the content of links found in that webpage until the agent feels like it’s got sufficient information to answer the original question.Note: while parallel can definitely be useful, I haven’t seen a lot of applications using it.Control flow with LLM agentsIn traditional software engineering, conditions for control flows are exact. With LLM applications (also known as agents), conditions might also be determined by prompting.For example, if you want your agent to choose between three actionssearch,SQL executor, andChat, you might explain how it should choose one of these actions as follows (very approximate), In other words, you can use LLMs to decide the condition of the control flow!You have access to three tools: Search, SQL executor, and Chat.\\n\\nSearch is useful when users want information about current events or products. \\n\\nSQL executor is useful when users want information that can be queried from a database.\\n\\nChat is useful when users want general information.\\n\\nProvide your response in the following format:\\n\\nInput: { input }\\nThought: { thought }\\nAction: { action }\\nAction Input: { action_input }\\nObservation: { action_output }\\nThought: { thought }Testing an agentFor agents to be reliable, we’d need to be able to build and test each task separately before combining them. There are two major types of failure modes:One or more tasks fail. Potential causes:Control flow is wrong: a non-optional action is chosenOne or more tasks produce incorrect resultsAll tasks produce correct results but the overall solution is incorrect. Press et al. (2022) call this “composability gap”: the fraction of compositional questions that the model answers incorrectly out of all the compositional questions for which the model answers the sub-questions correctly.Like with software engineering, you can and should unit test each component as well as the control flow. For each component, you can define pairs of(input, expected output)as evaluation examples, which can be used to evaluate your application every time you update your prompts or control flows. You can also do integration tests for the entire application.Part 3. Promising use casesThe Internet has been flooded with cool demos of applications built with LLMs. Here are some of the most common and promising applications that I’ve seen. I’m sure that I’m missing a ton.For more ideas, check out the projects from two hackathons I’ve seen:GPT-4 Hackathon Code Results[Mar 25, 2023]Langchain / Gen Mo Hackathon[Feb 25, 2023]AI assistantThis is hands down the most popular consumer use case. There are AI assistants built for different tasks for different groups of users – AI assistants for scheduling, making notes, pair programming, responding to emails, helping with parents, making reservations, booking flights, shopping, etc. – but, of course, the ultimate goal is an assistant that can assist you in everything.This is also the holy grail that all big companies are working towards for years: Google with Google Assistant and Bard, Facebook with M and Blender, OpenAI (and by extension, Microsoft) with ChatGPT. Quora, which has a very high risk of being replaced by AIs, released their own appPoethat lets you chat with multiple LLMs. I’m surprised Apple and Amazon haven’t joined the race yet.ChatbotChatbots are similar to AI assistants in terms of APIs. If AI assistants’ goal is to fulfill tasks given by users, whereas chatbots’ goal is to be more of a companion. For example, you can have chatbots that talk like celebrities, game/movie/book characters, businesspeople, authors, etc.Michelle Huang usedher childhood journal entries as part of the prompt to GPT-3 to talk to the inner child.The most interesting company in the consuming-chatbot space is probably Character.ai. It’s a platform for people to create and share chatbots. The most popular types of chatbots on the platform, as writing, are anime and game characters, but you can also talk to a psychologist, a pair programming partner, or a language practice partner. You can talk, act, draw pictures, play text-based games (like AI Dungeon), and even enable voices for characters. I tried a few popular chatbots – none of them seem to be able to hold a conversation yet, but we’re just at the beginning. Things can get even more interesting if there’s a revenue-sharing model so that chatbot creators can get paid.Programming and gamingThis is another popular category of LLM applications, as LLMs turn out to be incredibly good at writing and debugging code. GitHub Copilot is a pioneer (whose VSCode extension has had 5 million downloads as of writing). There have been pretty cool demos of using LLMs to write code:Create web apps from natural languagesFind security threats: Socket AI examinesnpm and PyPI packages in your codebase for security threats. When a potential issue is detected, they use ChatGPT to summarize findings.GamingCreate games: e.g. Wyatt Cheng has an awesome video showing how he usedChatGPT to clone Flappy Bird.Generate game characters.Let you have more realistic conversations with game characters:check out this awesome demo by Convai!LearningWhenever ChatGPT was down, OpenAI discord is flooded with students complaining about not being to complete their homework. Some responded by banning the use of ChatGPT in school altogether. Some have a much better idea: how to incorporate ChatGPT to help students learn even faster. All EdTech companies I know are going full-speed on ChatGPT exploration.Some use cases:Summarize booksAutomatically generate quizzes to make sure students understand a book or a lecture. Not only ChatGPT can generate questions, but it can also evaluate whether a student’s input answers are correct.I tried and ChatGPT seemed pretty good at generating quizzes for Designing Machine Learning Systems. Will publish the quizzes generated soon!Grade / give feedback on essaysWalk through math solutionsBe a debate partner: ChatGPT is really good at taking different sides of the same debate topic.Withthe rise of homeschooling, I expect to see a lot of applications of ChatGPT to help parents homeschool.Talk-to-your-dataThis is, in my observation, the most popular enterprise application (so far). Many, many startups are building tools to let enterprise users query their internal data and policies in natural languages or in the Q&A fashion. Some focus on verticals such as legal contracts, resumes, financial data, or customer support. Given a company’s all documentations, policies, and FAQs, you can build a chatbot that can respond your customer support requests.The main way to do this application usually involves these 4 steps:Organize your internal data into a database (SQL database, graph database, embedding/vector database, or just text database)Given an input in natural language, convert it into the query language of the internal database. For example, if it’s a SQL or graph database, this process can return a SQL query. If it’s embedding database, it’s might be an ANN (approximate nearest neighbor) retrieval query. If it’s just purely text, this process can extract a search query.Execute the query in the database to obtain the query result.Translate this query result into natural language.While this makes for really cool demos, I’m not sure how defensible this category is. I’ve seen startups building applications to let users query on top of databases like Google Drive or Notion, and it feels like that’s a feature Google Drive or Notion can implement in a week.OpenAI has a pretty goodtutorial on how to talk to your vector database.Can LLMs do data analysis for me?I tried inputting some data into gpt-3.5-turbo, and it seems to be able to detect some patterns. However, this only works for small data that can fit into the input prompt. Most production data is larger than that.Search and recommendationSearch and recommendation has always been the bread and butter of enterprise use cases. It’s going through a renaissance with LLMs. Search has been mostly keyword-based: you need a tent, you search for a tent. But what if you don’t know what you need yet? For example, if you’re going camping in the woods in Oregon in November, you might end up doing something like this:Search to read about other people’s experiences.Read those blog posts and manually extract a list of items you need.Search for each of these items, either on Google or other websites.If you search for “things you need for camping in oregon in november” directly on Amazon or any e-commerce website, you’ll get something like this:But what if searching for “things you need for camping in oregon in november” on Amazon actually returns you a list of things you need for your camping trip?It’s possible today with LLMs. For example, the application can be broken into the following steps:Task 1: convert the user query into a list of product names [LLM]Task 2: for each product name in the list, retrieve relevant products from your product catalog.If this works, I wonder if we’ll have LLM SEO: techniques to get your products recommended by LLMs.SalesThe most obvious way to use LLMs for sales is to write sales emails. But nobody really wants more or better sales emails. However, several companies in my network are using LLMs to synthesize information about a company to see what they need.SEOSEO is about to get very weird. Many companies today rely on creating a lot of content hoping to rank high on Google. However, given that LLMs are REALLY good at generating content, and I already know a few startups whose service is to create unlimited SEO-optimized content for any given keyword, search engines will be flooded. SEO might become even more of a cat-and-mouse game: search engines come up with new algorithms to detect AI-generated content, and companies get better at bypassing these algorithms. People might also rely less on search, and more on brands (e.g. trust only the content created by certain people or companies).And we haven’t even touched on SEO for LLMs yet: how to inject your content into LLMs’ responses!!ConclusionWe’re still in the early days of LLMs applications – everything is evolving so fast. I recently read a book proposal on LLMs, and my first thought was: most of this will be outdated in a month. APIs are changing day to day. New applications are being discovered. Infrastructure is being aggressively optimized. Cost and latency analysis needs to be done on a weekly basis. New terminologies are being introduced.Not all of these changes will matter. For example, many prompt engineering papers remind me of the early days of deep learning when there were thousands of papers describing different ways to initialize weights. I imagine that tricks to tweak your prompts like:\"Answer truthfully\",\"I want you to act like …\", writing\"question: \"instead of\"q:\"wouldn’t matter in the long run.Given that LLMs seem to be pretty good at writing prompts for themselves – seeLarge Language Models Are Human-Level Prompt Engineers(Zhou et al., 2022) – who knows that we’ll need humans to tune prompts?However, given so much happening, it’s hard to know which will matter, and which won’t.I recently asked onLinkedInhow people keep up to date with the field. The strategy ranges from ignoring the hype to trying out all the tools.Ignore (most of) the hypeVicki Boykis(Senior ML engineer @ Duo Security):I do the same thing as with any new frameworks in engineering or the data landscape: I skim the daily news, ignore most of it, and wait six months to see what sticks. Anything important will still be around, and there will be more survey papers and vetted implementations that help contextualize what’s happening.Read only the summariesShashank Chaurasia(Engineering @ Microsoft):I use the Creative mode of BingChat to give me a quick summary of new articles, blogs and research papers related to Gen AI! I often chat with the research papers and github repos to understand the details.Try to keep up to date with the latest toolsChris Alexiuk(Founding ML engineer @ Ox):I just try and build with each of the tools as they come out - that way, when the next step comes out, I’m only looking at the delta.What’s your strategy?Please enable JavaScript to view thecomments powered by Disqus.Chip Huyen[email\\xa0protected]chiphuyenchiprotherealhuyenchiphuyenchip19chiphuyenI help companies deploy machine learning into production. I write about MLOps tools and best practices.Subscribe to my nonexistent newsletter. I\\'ll start one some day, probably.Email Address'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "res = requests.get(\"https://huyenchip.com/2023/04/11/llm-engineering.html\")\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "sample_text = ''.join(soup.stripped_strings)\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 675, which is longer than the specified 500\n",
      "Created a chunk of size 547, which is longer than the specified 500\n",
      "Created a chunk of size 586, which is longer than the specified 500\n",
      "Created a chunk of size 651, which is longer than the specified 500\n",
      "Created a chunk of size 792, which is longer than the specified 500\n",
      "Created a chunk of size 544, which is longer than the specified 500\n",
      "Created a chunk of size 692, which is longer than the specified 500\n",
      "Created a chunk of size 633, which is longer than the specified 500\n",
      "Created a chunk of size 521, which is longer than the specified 500\n",
      "Created a chunk of size 518, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Building LLM applications for productionBuilding LLM applications for production | Chip HuyenChip HuyenBooksBlogList 100MLOps GuideTiếng ViệtBuilding LLM applications for productionApr 11, 2023•Chip Huyen[Hacker News discussion,LinkedIn discussion,Twitter thread]A question that I’ve been asked a lot recently is how large language models (LLMs) will change machine learning workflows.', 'After working with several companies who are working with LLM applications and personally going down a rabbit hole building my applications, I realized two things:It’s easy to make something cool with LLMs, but very hard to make something production-ready with them.LLM limitations are exacerbated by a lack of engineering rigor in prompt engineering, partially due to the ambiguous nature of natural languages, and partially due to the nascent nature of the field.This post consists of three parts.Part 1 discusses the key challenges of productionizing LLM applications and the solutions that I’ve seen.Part 2 discusses how to compose multiple tasks with control flows (e.g.', 'if statement, for loop) and incorporate tools (e.g.\\n\\nSQL executor, bash, web browsers, third-party APIs) for more complex and powerful applications.Part 3 covers some of the promising use cases that I’ve seen companies building on top of LLMs and how to construct them from smaller tasks.There has been so much written about LLMs, so feel free to skip any section you’re already familiar with.Table of contentsPart I.', 'Challenges of productionizing prompt engineering……..The ambiguity of natural languages…………Prompt evaluation…………Prompt versioning…………Prompt optimization……..Cost and latency…………Cost…………Latency…………The impossibility of cost + latency analysis for LLMs……..Prompting vs. finetuning vs. alternatives…………Prompt tuning…………Finetuning with distillation……..Embeddings + vector databases……..Backward and forward compatibilityPart 2.', 'Task composability……..Applications that consist of multiple tasks……..Agents, tools, and control flows…………Tools vs. plugins…………Control flows: sequential, parallel, if, for loop…………Control flow with LLM agents…………Testing an agentPart 3.\\n\\nPromising use cases……..AI assistant……..Chatbot……..Programming and gaming……..Learning……..Talk-to-your-data…………Can LLMs do data analysis for me?……..Search and recommendation……..Sales……..SEOConclusionPart I.', 'Challenges of productionizing prompt engineeringThe ambiguity of natural languagesFor most of the history of computers, engineers have written instructions in programming languages.\\n\\nProgramming languages are “mostly” exact.\\n\\nAmbiguity causes frustration and even passionate hatred in developers (think dynamic typing in Python or JavaScript).In prompt engineering, instructions are written in natural languages, which are a lot more flexible than programming languages.', 'This can make for a great user experience, but can lead to a pretty bad developer experience.The flexibility comes from two directions: how users define instructions, and how LLMs respond to these instructions.First, the flexibility in user-defined prompts leads to silent failures.\\n\\nIf someone accidentally makes some changes in code, like adding a random character or removing a line, it’ll likely throw an error.', 'If someone accidentally makes some changes in code, like adding a random character or removing a line, it’ll likely throw an error.\\n\\nHowever, if someone accidentally changes a prompt, it will still run but give very different outputs.While the flexibility in user-defined prompts is just an annoyance, the ambiguity in LLMs’ generated responses can be a dealbreaker.', 'It leads to two problems:Ambiguous output format: downstream applications on top of LLMs expect outputs in a certain format so that they can parse.\\n\\nWe can craft our prompts to be explicit about the output format, but there’s no guarantee that the outputs willalwaysfollow this format.Inconsistency in user experience: when using an application, users expect certain consistency.\\n\\nImagine an insurance company giving you a different quote every time you check on their website.', 'Imagine an insurance company giving you a different quote every time you check on their website.\\n\\nLLMs are stochastic – there’s no guarantee that an LLM will give you the same output for the same input every time.You can force an LLM to give the same response by settingtemperature = 0, which is, in general, a good practice.\\n\\nWhile itmostly solves the consistency problem, it doesn’t inspire trust in the system.', 'While itmostly solves the consistency problem, it doesn’t inspire trust in the system.\\n\\nImagine a teacher who gives you consistent scores only if that teacher sits in one particular room.\\n\\nIf that teacher sits in different rooms, that teacher’s scores for you will be wild.How to solve this ambiguity problem?This seems to be a problem that OpenAI is actively trying to mitigate.', 'If that teacher sits in different rooms, that teacher’s scores for you will be wild.How to solve this ambiguity problem?This seems to be a problem that OpenAI is actively trying to mitigate.\\n\\nThey have a notebook with tips on how to increase their models’ reliability.A couple of people who’ve worked with LLMs for years told me that they just accepted this ambiguity and built their workflows around that.', 'It’s a different mindset compared to developing deterministic programs, but not something impossible to get used to.This ambiguity can be mitigated by applying as much engineering rigor as possible.', 'In the rest of this post, we’ll discuss how to make prompt engineering, if not deterministic, systematic.Prompt evaluationA common technique for prompt engineering is to provide in the prompt a few examples and hope that the LLM will generalize from these examples (fewshot learners).As an example, consider trying to give a text a controversy score – it was a fun project that I did to find the correlation between a tweet’s popularity and its controversialness.', 'Here is the shortened prompt with 4 fewshot examples:Example: controversy scorerGiven a text, give it a controversy score from 0 to 10.', 'Examples:\\n\\n1 + 1 = 2\\nControversy score: 0\\n\\nStarting April 15th, only verified accounts on Twitter will be eligible to be in For You recommendations\\nControversy score: 5\\n\\nEveryone has the right to own and use guns\\nControversy score: 9\\n\\nImmigration should be completely banned to protect our country\\nControversy score: 10\\n\\nThe response should follow the format:\\n\\nControversy score: { score }\\nReason: { reason }\\n\\nHere is the text.When doing fewshot learning, two questions to keep in mind:Whether the LLM understands the examples given in the prompt.', 'One way to evaluate this is to input the same examples and see if the model outputs the expected scores.', 'If the model doesn’t perform well on the same examples given in the prompt, it is likely because the prompt isn’t clear – you might want to rewrite the prompt or break the task into smaller tasks (and combine them together, discussed in detail in Part II of this post).Whether the LLM overfits to these fewshot examples.You can evaluate your model on separate examples.One thing I’ve also found useful is to ask models to give examples for which it would give a certain label.', 'For example, I can ask the model to give me examples of texts for which it’d give a score of 4.', 'Then I’d input these examples into the LLM to see if it’ll indeed output 4.from llm import OpenAILLM\\n\\ndef eval_prompt(examples_file, eval_file):\\n    prompt = get_prompt(examples_file)\\n    model = OpenAILLM(prompt=prompt, temperature=0)\\n    compute_rmse(model, examples_file)\\n    compute_rmse(model, eval_file)\\neval_prompt(\"fewshot_examples.txt\", \"eval_examples.txt\")Prompt versioningSmall changes to a prompt can lead to very different results.', 'It’s essential to version and track the performance of each prompt.\\n\\nYou can use git to version each prompt and its performance, but I wouldn’t be surprised if there will be tools like MLflow or Weights & Biases for prompt experiments.Prompt optimizationThere have been many papers + blog posts written on how to optimize prompts.\\n\\nI agree with Lilian Weng inher helpful blog postthat most papers on prompt engineering are tricks that can be explained in a few sentences.', 'I agree with Lilian Weng inher helpful blog postthat most papers on prompt engineering are tricks that can be explained in a few sentences.\\n\\nOpenAI has a great notebook that explains manytips with examples.', 'OpenAI has a great notebook that explains manytips with examples.\\n\\nHere are some of them:Prompt the model to explain or explain step-by-step how it arrives at an answer, a technique known asChain-of-Thoughtor COT (Wei et al., 2022).Tradeoff: COT can increase both latency and cost due to the increased number of output tokens [seeCost and latencysection]Generate many outputs for the same input.', 'Pick the final output by either the majority vote  (also known asself-consistency techniqueby Wang et al., 2023) or you can ask your LLM to pick the best one.\\n\\nIn OpenAI API, you can generate multiple responses for the same input by passing in the argumentn(not an ideal API design if you ask me).Break one big prompt into smaller, simpler prompts.Many tools promise to auto-optimize your prompts – they are quite expensive and usually just apply these tricks.', 'One nice thing about these tools is that they’re no code, which makes them appealing to non-coders.Cost and latencyCostThe more explicit detail and examples you put into the prompt, the better the model performance (hopefully), and the more expensive your inference will cost.OpenAI API charges for both the input and output tokens.\\n\\nDepending on the task, a simple prompt might be anything between 300 - 1000 tokens.\\n\\nIf you want to include more context, e.g.', 'Depending on the task, a simple prompt might be anything between 300 - 1000 tokens.\\n\\nIf you want to include more context, e.g.\\n\\nadding your own documents or info retrieved from the Internet to the prompt, it can easily go up to 10k tokens for the prompt alone.The cost with long prompts isn’t in experimentation but in inference.Experimentation-wise,prompt engineering is a cheap and fast way get something up and running.', 'For example, even if you use GPT-4 with the following setting, your experimentation cost will still be just over $300.', 'The traditional ML cost of collecting data and training models is usually much higher and takes much longer.Prompt: 10k tokens ($0.06/1k tokens)Output: 200 tokens ($0.12/1k tokens)Evaluate on 20 examplesExperiment with 25 different versions of promptsThe cost of LLMOps is in inference.If you use GPT-4 with 10k tokens in input and 200 tokens in output, it’ll be $0.624 / prediction.If you use GPT-3.5-turbo with 4k tokens for both input and output, it’ll be $0.004 / prediction or $4 / 1k predictions.As a thought exercise, in 2021, DoorDash ML models made10 billion predictions a day.', 'If each prediction costs $0.004, that’d be $40 million a day!By comparison, AWS personalization costs about$0.0417 / 1k predictionsand AWS fraud detection costs about$7.5 / 1k predictions[for over 100,000 predictions a month].', 'AWS services are usually considered prohibitively expensive (and less flexible) for any company of a moderate scale.LatencyInput tokens can be processed in parallel, which means that input length shouldn’t affect the latency that much.However, output length significantly affects latency, which is likely due to output tokens being generated sequentially.Even for extremely short input (51 tokens) and output (1 token), the latency forgpt-3.5-turbois around 500ms.', 'If the output token increases to over 20 tokens, the latency is over 1 second.Here’s an experiment I ran, each setting is run 20 times.\\n\\nAll runs happen within 2 minutes.', 'If I do the experiment again, the latency will be very different, but the relationship between the 3 settings should be similar.This is another challenge of productionizing LLM applications using APIs like OpenAI:APIs are very unreliable, and no commitment yet on when SLAs will be provided.# tokensp50 latency (sec)p75 latencyp90 latencyinput: 51 tokens, output: 1 token0.580.630.75input: 232 tokens, output: 1 token0.530.580.64input: 228 tokens, output: 26 tokens1.431.491.62It is, unclear, how much of the latency is due to model, networking (which I imagine is huge due to high variance across runs), or some just inefficient engineering overhead.', 'It’s very possible that the latency will reduce significantly in a near future.While half a second seems high for many use cases, this number is incredibly impressive given how big the model is and the scale at which the API is being used.\\n\\nThe number of parameters for gpt-3.5-turbo isn’t public but is guesstimated to be around 150B.\\n\\nAs of writing, no open-source model is that big.\\n\\nGoogle’s T5 is 11B parameters and Facebook’s largest LLaMA model is 65B parameters.', 'As of writing, no open-source model is that big.\\n\\nGoogle’s T5 is 11B parameters and Facebook’s largest LLaMA model is 65B parameters.\\n\\nPeople discussed onthis GitHub threadwhat configuration they needed to make LLaMA models work, and it seemed like getting the 30B parameter model to work is hard enough.', 'The most successful one seemed to berandallerwho was able to get the30B parameter model work on 128 GB of RAM, which takes a few seconds just to generate one token.The impossibility of cost + latency analysis for LLMsThe LLM application world is moving so fast that any cost + latency analysis is bound to go outdated quickly.Matt Ross, a senior manager of applied research at Scribd, told me that the estimated API cost for his use cases has gone down two orders of magnitude over the last year.', 'Latency has significantly decreased as well.', 'Similarly, many teams have told me they feel like they have to redo the feasibility estimation and buy (using paid APIs) vs. build (using open source models) decision every week.Prompting vs. finetuning vs. alternativesPrompting: for each sample, explicitly tell your model how it should respond.Finetuning: train a model on how to respond, so you don’t have to specify that in your prompt.There are 3 main factors when considering prompting vs. finetuning: data availability, performance, and cost.If you have only a few examples, prompting is quick and easy to get started.There’s a limit to how many examples you can include in your prompt due to the maximum input token length.The number of examples you need to finetune a model to your task, of course, depends on the task and the model.', 'In my experience, however, you can expect a noticeable change in your model performance if you finetune on 100s examples.\\n\\nHowever, the result might not be much better than prompting.InHow Many Data Points is a Prompt Worth?\\n\\n(2021), \\u200b\\u200bScao and Rush found that a prompt is worth approximately 100 examples (caveat: variance across tasks and models is high – see image below).', '(2021), \\u200b\\u200bScao and Rush found that a prompt is worth approximately 100 examples (caveat: variance across tasks and models is high – see image below).\\n\\nThe general trend is thatas you increase the number of examples, finetuning will give better model performance than prompting.', 'The general trend is thatas you increase the number of examples, finetuning will give better model performance than prompting.\\n\\nThere’s no limit to how many examples you can use to finetune a model.The benefit of finetuning is two folds:You can get better model performance: can use more examples, examples becoming part of the model’s internal knowledge.You can reduce the cost of prediction.\\n\\nThe more instruction you can bake into your model, the less instruction you have to put into your prompt.', 'The more instruction you can bake into your model, the less instruction you have to put into your prompt.\\n\\nSay, if you can reduce 1k tokens in your prompt for each prediction, for 1M predictions ongpt-3.5-turbo, you’d save $2000.Prompt tuningA cool idea that is between prompting and finetuning isprompt tuning, introduced by Leister et al.\\n\\nin 2021.\\n\\nStarting with a prompt, instead of changing this prompt, you programmatically change the embedding of this prompt.', 'in 2021.\\n\\nStarting with a prompt, instead of changing this prompt, you programmatically change the embedding of this prompt.\\n\\nFor prompt tuning to work, you need to be able to input prompts’ embeddings into your LLM model and generate tokens from these embeddings, which currently, can only be done with open-source LLMs and not in OpenAI API.', 'On T5, prompt tuning appears to perform much better than prompt engineering and can catch up with model tuning (see image below).Finetuning with distillationIn March 2023, a group of Stanford students released a promising idea: finetune a smaller open-source language model (LLaMA-7B, the 7 billion parameter version of LLaMA) on examples generated by a larger language model (text-davinci-003– 175 billion parameters).', 'This technique of training a small model to imitate the behavior of a larger model is called distillation.\\n\\nThe resulting finetuned model behaves similarly totext-davinci-003, while being a lot smaller and cheaper to run.For finetuning, they used 52k instructions, which they inputted intotext-davinci-003to obtain outputs, which are then used to finetune LLaMa-7B.\\n\\nThis costs under $500 to generate.\\n\\nThe training process for finetuning costs under $100.', 'This costs under $500 to generate.\\n\\nThe training process for finetuning costs under $100.\\n\\nSeeStanford Alpaca: An Instruction-following LLaMA Model(Taori et al., 2023).The appeal of this approach is obvious.\\n\\nAfter 3 weeks, their GitHub repo got almost 20K stars!!', 'SeeStanford Alpaca: An Instruction-following LLaMA Model(Taori et al., 2023).The appeal of this approach is obvious.\\n\\nAfter 3 weeks, their GitHub repo got almost 20K stars!!\\n\\nBy comparison,HuggingFace’s transformersrepo took over a year to achieve a similar number of stars, andTensorFlowrepo took 4 months.Embeddings + vector databasesOne direction that I find very promising is to use LLMs to generate embeddings and then build your ML applications on top of these embeddings, e.g.', 'for search and recsys.\\n\\nAs of April 2023, the cost for embeddings using the smaller modeltext-embedding-ada-002is $0.0004/1k tokens.', 'If each item averages 250 tokens (187 words), this pricing means $1 for every 10k items or $100 for 1 million items.While this still costs more than some existing open-source models, this is still very affordable, given that:You usually only have to generate the embedding for each item once.With OpenAI API, it’s easy to generate embeddings for queries and new items in real-time.To learn more about using GPT embeddings, check outSGPT(Niklas Muennighoff, 2022) or this analysison the performance and cost GPT-3 embeddings(Nils Reimers, 2022).', 'Some of the numbers in Nils’ post are already outdated (the field is moving so fast!!\\n\\n), but the method is great!The main cost of embedding models for real-time use cases is loading these embeddings into a vector database for low-latency retrieval.\\n\\nHowever, you’ll have this cost regardless of which embeddings you use.', 'It’s exciting to see so many vector databases blossoming – the new ones such as Pinecone, Qdrant, Weaviate, Chroma as well as the incumbents Faiss, Redis, Milvus, ScaNN.If 2021 was the year of graph databases, 2023 is the year of vector databases.Backward and forward compatibilityHacker News discussion:Who is working on forward and backward compatibility for LLMs?Foundational models can work out of the box for many tasks without us having to retrain them as much.', 'However, they do need to be retrained or finetuned from time to time as they go outdated.\\n\\nAccording to Lilian Weng’sPrompt Engineering post:One observation withSituatedQAdataset for questions grounded in different dates is that despite LM (pretraining cutoff is year 2020) has access to latest information via Google Search, its performance on post-2020 questions are still a lot worse than on pre-2020 questions.', 'This suggests the existence of some discrepencies or conflicting parametric between contextual information and model internal knowledge.In traditional software, when software gets an update, ideally it should still work with the code written for its older version.', 'However, with prompt engineering, if you want to use a newer model, there’s no way to guarantee that all your prompts will still work as intended with the newer model, so you’ll likely have to rewrite your prompts again.If you expect the models you use to change at all, it’s important to unit-test all your prompts using evaluation examples.One argument I often hear is that prompt rewriting shouldn’t be a problem because:Newer models shouldonlywork better than existing models.', 'I’m not convinced about this.\\n\\nNewer models might, overall, be better, but there will be use cases for which newer models are worse.Experiments with prompts are fast and cheap, as we discussed in the sectionCost.\\n\\nWhile I agree with this argument, a big challenge I see in MLOps today is that there’s a lack of centralized knowledge for model logic, feature logic, prompts, etc.\\n\\nAn application might contain multiple prompts with complex logic (discussed in Part 2.\\n\\nTask composability).', 'An application might contain multiple prompts with complex logic (discussed in Part 2.\\n\\nTask composability).\\n\\nIf the person who wrote the original prompt leaves, it might be hard to understand the intention behind the original prompt to update it.\\n\\nThis can become similar to the situation when someone leaves behind a 700-line SQL query that nobody dares to touch.Another challenge is that prompt patterns are not robust to changes.', 'This can become similar to the situation when someone leaves behind a 700-line SQL query that nobody dares to touch.Another challenge is that prompt patterns are not robust to changes.\\n\\nFor example, many of the published prompts I’ve seen start with “I want you to act as XYZ”.\\n\\nIf OpenAI one day decides to print something like: “I’m an AI assistant and I can’t act like XYZ”, all these prompts will need to be updated.Part 2.', 'If OpenAI one day decides to print something like: “I’m an AI assistant and I can’t act like XYZ”, all these prompts will need to be updated.Part 2.\\n\\nTask composabilityApplications that consist of multiple tasksThe example controversy scorer above consists of one single task: given an input, output a controversy score.\\n\\nMost applications, however, are more complex.\\n\\nConsider the “talk-to-your-data” use case where we want to connect to a database and query this database in natural language.', 'Most applications, however, are more complex.\\n\\nConsider the “talk-to-your-data” use case where we want to connect to a database and query this database in natural language.\\n\\nImagine a credit card transaction table.\\n\\nYou want to ask things like:\"How many unique merchants are there in Phoenix and what are their names?', '\"and your database will return:\"There are 9 unique merchants in Phoenix and they are …\".One way to do this is to write a program that performs the following sequence of tasks:Task 1: convert natural language input from user to SQL query [LLM]Task 2: execute SQL query in the SQL database [SQL executor]Task 3: convert the SQL result into a natural language response to show user [LLM]Agents, tools, and control flowsI did a small survey among people in my network and there doesn’t seem to be any consensus on terminologies, yet.The word agent is being thrown around a lot to refer to an application that can execute multiple tasks according to a givencontrol flow(see Control flows section).', 'A task can leverage one or moretools.\\n\\nIn the example above, SQL executor is an example of a tool.Note: some people in my network resist using the term agent in this context as it is already overused in other contexts (e.g.\\n\\nagent to refer to a policy inreinforcement learning).Tools vs. pluginsOther than SQL executor, here are more examples of tools:search (e.g.\\n\\nby using Google Search API or Bing API)web browser (e.g.', 'agent to refer to a policy inreinforcement learning).Tools vs. pluginsOther than SQL executor, here are more examples of tools:search (e.g.\\n\\nby using Google Search API or Bing API)web browser (e.g.\\n\\ngiven a URL, fetch its content)bash executorcalculatorTools and plugins are basically the same things.\\n\\nYou can think of plugins as tools contributed to the OpenAI plugin store.', 'given a URL, fetch its content)bash executorcalculatorTools and plugins are basically the same things.\\n\\nYou can think of plugins as tools contributed to the OpenAI plugin store.\\n\\nAs of writing, OpenAI plugins aren’t open to the public yet, but anyone can create and use tools.Control flows: sequential, parallel, if, for loopIn the example above, sequential is an example of a control flow in which one task is executed after another.', 'There are other types of control flows such as parallel, if statement, for loop.Sequential: executing task B after task A completes, likely because task B depends on Task A.\\n\\nFor example, the SQL query can only be executed after it’s been translated from the user input.Parallel: executing tasks A and B at the same time.If statement: executing task A or task B depending on the input.For loop: repeat executing task A until a certain condition is met.', 'For example, imagine you use browser action to get the content of a webpage and keep on using browser action to get the content of links found in that webpage until the agent feels like it’s got sufficient information to answer the original question.Note: while parallel can definitely be useful, I haven’t seen a lot of applications using it.Control flow with LLM agentsIn traditional software engineering, conditions for control flows are exact.', 'With LLM applications (also known as agents), conditions might also be determined by prompting.For example, if you want your agent to choose between three actionssearch,SQL executor, andChat, you might explain how it should choose one of these actions as follows (very approximate), In other words, you can use LLMs to decide the condition of the control flow!You have access to three tools: Search, SQL executor, and Chat.', 'Search is useful when users want information about current events or products.\\n\\nSQL executor is useful when users want information that can be queried from a database.\\n\\nChat is useful when users want general information.', 'SQL executor is useful when users want information that can be queried from a database.\\n\\nChat is useful when users want general information.\\n\\nProvide your response in the following format:\\n\\nInput: { input }\\nThought: { thought }\\nAction: { action }\\nAction Input: { action_input }\\nObservation: { action_output }\\nThought: { thought }Testing an agentFor agents to be reliable, we’d need to be able to build and test each task separately before combining them.', 'There are two major types of failure modes:One or more tasks fail.\\n\\nPotential causes:Control flow is wrong: a non-optional action is chosenOne or more tasks produce incorrect resultsAll tasks produce correct results but the overall solution is incorrect.\\n\\nPress et al.', 'Press et al.\\n\\n(2022) call this “composability gap”: the fraction of compositional questions that the model answers incorrectly out of all the compositional questions for which the model answers the sub-questions correctly.Like with software engineering, you can and should unit test each component as well as the control flow.', 'For each component, you can define pairs of(input, expected output)as evaluation examples, which can be used to evaluate your application every time you update your prompts or control flows.\\n\\nYou can also do integration tests for the entire application.Part 3.\\n\\nPromising use casesThe Internet has been flooded with cool demos of applications built with LLMs.\\n\\nHere are some of the most common and promising applications that I’ve seen.', 'Promising use casesThe Internet has been flooded with cool demos of applications built with LLMs.\\n\\nHere are some of the most common and promising applications that I’ve seen.\\n\\nI’m sure that I’m missing a ton.For more ideas, check out the projects from two hackathons I’ve seen:GPT-4 Hackathon Code Results[Mar 25, 2023]Langchain / Gen Mo Hackathon[Feb 25, 2023]AI assistantThis is hands down the most popular consumer use case.', 'There are AI assistants built for different tasks for different groups of users – AI assistants for scheduling, making notes, pair programming, responding to emails, helping with parents, making reservations, booking flights, shopping, etc.', '– but, of course, the ultimate goal is an assistant that can assist you in everything.This is also the holy grail that all big companies are working towards for years: Google with Google Assistant and Bard, Facebook with M and Blender, OpenAI (and by extension, Microsoft) with ChatGPT.\\n\\nQuora, which has a very high risk of being replaced by AIs, released their own appPoethat lets you chat with multiple LLMs.', 'Quora, which has a very high risk of being replaced by AIs, released their own appPoethat lets you chat with multiple LLMs.\\n\\nI’m surprised Apple and Amazon haven’t joined the race yet.ChatbotChatbots are similar to AI assistants in terms of APIs.\\n\\nIf AI assistants’ goal is to fulfill tasks given by users, whereas chatbots’ goal is to be more of a companion.', 'If AI assistants’ goal is to fulfill tasks given by users, whereas chatbots’ goal is to be more of a companion.\\n\\nFor example, you can have chatbots that talk like celebrities, game/movie/book characters, businesspeople, authors, etc.Michelle Huang usedher childhood journal entries as part of the prompt to GPT-3 to talk to the inner child.The most interesting company in the consuming-chatbot space is probably Character.ai.\\n\\nIt’s a platform for people to create and share chatbots.', 'It’s a platform for people to create and share chatbots.\\n\\nThe most popular types of chatbots on the platform, as writing, are anime and game characters, but you can also talk to a psychologist, a pair programming partner, or a language practice partner.\\n\\nYou can talk, act, draw pictures, play text-based games (like AI Dungeon), and even enable voices for characters.\\n\\nI tried a few popular chatbots – none of them seem to be able to hold a conversation yet, but we’re just at the beginning.', 'I tried a few popular chatbots – none of them seem to be able to hold a conversation yet, but we’re just at the beginning.\\n\\nThings can get even more interesting if there’s a revenue-sharing model so that chatbot creators can get paid.Programming and gamingThis is another popular category of LLM applications, as LLMs turn out to be incredibly good at writing and debugging code.\\n\\nGitHub Copilot is a pioneer (whose VSCode extension has had 5 million downloads as of writing).', 'GitHub Copilot is a pioneer (whose VSCode extension has had 5 million downloads as of writing).\\n\\nThere have been pretty cool demos of using LLMs to write code:Create web apps from natural languagesFind security threats: Socket AI examinesnpm and PyPI packages in your codebase for security threats.\\n\\nWhen a potential issue is detected, they use ChatGPT to summarize findings.GamingCreate games: e.g.', 'When a potential issue is detected, they use ChatGPT to summarize findings.GamingCreate games: e.g.\\n\\nWyatt Cheng has an awesome video showing how he usedChatGPT to clone Flappy Bird.Generate game characters.Let you have more realistic conversations with game characters:check out this awesome demo by Convai!LearningWhenever ChatGPT was down, OpenAI discord is flooded with students complaining about not being to complete their homework.', 'Some responded by banning the use of ChatGPT in school altogether.\\n\\nSome have a much better idea: how to incorporate ChatGPT to help students learn even faster.\\n\\nAll EdTech companies I know are going full-speed on ChatGPT exploration.Some use cases:Summarize booksAutomatically generate quizzes to make sure students understand a book or a lecture.', 'All EdTech companies I know are going full-speed on ChatGPT exploration.Some use cases:Summarize booksAutomatically generate quizzes to make sure students understand a book or a lecture.\\n\\nNot only ChatGPT can generate questions, but it can also evaluate whether a student’s input answers are correct.I tried and ChatGPT seemed pretty good at generating quizzes for Designing Machine Learning Systems.', 'Will publish the quizzes generated soon!Grade / give feedback on essaysWalk through math solutionsBe a debate partner: ChatGPT is really good at taking different sides of the same debate topic.Withthe rise of homeschooling, I expect to see a lot of applications of ChatGPT to help parents homeschool.Talk-to-your-dataThis is, in my observation, the most popular enterprise application (so far).', 'Many, many startups are building tools to let enterprise users query their internal data and policies in natural languages or in the Q&A fashion.\\n\\nSome focus on verticals such as legal contracts, resumes, financial data, or customer support.', 'Given a company’s all documentations, policies, and FAQs, you can build a chatbot that can respond your customer support requests.The main way to do this application usually involves these 4 steps:Organize your internal data into a database (SQL database, graph database, embedding/vector database, or just text database)Given an input in natural language, convert it into the query language of the internal database.', 'For example, if it’s a SQL or graph database, this process can return a SQL query.\\n\\nIf it’s embedding database, it’s might be an ANN (approximate nearest neighbor) retrieval query.\\n\\nIf it’s just purely text, this process can extract a search query.Execute the query in the database to obtain the query result.Translate this query result into natural language.While this makes for really cool demos, I’m not sure how defensible this category is.', 'I’ve seen startups building applications to let users query on top of databases like Google Drive or Notion, and it feels like that’s a feature Google Drive or Notion can implement in a week.OpenAI has a pretty goodtutorial on how to talk to your vector database.Can LLMs do data analysis for me?I tried inputting some data into gpt-3.5-turbo, and it seems to be able to detect some patterns.\\n\\nHowever, this only works for small data that can fit into the input prompt.', 'However, this only works for small data that can fit into the input prompt.\\n\\nMost production data is larger than that.Search and recommendationSearch and recommendation has always been the bread and butter of enterprise use cases.\\n\\nIt’s going through a renaissance with LLMs.\\n\\nSearch has been mostly keyword-based: you need a tent, you search for a tent.\\n\\nBut what if you don’t know what you need yet?', 'For example, if you’re going camping in the woods in Oregon in November, you might end up doing something like this:Search to read about other people’s experiences.Read those blog posts and manually extract a list of items you need.Search for each of these items, either on Google or other websites.If you search for “things you need for camping in oregon in november” directly on Amazon or any e-commerce website, you’ll get something like this:But what if searching for “things you need for camping in oregon in november” on Amazon actually returns you a list of things you need for your camping trip?It’s possible today with LLMs.', 'For example, the application can be broken into the following steps:Task 1: convert the user query into a list of product names [LLM]Task 2: for each product name in the list, retrieve relevant products from your product catalog.If this works, I wonder if we’ll have LLM SEO: techniques to get your products recommended by LLMs.SalesThe most obvious way to use LLMs for sales is to write sales emails.\\n\\nBut nobody really wants more or better sales emails.', 'But nobody really wants more or better sales emails.\\n\\nHowever, several companies in my network are using LLMs to synthesize information about a company to see what they need.SEOSEO is about to get very weird.\\n\\nMany companies today rely on creating a lot of content hoping to rank high on Google.', 'Many companies today rely on creating a lot of content hoping to rank high on Google.\\n\\nHowever, given that LLMs are REALLY good at generating content, and I already know a few startups whose service is to create unlimited SEO-optimized content for any given keyword, search engines will be flooded.\\n\\nSEO might become even more of a cat-and-mouse game: search engines come up with new algorithms to detect AI-generated content, and companies get better at bypassing these algorithms.', 'SEO might become even more of a cat-and-mouse game: search engines come up with new algorithms to detect AI-generated content, and companies get better at bypassing these algorithms.\\n\\nPeople might also rely less on search, and more on brands (e.g.\\n\\ntrust only the content created by certain people or companies).And we haven’t even touched on SEO for LLMs yet: how to inject your content into LLMs’ responses!', 'trust only the content created by certain people or companies).And we haven’t even touched on SEO for LLMs yet: how to inject your content into LLMs’ responses!\\n\\n!ConclusionWe’re still in the early days of LLMs applications – everything is evolving so fast.\\n\\nI recently read a book proposal on LLMs, and my first thought was: most of this will be outdated in a month.\\n\\nAPIs are changing day to day.\\n\\nNew applications are being discovered.\\n\\nInfrastructure is being aggressively optimized.', 'APIs are changing day to day.\\n\\nNew applications are being discovered.\\n\\nInfrastructure is being aggressively optimized.\\n\\nCost and latency analysis needs to be done on a weekly basis.\\n\\nNew terminologies are being introduced.Not all of these changes will matter.\\n\\nFor example, many prompt engineering papers remind me of the early days of deep learning when there were thousands of papers describing different ways to initialize weights.', 'I imagine that tricks to tweak your prompts like:\"Answer truthfully\",\"I want you to act like …\", writing\"question: \"instead of\"q:\"wouldn’t matter in the long run.Given that LLMs seem to be pretty good at writing prompts for themselves – seeLarge Language Models Are Human-Level Prompt Engineers(Zhou et al., 2022) – who knows that we’ll need humans to tune prompts?However, given so much happening, it’s hard to know which will matter, and which won’t.I recently asked onLinkedInhow people keep up to date with the field.', 'The strategy ranges from ignoring the hype to trying out all the tools.Ignore (most of) the hypeVicki Boykis(Senior ML engineer @ Duo Security):I do the same thing as with any new frameworks in engineering or the data landscape: I skim the daily news, ignore most of it, and wait six months to see what sticks.', 'Anything important will still be around, and there will be more survey papers and vetted implementations that help contextualize what’s happening.Read only the summariesShashank Chaurasia(Engineering @ Microsoft):I use the Creative mode of BingChat to give me a quick summary of new articles, blogs and research papers related to Gen AI!', 'I often chat with the research papers and github repos to understand the details.Try to keep up to date with the latest toolsChris Alexiuk(Founding ML engineer @ Ox):I just try and build with each of the tools as they come out - that way, when the next step comes out, I’m only looking at the delta.What’s your strategy?Please enable JavaScript to view thecomments powered by Disqus.Chip Huyen[email\\xa0protected]chiphuyenchiprotherealhuyenchiphuyenchip19chiphuyenI help companies deploy machine learning into production.', \"I write about MLOps tools and best practices.Subscribe to my nonexistent newsletter.\\n\\nI'll start one some day, probably.Email Address\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load a long document\n",
    "import requests\n",
    "res = requests.get(\"https://huyenchip.com/2023/04/11/llm-engineering.html\")\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "sample_text = ''.join(soup.stripped_strings)\n",
    "\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "text_splitter = NLTKTextSplitter(chunk_size=500)\n",
    "\n",
    "\n",
    "texts = text_splitter.split_text(sample_text)\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 630, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After working with several companies who are working with LLM applications and personally going down a rabbit hole building my applications, I realized two things:It’s easy to make something cool with LLMs, but very hard to make something production-ready with them.\n",
      "\n",
      "LLM limitations are exacerbated by a lack of engineering rigor in prompt engineering, partially due to the ambiguous nature of natural languages, and partially due to the nascent nature of the field.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "\n",
    "\n",
    "# Load a long document\n",
    "soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "sample_text = ''.join(soup.stripped_strings)\n",
    "\n",
    "# Instantiate the SpacyTextSplitter with the desired chunk size\n",
    "text_splitter = SpacyTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "\n",
    "\n",
    "# Split the text using SpacyTextSplitter\n",
    "texts = text_splitter.split_text(sample_text)\n",
    "\n",
    "# Print the first readable chunk\n",
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='# \\n\\n# Welcome to My Blog!', metadata={}), Document(page_content='## Introduction', metadata={}), Document(page_content='Hello everyone! My name is **John Doe** and I am a _software developer_. I specialize in Python,', metadata={}), Document(page_content='Java, and JavaScript.', metadata={}), Document(page_content=\"Here's a list of my favorite programming languages:\\n\\n1. Python\\n2. JavaScript\\n3. Java\", metadata={}), Document(page_content='You can check out some of my projects on [GitHub](https://github.com).', metadata={}), Document(page_content='## About this Blog', metadata={}), Document(page_content=\"In this blog, I will share my journey as a software developer. I'll post tutorials, my thoughts on\", metadata={}), Document(page_content='the latest technology trends, and occasional book reviews.', metadata={}), Document(page_content=\"Here's a small piece of Python code to say hello:\", metadata={}), Document(page_content='\\\\``` python\\ndef say_hello(name):\\n    print(f\"Hello, {name}!\")\\n\\nsay_hello(\"John\")\\n\\\\', metadata={}), Document(page_content='```\\n\\nStay tuned for more updates!', metadata={}), Document(page_content='## Contact Me', metadata={}), Document(page_content='Feel free to reach out to me on [Twitter](https://twitter.com) or send me an email at', metadata={}), Document(page_content='johndoe@email.com.', metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "markdown_text = \"\"\"\n",
    "# \n",
    "\n",
    "# Welcome to My Blog!\n",
    "\n",
    "## Introduction\n",
    "Hello everyone! My name is **John Doe** and I am a _software developer_. I specialize in Python, Java, and JavaScript.\n",
    "\n",
    "Here's a list of my favorite programming languages:\n",
    "\n",
    "1. Python\n",
    "2. JavaScript\n",
    "3. Java\n",
    "\n",
    "You can check out some of my projects on [GitHub](https://github.com).\n",
    "\n",
    "## About this Blog\n",
    "In this blog, I will share my journey as a software developer. I'll post tutorials, my thoughts on the latest technology trends, and occasional book reviews.\n",
    "\n",
    "Here's a small piece of Python code to say hello:\n",
    "\n",
    "\\``` python\n",
    "def say_hello(name):\n",
    "    print(f\"Hello, {name}!\")\n",
    "\n",
    "say_hello(\"John\")\n",
    "\\```\n",
    "\n",
    "Stay tuned for more updates!\n",
    "\n",
    "## Contact Me\n",
    "Feel free to reach out to me on [Twitter](https://twitter.com) or send me an email at johndoe@email.com.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "markdown_splitter = MarkdownTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "docs = markdown_splitter.create_documents([markdown_text])\n",
    "\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chip Huyen[Hacker News discussion,LinkedIn discussion,Twitter thread]A question that I’ve been asked a lot recently is how large language models (LLMs) will change machine learning workflows. After working with several companies who are working with LLM applications and personally going down a rabbit hole building my applications, I realized two things:It’s easy to make something cool with LLMs, but very hard to make something production-ready with them.LLM limitations\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# Load a long document\n",
    "\n",
    "# Initialize the TokenTextSplitter with desired chunk size and overlap\n",
    "text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=50)\n",
    "\n",
    "# Split into smaller chunks\n",
    "texts = text_splitter.split_text(sample_text)\n",
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
