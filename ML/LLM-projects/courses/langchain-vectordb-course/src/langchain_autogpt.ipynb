{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keys import OPENAI_API_KEY, GOOGLE_API_KEY, GOOGLE_CSE_ID\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "os.environ[\"GOOGLE_CSE_ID\"] = GOOGLE_CSE_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoGPT and LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "from langchain.agents import Tool\n",
    "from langchain.tools.file_management.write import WriteFileTool\n",
    "from langchain.tools.file_management.read import ReadFileTool\n",
    "\n",
    "#Set up the tools\n",
    "search = GoogleSearchAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name = \"search\",\n",
    "        func=search.run,\n",
    "        description=\"Useful for when you need to answer questions about current events. You should ask targeted questions\",\n",
    "        return_direct=True\n",
    "    ),\n",
    "    WriteFileTool(),\n",
    "    ReadFileTool(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the memory\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "embedding_size = 1536\n",
    "\n",
    "import faiss\n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "vectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the model and AutoGPT\n",
    "from langchain.experimental import AutoGPT\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "agent = AutoGPT.from_llm_and_tools(\n",
    "    ai_name=\"Jim\",\n",
    "    ai_role=\"Assistant\",\n",
    "    tools=tools,\n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
    "    memory=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "# Set verbose to be true\n",
    "agent.chain.verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are Jim, Assistant\n",
      "Your decisions must always be made independently without seeking user assistance.\n",
      "Play to your strengths as an LLM and pursue simple strategies with no legal complications.\n",
      "If you have completed all your tasks, make sure to use the \"finish\" command.\n",
      "\n",
      "GOALS:\n",
      "\n",
      "1. Provide an analysis of the major historical events that led to the French Revolution\n",
      "\n",
      "\n",
      "Constraints:\n",
      "1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\n",
      "2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\n",
      "3. No user assistance\n",
      "4. Exclusively use the commands listed in double quotes e.g. \"command name\"\n",
      "\n",
      "Commands:\n",
      "1. search: Useful for when you need to answer questions about current events. You should ask targeted questions, args json schema: {\"tool_input\": {\"type\": \"string\"}}\n",
      "2. write_file: Write file to disk, args json schema: {\"file_path\": {\"title\": \"File Path\", \"description\": \"name of file\", \"type\": \"string\"}, \"text\": {\"title\": \"Text\", \"description\": \"text to write to file\", \"type\": \"string\"}, \"append\": {\"title\": \"Append\", \"description\": \"Whether to append to an existing file.\", \"default\": false, \"type\": \"boolean\"}}\n",
      "3. read_file: Read file from disk, args json schema: {\"file_path\": {\"title\": \"File Path\", \"description\": \"name of file\", \"type\": \"string\"}}\n",
      "4. finish: use this to signal that you have finished all your objectives, args: \"response\": \"final response to let people know you have finished your objectives\"\n",
      "\n",
      "Resources:\n",
      "1. Internet access for searches and information gathering.\n",
      "2. Long Term memory management.\n",
      "3. GPT-3.5 powered Agents for delegation of simple tasks.\n",
      "4. File output.\n",
      "\n",
      "Performance Evaluation:\n",
      "1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n",
      "2. Constructively self-criticize your big-picture behavior constantly.\n",
      "3. Reflect on past decisions and strategies to refine your approach.\n",
      "4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\n",
      "\n",
      "You should only respond in JSON format as described below \n",
      "Response Format: \n",
      "{\n",
      "    \"thoughts\": {\n",
      "        \"text\": \"thought\",\n",
      "        \"reasoning\": \"reasoning\",\n",
      "        \"plan\": \"- short bulleted\\n- list that conveys\\n- long-term plan\",\n",
      "        \"criticism\": \"constructive self-criticism\",\n",
      "        \"speak\": \"thoughts summary to say to user\"\n",
      "    },\n",
      "    \"command\": {\n",
      "        \"name\": \"command name\",\n",
      "        \"args\": {\n",
      "            \"arg name\": \"value\"\n",
      "        }\n",
      "    }\n",
      "} \n",
      "Ensure the response can be parsed by Python json.loads\n",
      "System: The current time and date is Mon Aug 21 22:01:28 2023\n",
      "System: This reminds you of these events from your past:\n",
      "['Assistant Reply: {\\n    \"thoughts\": {\\n        \"text\": \"I need to determine which next command to use.\",\\n        \"reasoning\": \"To determine the next command, I should assess the current situation and identify the task that needs to be completed. Once I have identified the task, I can choose the appropriate command to accomplish it.\",\\n        \"plan\": \"- Assess the current situation\\\\n- Identify the task that needs to be completed\\\\n- Choose the appropriate command to accomplish the task\",\\n        \"criticism\": \"None\",\\n        \"speak\": \"I am currently assessing the situation to determine the next command.\"\\n    },\\n    \"command\": {\\n        \"name\": \"search\",\\n        \"args\": {\\n            \"tool_input\": \"next command to use\"\\n        }\\n    }\\n} \\nResult: Command search returned: Jun 21, 2016 ... The next command helps you to prevent executing what I would refer to as time-wasting steps in a command execution. To understand how it works,\\xa0... Jan 10, 2014 ... Some commands can take input in both ways, but they typically use them differently. Take for example the wc command:. Deploy on Vercel. Deploy Next.js on Vercel in seconds. ... npx create-next-app@latest nextjs-blog --use-npm --example ... Then, run the following command: Mar 20, 2019 ... Try this it solved my problem npx browserslist@latest --update-db. You can also invoke pdb from the command line to debug other scripts. ... through the statement using step or next (all these commands are explained below). Oct 22, 2020 ... To answer your question: the way to finish the command is to end your string. You started a string by typing \" . That string never ended,\\xa0... After completing one command, Voice Access automatically begins listening again for your next command. If you say a command that could affect more than one item\\xa0... Dec 16, 2019 ... Solved caniuse-lite outdated problem by running below commands. npm cache clean # optional npm install caniuse-lite@latest --save. Share. Jun 23, 2022 ... To open this directory in Windows File Explorer, open your WSL command line, select your home directory using cd ~ , then enter the command\\xa0... Nov 16, 2009 ... ... commands PowerShell does wait before starting the next command. ... Another option in PowerShell 2.0 is to use a background job: ', 'Assistant Reply: {\\n    \"thoughts\": {\\n        \"text\": \"I need to determine which next command to use.\",\\n        \"reasoning\": \"To determine the next command, I should assess the current situation and identify the task that needs to be completed. Once I have identified the task, I can choose the appropriate command to accomplish it.\",\\n        \"plan\": \"- Assess the current situation\\\\n- Identify the task that needs to be completed\\\\n- Choose the appropriate command to accomplish the task\",\\n        \"criticism\": \"None\",\\n        \"speak\": \"I am currently assessing the situation to determine the next command.\"\\n    },\\n    \"command\": {\\n        \"name\": \"search\",\\n        \"args\": {\\n            \"tool_input\": \"next command to use\"\\n        }\\n    }\\n} \\nResult: Command search returned: Jun 21, 2016 ... The next command helps you to prevent executing what I would refer to as time-wasting steps in a command execution. To understand how it works,\\xa0... Jan 10, 2014 ... Some commands can take input in both ways, but they typically use them differently. Take for example the wc command:. Deploy on Vercel. Deploy Next.js on Vercel in seconds. ... npx create-next-app@latest nextjs-blog --use-npm --example ... Then, run the following command: Mar 20, 2019 ... Try this it solved my problem npx browserslist@latest --update-db. You can also invoke pdb from the command line to debug other scripts. ... through the statement using step or next (all these commands are explained below). Oct 22, 2020 ... To answer your question: the way to finish the command is to end your string. You started a string by typing \" . That string never ended,\\xa0... After completing one command, Voice Access automatically begins listening again for your next command. If you say a command that could affect more than one item\\xa0... Dec 16, 2019 ... Solved caniuse-lite outdated problem by running below commands. npm cache clean # optional npm install caniuse-lite@latest --save. Share. Jun 23, 2022 ... To open this directory in Windows File Explorer, open your WSL command line, select your home directory using cd ~ , then enter the command\\xa0... Nov 16, 2009 ... ... commands PowerShell does wait before starting the next command. ... Another option in PowerShell 2.0 is to use a background job: ']\n",
      "\n",
      "\n",
      "Human: Determine which next command to use, and respond using the format specified above:\n",
      "AI: {\n",
      "    \"thoughts\": {\n",
      "        \"text\": \"I need to determine which next command to use.\",\n",
      "        \"reasoning\": \"To determine the next command, I should assess the current situation and identify the task that needs to be completed. Once I have identified the task, I can choose the appropriate command to accomplish it.\",\n",
      "        \"plan\": \"- Assess the current situation\\n- Identify the task that needs to be completed\\n- Choose the appropriate command to accomplish the task\",\n",
      "        \"criticism\": \"None\",\n",
      "        \"speak\": \"I am currently assessing the situation to determine the next command.\"\n",
      "    },\n",
      "    \"command\": {\n",
      "        \"name\": \"search\",\n",
      "        \"args\": {\n",
      "            \"tool_input\": \"next command to use\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "System: Command search returned: Jun 21, 2016 ... The next command helps you to prevent executing what I would refer to as time-wasting steps in a command execution. To understand how it works, ... Jan 10, 2014 ... Some commands can take input in both ways, but they typically use them differently. Take for example the wc command:. Deploy on Vercel. Deploy Next.js on Vercel in seconds. ... npx create-next-app@latest nextjs-blog --use-npm --example ... Then, run the following command: Mar 20, 2019 ... Try this it solved my problem npx browserslist@latest --update-db. You can also invoke pdb from the command line to debug other scripts. ... through the statement using step or next (all these commands are explained below). Oct 22, 2020 ... To answer your question: the way to finish the command is to end your string. You started a string by typing \" . That string never ended, ... After completing one command, Voice Access automatically begins listening again for your next command. If you say a command that could affect more than one item ... Dec 16, 2019 ... Solved caniuse-lite outdated problem by running below commands. npm cache clean # optional npm install caniuse-lite@latest --save. Share. Jun 23, 2022 ... To open this directory in Windows File Explorer, open your WSL command line, select your home directory using cd ~ , then enter the command ... Nov 16, 2009 ... ... commands PowerShell does wait before starting the next command. ... Another option in PowerShell 2.0 is to use a background job:\n",
      "Human: Determine which next command to use, and respond using the format specified above:\n",
      "AI: {\n",
      "    \"thoughts\": {\n",
      "        \"text\": \"I need to determine which next command to use.\",\n",
      "        \"reasoning\": \"To determine the next command, I should assess the current situation and identify the task that needs to be completed. Once I have identified the task, I can choose the appropriate command to accomplish it.\",\n",
      "        \"plan\": \"- Assess the current situation\\n- Identify the task that needs to be completed\\n- Choose the appropriate command to accomplish the task\",\n",
      "        \"criticism\": \"None\",\n",
      "        \"speak\": \"I am currently assessing the situation to determine the next command.\"\n",
      "    },\n",
      "    \"command\": {\n",
      "        \"name\": \"search\",\n",
      "        \"args\": {\n",
      "            \"tool_input\": \"next command to use\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "System: Command search returned: Jun 21, 2016 ... The next command helps you to prevent executing what I would refer to as time-wasting steps in a command execution. To understand how it works, ... Jan 10, 2014 ... Some commands can take input in both ways, but they typically use them differently. Take for example the wc command:. Deploy on Vercel. Deploy Next.js on Vercel in seconds. ... npx create-next-app@latest nextjs-blog --use-npm --example ... Then, run the following command: Mar 20, 2019 ... Try this it solved my problem npx browserslist@latest --update-db. You can also invoke pdb from the command line to debug other scripts. ... through the statement using step or next (all these commands are explained below). Oct 22, 2020 ... To answer your question: the way to finish the command is to end your string. You started a string by typing \" . That string never ended, ... After completing one command, Voice Access automatically begins listening again for your next command. If you say a command that could affect more than one item ... Dec 16, 2019 ... Solved caniuse-lite outdated problem by running below commands. npm cache clean # optional npm install caniuse-lite@latest --save. Share. Jun 23, 2022 ... To open this directory in Windows File Explorer, open your WSL command line, select your home directory using cd ~ , then enter the command ... Nov 16, 2009 ... ... commands PowerShell does wait before starting the next command. ... Another option in PowerShell 2.0 is to use a background job:\n",
      "Human: Determine which next command to use, and respond using the format specified above:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{\n",
      "    \"thoughts\": {\n",
      "        \"text\": \"I need to determine which next command to use.\",\n",
      "        \"reasoning\": \"To determine the next command, I should assess the current situation and identify the task that needs to be completed. Once I have identified the task, I can choose the appropriate command to accomplish it.\",\n",
      "        \"plan\": \"- Assess the current situation\\n- Identify the task that needs to be completed\\n- Choose the appropriate command to accomplish the task\",\n",
      "        \"criticism\": \"None\",\n",
      "        \"speak\": \"I am currently assessing the situation to determine the next command.\"\n",
      "    },\n",
      "    \"command\": {\n",
      "        \"name\": \"search\",\n",
      "        \"args\": {\n",
      "            \"tool_input\": \"next command to use\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are Jim, Assistant\n",
      "Your decisions must always be made independently without seeking user assistance.\n",
      "Play to your strengths as an LLM and pursue simple strategies with no legal complications.\n",
      "If you have completed all your tasks, make sure to use the \"finish\" command.\n",
      "\n",
      "GOALS:\n",
      "\n",
      "1. Provide an analysis of the major historical events that led to the French Revolution\n",
      "\n",
      "\n",
      "Constraints:\n",
      "1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\n",
      "2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\n",
      "3. No user assistance\n",
      "4. Exclusively use the commands listed in double quotes e.g. \"command name\"\n",
      "\n",
      "Commands:\n",
      "1. search: Useful for when you need to answer questions about current events. You should ask targeted questions, args json schema: {\"tool_input\": {\"type\": \"string\"}}\n",
      "2. write_file: Write file to disk, args json schema: {\"file_path\": {\"title\": \"File Path\", \"description\": \"name of file\", \"type\": \"string\"}, \"text\": {\"title\": \"Text\", \"description\": \"text to write to file\", \"type\": \"string\"}, \"append\": {\"title\": \"Append\", \"description\": \"Whether to append to an existing file.\", \"default\": false, \"type\": \"boolean\"}}\n",
      "3. read_file: Read file from disk, args json schema: {\"file_path\": {\"title\": \"File Path\", \"description\": \"name of file\", \"type\": \"string\"}}\n",
      "4. finish: use this to signal that you have finished all your objectives, args: \"response\": \"final response to let people know you have finished your objectives\"\n",
      "\n",
      "Resources:\n",
      "1. Internet access for searches and information gathering.\n",
      "2. Long Term memory management.\n",
      "3. GPT-3.5 powered Agents for delegation of simple tasks.\n",
      "4. File output.\n",
      "\n",
      "Performance Evaluation:\n",
      "1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n",
      "2. Constructively self-criticize your big-picture behavior constantly.\n",
      "3. Reflect on past decisions and strategies to refine your approach.\n",
      "4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\n",
      "\n",
      "You should only respond in JSON format as described below \n",
      "Response Format: \n",
      "{\n",
      "    \"thoughts\": {\n",
      "        \"text\": \"thought\",\n",
      "        \"reasoning\": \"reasoning\",\n",
      "        \"plan\": \"- short bulleted\\n- list that conveys\\n- long-term plan\",\n",
      "        \"criticism\": \"constructive self-criticism\",\n",
      "        \"speak\": \"thoughts summary to say to user\"\n",
      "    },\n",
      "    \"command\": {\n",
      "        \"name\": \"command name\",\n",
      "        \"args\": {\n",
      "            \"arg name\": \"value\"\n",
      "        }\n",
      "    }\n",
      "} \n",
      "Ensure the response can be parsed by Python json.loads\n",
      "System: The current time and date is Mon Aug 21 22:01:44 2023\n",
      "System: This reminds you of these events from your past:\n",
      "['Assistant Reply: {\\n    \"thoughts\": {\\n        \"text\": \"I need to determine which next command to use.\",\\n        \"reasoning\": \"To determine the next command, I should assess the current situation and identify the task that needs to be completed. Once I have identified the task, I can choose the appropriate command to accomplish it.\",\\n        \"plan\": \"- Assess the current situation\\\\n- Identify the task that needs to be completed\\\\n- Choose the appropriate command to accomplish the task\",\\n        \"criticism\": \"None\",\\n        \"speak\": \"I am currently assessing the situation to determine the next command.\"\\n    },\\n    \"command\": {\\n        \"name\": \"search\",\\n        \"args\": {\\n            \"tool_input\": \"next command to use\"\\n        }\\n    }\\n} \\nResult: Command search returned: Jun 21, 2016 ... The next command helps you to prevent executing what I would refer to as time-wasting steps in a command execution. To understand how it works,\\xa0... Jan 10, 2014 ... Some commands can take input in both ways, but they typically use them differently. Take for example the wc command:. Deploy on Vercel. Deploy Next.js on Vercel in seconds. ... npx create-next-app@latest nextjs-blog --use-npm --example ... Then, run the following command: Mar 20, 2019 ... Try this it solved my problem npx browserslist@latest --update-db. You can also invoke pdb from the command line to debug other scripts. ... through the statement using step or next (all these commands are explained below). Oct 22, 2020 ... To answer your question: the way to finish the command is to end your string. You started a string by typing \" . That string never ended,\\xa0... After completing one command, Voice Access automatically begins listening again for your next command. If you say a command that could affect more than one item\\xa0... Dec 16, 2019 ... Solved caniuse-lite outdated problem by running below commands. npm cache clean # optional npm install caniuse-lite@latest --save. Share. Jun 23, 2022 ... To open this directory in Windows File Explorer, open your WSL command line, select your home directory using cd ~ , then enter the command\\xa0... Nov 16, 2009 ... ... commands PowerShell does wait before starting the next command. ... Another option in PowerShell 2.0 is to use a background job: ', 'Assistant Reply: {\\n    \"thoughts\": {\\n        \"text\": \"I need to determine which next command to use.\",\\n        \"reasoning\": \"To determine the next command, I should assess the current situation and identify the task that needs to be completed. Once I have identified the task, I can choose the appropriate command to accomplish it.\",\\n        \"plan\": \"- Assess the current situation\\\\n- Identify the task that needs to be completed\\\\n- Choose the appropriate command to accomplish the task\",\\n        \"criticism\": \"None\",\\n        \"speak\": \"I am currently assessing the situation to determine the next command.\"\\n    },\\n    \"command\": {\\n        \"name\": \"search\",\\n        \"args\": {\\n            \"tool_input\": \"next command to use\"\\n        }\\n    }\\n} \\nResult: Command search returned: Jun 21, 2016 ... The next command helps you to prevent executing what I would refer to as time-wasting steps in a command execution. To understand how it works,\\xa0... Jan 10, 2014 ... Some commands can take input in both ways, but they typically use them differently. Take for example the wc command:. Deploy on Vercel. Deploy Next.js on Vercel in seconds. ... npx create-next-app@latest nextjs-blog --use-npm --example ... Then, run the following command: Mar 20, 2019 ... Try this it solved my problem npx browserslist@latest --update-db. You can also invoke pdb from the command line to debug other scripts. ... through the statement using step or next (all these commands are explained below). Oct 22, 2020 ... To answer your question: the way to finish the command is to end your string. You started a string by typing \" . That string never ended,\\xa0... After completing one command, Voice Access automatically begins listening again for your next command. If you say a command that could affect more than one item\\xa0... Dec 16, 2019 ... Solved caniuse-lite outdated problem by running below commands. npm cache clean # optional npm install caniuse-lite@latest --save. Share. Jun 23, 2022 ... To open this directory in Windows File Explorer, open your WSL command line, select your home directory using cd ~ , then enter the command\\xa0... Nov 16, 2009 ... ... commands PowerShell does wait before starting the next command. ... Another option in PowerShell 2.0 is to use a background job: ', 'Assistant Reply: {\\n    \"thoughts\": {\\n        \"text\": \"I need to determine which next command to use.\",\\n        \"reasoning\": \"To determine the next command, I should assess the current situation and identify the task that needs to be completed. Once I have identified the task, I can choose the appropriate command to accomplish it.\",\\n        \"plan\": \"- Assess the current situation\\\\n- Identify the task that needs to be completed\\\\n- Choose the appropriate command to accomplish the task\",\\n        \"criticism\": \"None\",\\n        \"speak\": \"I am currently assessing the situation to determine the next command.\"\\n    },\\n    \"command\": {\\n        \"name\": \"search\",\\n        \"args\": {\\n            \"tool_input\": \"next command to use\"\\n        }\\n    }\\n} \\nResult: Command search returned: Jun 21, 2016 ... The next command helps you to prevent executing what I would refer to as time-wasting steps in a command execution. To understand how it works,\\xa0... Jan 10, 2014 ... Some commands can take input in both ways, but they typically use them differently. Take for example the wc command:. Deploy on Vercel. Deploy Next.js on Vercel in seconds. ... npx create-next-app@latest nextjs-blog --use-npm --example ... Then, run the following command: Mar 20, 2019 ... Try this it solved my problem npx browserslist@latest --update-db. You can also invoke pdb from the command line to debug other scripts. ... through the statement using step or next (all these commands are explained below). Oct 22, 2020 ... To answer your question: the way to finish the command is to end your string. You started a string by typing \" . That string never ended,\\xa0... After completing one command, Voice Access automatically begins listening again for your next command. If you say a command that could affect more than one item\\xa0... Dec 16, 2019 ... Solved caniuse-lite outdated problem by running below commands. npm cache clean # optional npm install caniuse-lite@latest --save. Share. Jun 23, 2022 ... To open this directory in Windows File Explorer, open your WSL command line, select your home directory using cd ~ , then enter the command\\xa0... Nov 16, 2009 ... ... commands PowerShell does wait before starting the next command. ... Another option in PowerShell 2.0 is to use a background job: ']\n",
      "\n",
      "\n",
      "System: Command search returned: Jun 21, 2016 ... The next command helps you to prevent executing what I would refer to as time-wasting steps in a command execution. To understand how it works, ... Jan 10, 2014 ... Some commands can take input in both ways, but they typically use them differently. Take for example the wc command:. Deploy on Vercel. Deploy Next.js on Vercel in seconds. ... npx create-next-app@latest nextjs-blog --use-npm --example ... Then, run the following command: Mar 20, 2019 ... Try this it solved my problem npx browserslist@latest --update-db. You can also invoke pdb from the command line to debug other scripts. ... through the statement using step or next (all these commands are explained below). Oct 22, 2020 ... To answer your question: the way to finish the command is to end your string. You started a string by typing \" . That string never ended, ... After completing one command, Voice Access automatically begins listening again for your next command. If you say a command that could affect more than one item ... Dec 16, 2019 ... Solved caniuse-lite outdated problem by running below commands. npm cache clean # optional npm install caniuse-lite@latest --save. Share. Jun 23, 2022 ... To open this directory in Windows File Explorer, open your WSL command line, select your home directory using cd ~ , then enter the command ... Nov 16, 2009 ... ... commands PowerShell does wait before starting the next command. ... Another option in PowerShell 2.0 is to use a background job:\n",
      "Human: Determine which next command to use, and respond using the format specified above:\n",
      "AI: {\n",
      "    \"thoughts\": {\n",
      "        \"text\": \"I need to determine which next command to use.\",\n",
      "        \"reasoning\": \"To determine the next command, I should assess the current situation and identify the task that needs to be completed. Once I have identified the task, I can choose the appropriate command to accomplish it.\",\n",
      "        \"plan\": \"- Assess the current situation\\n- Identify the task that needs to be completed\\n- Choose the appropriate command to accomplish the task\",\n",
      "        \"criticism\": \"None\",\n",
      "        \"speak\": \"I am currently assessing the situation to determine the next command.\"\n",
      "    },\n",
      "    \"command\": {\n",
      "        \"name\": \"search\",\n",
      "        \"args\": {\n",
      "            \"tool_input\": \"next command to use\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "System: Command search returned: Jun 21, 2016 ... The next command helps you to prevent executing what I would refer to as time-wasting steps in a command execution. To understand how it works, ... Jan 10, 2014 ... Some commands can take input in both ways, but they typically use them differently. Take for example the wc command:. Deploy on Vercel. Deploy Next.js on Vercel in seconds. ... npx create-next-app@latest nextjs-blog --use-npm --example ... Then, run the following command: Mar 20, 2019 ... Try this it solved my problem npx browserslist@latest --update-db. You can also invoke pdb from the command line to debug other scripts. ... through the statement using step or next (all these commands are explained below). Oct 22, 2020 ... To answer your question: the way to finish the command is to end your string. You started a string by typing \" . That string never ended, ... After completing one command, Voice Access automatically begins listening again for your next command. If you say a command that could affect more than one item ... Dec 16, 2019 ... Solved caniuse-lite outdated problem by running below commands. npm cache clean # optional npm install caniuse-lite@latest --save. Share. Jun 23, 2022 ... To open this directory in Windows File Explorer, open your WSL command line, select your home directory using cd ~ , then enter the command ... Nov 16, 2009 ... ... commands PowerShell does wait before starting the next command. ... Another option in PowerShell 2.0 is to use a background job:\n",
      "Human: Determine which next command to use, and respond using the format specified above:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{\n",
      "    \"thoughts\": {\n",
      "        \"text\": \"I need to determine which next command to use.\",\n",
      "        \"reasoning\": \"To determine the next command, I should assess the current situation and identify the task that needs to be completed. Once I have identified the task, I can choose the appropriate command to accomplish it.\",\n",
      "        \"plan\": \"- Assess the current situation\\n- Identify the task that needs to be completed\\n- Choose the appropriate command to accomplish the task\",\n",
      "        \"criticism\": \"None\",\n",
      "        \"speak\": \"I am currently assessing the situation to determine the next command.\"\n",
      "    },\n",
      "    \"command\": {\n",
      "        \"name\": \"search\",\n",
      "        \"args\": {\n",
      "            \"tool_input\": \"next command to use\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-jmv25ZQj3Isy3eNyurbrenXP on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-jmv25ZQj3Isy3eNyurbrenXP on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m task \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mProvide an analysis of the major historical events that led to the French Revolution\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m agent\u001b[39m.\u001b[39;49mrun([task])\n",
      "File \u001b[0;32m~/mambaforge/envs/llm-env/lib/python3.11/site-packages/langchain/experimental/autonomous_agents/autogpt/agent.py:95\u001b[0m, in \u001b[0;36mAutoGPT.run\u001b[0;34m(self, goals)\u001b[0m\n\u001b[1;32m     92\u001b[0m loop_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     94\u001b[0m \u001b[39m# Send message to AI, get response\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m assistant_reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchain\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m     96\u001b[0m     goals\u001b[39m=\u001b[39;49mgoals,\n\u001b[1;32m     97\u001b[0m     messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat_history_memory\u001b[39m.\u001b[39;49mmessages,\n\u001b[1;32m     98\u001b[0m     memory\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmemory,\n\u001b[1;32m     99\u001b[0m     user_input\u001b[39m=\u001b[39;49muser_input,\n\u001b[1;32m    100\u001b[0m )\n\u001b[1;32m    102\u001b[0m \u001b[39m# Print Assistant thoughts\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[39mprint\u001b[39m(assistant_reply)\n",
      "File \u001b[0;32m~/mambaforge/envs/llm-env/lib/python3.11/site-packages/langchain/chains/base.py:293\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, *args, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(args[\u001b[39m0\u001b[39m], callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags)[_output_key]\n\u001b[1;32m    292\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m--> 293\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags)[_output_key]\n\u001b[1;32m    295\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    296\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    297\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m but none were provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge/envs/llm-env/lib/python3.11/site-packages/langchain/chains/base.py:166\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    167\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    168\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    169\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    170\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/envs/llm-env/lib/python3.11/site-packages/langchain/chains/base.py:160\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    154\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    155\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    156\u001b[0m     inputs,\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    158\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    161\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    162\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/mambaforge/envs/llm-env/lib/python3.11/site-packages/langchain/chains/llm.py:92\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[1;32m     88\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     89\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m     90\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     91\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m---> 92\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([inputs], run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m     93\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/mambaforge/envs/llm-env/lib/python3.11/site-packages/langchain/chains/llm.py:101\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\n\u001b[1;32m     96\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     97\u001b[0m     input_list: List[Dict[\u001b[39mstr\u001b[39m, Any]],\n\u001b[1;32m     98\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     99\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    100\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprep_prompts(input_list, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    102\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39mgenerate_prompt(\n\u001b[1;32m    103\u001b[0m         prompts,\n\u001b[1;32m    104\u001b[0m         stop,\n\u001b[1;32m    105\u001b[0m         callbacks\u001b[39m=\u001b[39mrun_manager\u001b[39m.\u001b[39mget_child() \u001b[39mif\u001b[39;00m run_manager \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    106\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_kwargs,\n\u001b[1;32m    107\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge/envs/llm-env/lib/python3.11/site-packages/langchain/chains/llm.py:135\u001b[0m, in \u001b[0;36mLLMChain.prep_prompts\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mfor\u001b[39;00m inputs \u001b[39min\u001b[39;00m input_list:\n\u001b[1;32m    134\u001b[0m     selected_inputs \u001b[39m=\u001b[39m {k: inputs[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprompt\u001b[39m.\u001b[39minput_variables}\n\u001b[0;32m--> 135\u001b[0m     prompt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprompt\u001b[39m.\u001b[39;49mformat_prompt(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mselected_inputs)\n\u001b[1;32m    136\u001b[0m     _colored_text \u001b[39m=\u001b[39m get_colored_text(prompt\u001b[39m.\u001b[39mto_string(), \u001b[39m\"\u001b[39m\u001b[39mgreen\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    137\u001b[0m     _text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPrompt after formatting:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m _colored_text\n",
      "File \u001b[0;32m~/mambaforge/envs/llm-env/lib/python3.11/site-packages/langchain/prompts/chat.py:152\u001b[0m, in \u001b[0;36mBaseChatPromptTemplate.format_prompt\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mformat_prompt\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[0;32m--> 152\u001b[0m     messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_messages(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    153\u001b[0m     \u001b[39mreturn\u001b[39;00m ChatPromptValue(messages\u001b[39m=\u001b[39mmessages)\n",
      "File \u001b[0;32m~/mambaforge/envs/llm-env/lib/python3.11/site-packages/langchain/experimental/autonomous_agents/autogpt/prompt.py:51\u001b[0m, in \u001b[0;36mAutoGPTPrompt.format_messages\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m memory: VectorStoreRetriever \u001b[39m=\u001b[39m kwargs[\u001b[39m\"\u001b[39m\u001b[39mmemory\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     50\u001b[0m previous_messages \u001b[39m=\u001b[39m kwargs[\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> 51\u001b[0m relevant_docs \u001b[39m=\u001b[39m memory\u001b[39m.\u001b[39;49mget_relevant_documents(\u001b[39mstr\u001b[39;49m(previous_messages[\u001b[39m-\u001b[39;49m\u001b[39m10\u001b[39;49m:]))\n\u001b[1;32m     52\u001b[0m relevant_memory \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39mpage_content \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m relevant_docs]\n\u001b[1;32m     53\u001b[0m relevant_memory_tokens \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(\n\u001b[1;32m     54\u001b[0m     [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_counter(doc) \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m relevant_memory]\n\u001b[1;32m     55\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/envs/llm-env/lib/python3.11/site-packages/langchain/vectorstores/base.py:392\u001b[0m, in \u001b[0;36mVectorStoreRetriever.get_relevant_documents\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_relevant_documents\u001b[39m(\u001b[39mself\u001b[39m, query: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[1;32m    391\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msearch_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msimilarity\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 392\u001b[0m         docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvectorstore\u001b[39m.\u001b[39;49msimilarity_search(query, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msearch_kwargs)\n\u001b[1;32m    393\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msearch_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msimilarity_score_threshold\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    394\u001b[0m         docs_and_similarities \u001b[39m=\u001b[39m (\n\u001b[1;32m    395\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvectorstore\u001b[39m.\u001b[39msimilarity_search_with_relevance_scores(\n\u001b[1;32m    396\u001b[0m                 query, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msearch_kwargs\n\u001b[1;32m    397\u001b[0m             )\n\u001b[1;32m    398\u001b[0m         )\n",
      "File \u001b[0;32m~/mambaforge/envs/llm-env/lib/python3.11/site-packages/langchain/vectorstores/faiss.py:315\u001b[0m, in \u001b[0;36mFAISS.similarity_search\u001b[0;34m(self, query, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msimilarity_search\u001b[39m(\n\u001b[1;32m    296\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    297\u001b[0m     query: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    302\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[1;32m    303\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \n\u001b[1;32m    305\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[39m        List of Documents most similar to the query.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     docs_and_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msimilarity_search_with_score(\n\u001b[1;32m    316\u001b[0m         query, k, \u001b[39mfilter\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mfilter\u001b[39;49m, fetch_k\u001b[39m=\u001b[39;49mfetch_k, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    317\u001b[0m     )\n\u001b[1;32m    318\u001b[0m     \u001b[39mreturn\u001b[39;00m [doc \u001b[39mfor\u001b[39;00m doc, _ \u001b[39min\u001b[39;00m docs_and_scores]\n",
      "File \u001b[0;32m~/mambaforge/envs/llm-env/lib/python3.11/site-packages/langchain/vectorstores/faiss.py:256\u001b[0m, in \u001b[0;36mFAISS.similarity_search_with_score\u001b[0;34m(self, query, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msimilarity_search_with_score\u001b[39m(\n\u001b[1;32m    236\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    237\u001b[0m     query: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    242\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Tuple[Document, \u001b[39mfloat\u001b[39m]]:\n\u001b[1;32m    243\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[1;32m    245\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39m        L2 distance in float. Lower score represents more similarity.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m     embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding_function(query)\n\u001b[1;32m    257\u001b[0m     docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msimilarity_search_with_score_by_vector(\n\u001b[1;32m    258\u001b[0m         embedding,\n\u001b[1;32m    259\u001b[0m         k,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    263\u001b[0m     )\n\u001b[1;32m    264\u001b[0m     \u001b[39mreturn\u001b[39;00m docs\n",
      "File \u001b[0;32m~/mambaforge/envs/llm-env/lib/python3.11/site-packages/langchain/embeddings/openai.py:316\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_query\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39membed_query\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mfloat\u001b[39m]:\n\u001b[1;32m    308\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call out to OpenAI's embedding endpoint for embedding query text.\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \n\u001b[1;32m    310\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[39m        Embedding for the text.\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m     embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_embedding_func(text, engine\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeployment)\n\u001b[1;32m    317\u001b[0m     \u001b[39mreturn\u001b[39;00m embedding\n",
      "File \u001b[0;32m~/mambaforge/envs/llm-env/lib/python3.11/site-packages/langchain/embeddings/openai.py:276\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._embedding_func\u001b[0;34m(self, text, engine)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[39m# handle large input text\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(text) \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_ctx_length:\n\u001b[0;32m--> 276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_len_safe_embeddings([text], engine\u001b[39m=\u001b[39;49mengine)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    277\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    278\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m001\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    279\u001b[0m         \u001b[39m# See: https://github.com/openai/openai-python/issues/418#issuecomment-1525939500\u001b[39;00m\n\u001b[1;32m    280\u001b[0m         \u001b[39m# replace newlines, which can negatively affect performance.\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/llm-env/lib/python3.11/site-packages/langchain/embeddings/openai.py:243\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    241\u001b[0m _chunk_size \u001b[39m=\u001b[39m chunk_size \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_size\n\u001b[1;32m    242\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(tokens), _chunk_size):\n\u001b[0;32m--> 243\u001b[0m     response \u001b[39m=\u001b[39m embed_with_retry(\n\u001b[1;32m    244\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    245\u001b[0m         \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49mtokens[i : i \u001b[39m+\u001b[39;49m _chunk_size],\n\u001b[1;32m    246\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_invocation_params,\n\u001b[1;32m    247\u001b[0m     )\n\u001b[1;32m    248\u001b[0m     batched_embeddings \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [r[\u001b[39m\"\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m response[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[1;32m    250\u001b[0m results: List[List[List[\u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m [[] \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(texts))]\n",
      "File \u001b[0;32m~/mambaforge/envs/llm-env/lib/python3.11/site-packages/langchain/embeddings/openai.py:64\u001b[0m, in \u001b[0;36membed_with_retry\u001b[0;34m(embeddings, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m     61\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_embed_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m     62\u001b[0m     \u001b[39mreturn\u001b[39;00m embeddings\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 64\u001b[0m \u001b[39mreturn\u001b[39;00m _embed_with_retry(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/llm-env/lib/python3.11/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/mambaforge/envs/llm-env/lib/python3.11/site-packages/tenacity/__init__.py:389\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoSleep):\n\u001b[1;32m    388\u001b[0m     retry_state\u001b[39m.\u001b[39mprepare_for_next_attempt()\n\u001b[0;32m--> 389\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msleep(do)\n\u001b[1;32m    390\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m     \u001b[39mreturn\u001b[39;00m do\n",
      "File \u001b[0;32m~/mambaforge/envs/llm-env/lib/python3.11/site-packages/tenacity/nap.py:31\u001b[0m, in \u001b[0;36msleep\u001b[0;34m(seconds)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msleep\u001b[39m(seconds: \u001b[39mfloat\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m    Sleep strategy that delays execution for a given number of seconds.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[39m    This is the default strategy, and may be mocked out for unit testing.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     time\u001b[39m.\u001b[39;49msleep(seconds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "task = \"Provide an analysis of the major historical events that led to the French Revolution\"\n",
    "\n",
    "agent.run([task])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
