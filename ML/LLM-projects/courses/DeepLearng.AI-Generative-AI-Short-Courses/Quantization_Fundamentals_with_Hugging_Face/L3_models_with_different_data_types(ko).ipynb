{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3: Loading ML Models with Different Data Types\n",
    "\n",
    "이번 챕터에서는 ML 모델을 여러 자료형으로 불러오는 방법에 대해 배웁니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- helper file에서 더미 모델을 불러옵니다.\n",
    "- `helper.py` 파일은 강의 환경에서 다운로드 할 수 있는데, 다운로드 받아 함께 업로드 해두었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "더미 모델을 인스턴스를 생성하고 그 결과를 확인해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import DummyModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DummyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델에 존재하는 파라미터의 자료형을 검사하는 함수를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_param_dtype(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name} is loaded in {param.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_param_dtype(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Casting: `float16`\n",
    "\n",
    "- 허깅페이스의 모델들은 `float32`으로 불러오도록 세팅이 되어 있습니다.\n",
    "- 여기서는 `float16` 자료형으로 모델을 로드해봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "간단하게, 모델의 인스턴스에 자료형을 바로 선언해주면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# float 16\n",
    "model_fp16 = DummyModel().half()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 파라미터의 자료형을 확인해봅니다. `torch.float16`으로 변경되어야 정상입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_param_dtype(model_fp16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 간단한 추론을 해봅니다. 이때 모델은 정수를 입력으로 받기 때문에 (embedding을 위해) `LongTensor`를 생성해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.LongTensor([[1, 0], [0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`float32` 자료형으로는 오류가 발생하지 않고 잘 계산됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference using float32 model\n",
    "logits_fp32 = model(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_fp32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나 `float16` 자료형으로 변경한 경우 에러가 발생합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference using float16 model\n",
    "try:\n",
    "    logits_fp16 = model_fp16(dummy_input)\n",
    "except Exception as error:\n",
    "    print(\"\\033[91m\", type(error).__name__, \": \", error, \"\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Casting: `bfloat16`\n",
    "\n",
    "위에서 언급한 것처럼 허깅페이스에서는 모델을 `float32` 자료형으로 로드하는 것이 default입니다.\n",
    "\n",
    "이를 억지로 `float16` 자료형으로 로드한다고 하더라도 일반적으로 연산을 지원하지 않습니다.\n",
    "\n",
    "따라서 일반적으로 사용되는 메모리의 양을 줄이고 싶다면 `bfloat16` 자료형으로 모델을 로드해야 합니다.\n",
    "\n",
    "#### Note about deepcopy\n",
    "- `copy.deepcopy`는 원래 모델과 독립적인 객체로 복사해줍니다. 복사본에 대한 수정 결과는 원본에 영향을 주지 않습니다. 자세한 내용을 확인하기 원하는 경우 [링크](https://docs.python.org/3/library/copy.html)를 참고하시기 바랍니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bf16 = deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델의 파라미터를 bf16 자료형으로 변경합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bf16 = model_bf16.to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_param_dtype(model_bf16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 에러가 발생하지 않고 정상적으로 연산이 수행된다는 것이 확인됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_bf16 = model_bf16(dummy_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이제 `fp32` 자료형으로 연산한 결과와 `bf16` 자료형으로 연산한 결과를 비교해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_diff = torch.abs(logits_bf16 - logits_fp32).mean().item()\n",
    "max_diff = torch.abs(logits_bf16 - logits_fp32).max().item()\n",
    "\n",
    "print(f\"Mean diff: {mean_diff} | Max diff: {max_diff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Popular Generative Models in Different Data Types\n",
    "\n",
    "- 이미지 캡셔닝 태스크를 위해 [Salesforce/blip-image-captioning-base](https://huggingface.co/Salesforce/blip-image-captioning-base)를 불러옵니다.\n",
    "\n",
    "#### 샘플 코드를 확인하는 방법\n",
    "- \"Model Card\" 탭을 클릭합니다.\n",
    "- 우측 \"<> Use in Transformers\"를 클릭하면 모델을 불러오는 샘플 코드가 팝업됩니다.\n",
    "\n",
    "```Python\n",
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "```\n",
    "\n",
    "- 예시가 포함된 샘플 코드를 확인하고 싶은 경우, 팝업 맨 아래의 \"Read model documentation\"를 클릭하세요. 그러면 새 탭이 열립니다.\n",
    "  https://huggingface.co/docs/transformers/main/en/model_doc/blip#transformers.BlipForConditionalGeneration\n",
    "- 해당 페이지에서 스크롤을 조금 내려 \"parameters\" 섹션을 지나면 \"Examples:\"가 보일 것입니다.\n",
    "\n",
    "```Python\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "text = \"A picture of\"\n",
    "\n",
    "inputs = processor(images=image, text=text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Salesforce/blip-image-captioning-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BlipForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 주석을 해제하고 코드를 실행하면 `fp32` 자료형으로 로드된 것이 확인됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the default data types of the model\n",
    "\n",
    "# print_param_dtype(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델의 메모리 사용량을 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp32_mem_footprint = model.get_memory_footprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Footprint of the fp32 model in bytes: \",\n",
    "      fp32_mem_footprint)\n",
    "print(\"Footprint of the fp32 model in MBs: \", \n",
    "      fp32_mem_footprint/1e+6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이번에는 같은 모델을 `bfloat16` 자료형으로 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bf16 = BlipForConditionalGeneration.from_pretrained(\n",
    "                                               model_name,\n",
    "                               torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf16_mem_footprint = model_bf16.get_memory_footprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "메모리 사용량의 차이를 비교해보면 기존 대비 절반으로 줄었다는 것을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the relative difference\n",
    "relative_diff = bf16_mem_footprint / fp32_mem_footprint\n",
    "\n",
    "print(\"Footprint of the bf16 model in MBs: \", \n",
    "      bf16_mem_footprint/1e+6)\n",
    "print(f\"Relative diff: {relative_diff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance: `float32` vs `bfloat16`\n",
    "\n",
    "- 이번에는 실제로 생성 결과가 어떻게 달라지는지 확인해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BlipProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이미지를 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import load_image, get_generation\n",
    "from IPython.display import display\n",
    "\n",
    "img_url = 'https://storage.googleapis.com/\\\n",
    "sfr-vision-language-research/BLIP/demo.jpg'\n",
    "\n",
    "image = load_image(img_url)\n",
    "display(image.resize((500, 350)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_fp32 = get_generation(model, \n",
    "                              processor, \n",
    "                              image, \n",
    "                              torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"fp32 Model Results:\\n\", results_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_bf16 = get_generation(model_bf16, \n",
    "                              processor, \n",
    "                              image, \n",
    "                              torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"bf16 Model Results:\\n\", results_bf16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Data Type\n",
    "\n",
    "- 언급한 것처럼 허깅페이스의 트랜스포머 라이브러리는 기본적으로 모델을 `float32` 자료형으로 불러옵니다.\n",
    "- 모델을 불필요하게 큰 사이즈로 불러오는 것을 방지하기 위해 \"default date type\"을 변경할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_dtype = torch.bfloat16\n",
    "torch.set_default_dtype(desired_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model_bf16 = DummyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_param_dtype(dummy_model_bf16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델을 불러왔다면 default data type을 초기화 해줍니다.\n",
    "- 이 작업은 이미 불러온 모델에 대해 영향을 주지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_param_dtype(dummy_model_bf16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Note\n",
    "- quantization의 아주 간단한 형태로 모델의 파라미터를 더 낮은 자료형으로 불러오는 위 방법을 사용할 수 있습니다. 추론 동안에 모델은 해당 자료형을 기준으로 연산한 결과를 반환하게 됩니다.\n",
    "- 다음 강의에서는 다른 quantization 기법인 \"linear quantization\"에 대해 배웁니다. 이는 quantized 모델이 기존의 성능을 잘 유지할 수 있도록 압축된 데이터를 기존의 FP 32 자료형으로 변경하여 계산하는 테크닉을 추론에 적용하는 방식을 뜻합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
