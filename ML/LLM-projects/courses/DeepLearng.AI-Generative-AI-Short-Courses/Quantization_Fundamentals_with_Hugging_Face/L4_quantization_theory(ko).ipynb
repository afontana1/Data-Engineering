{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6tCv-A_bJVb"
   },
   "source": [
    "# Lesson 4: Quantization Theory\n",
    "\n",
    "이건 강의에서는 Linear Quantization에 대해 다룹니다.\n",
    "\n",
    "이론적인 내용들이 약간 포함되어 있으니 강의를 꼭 보시길 권장드려요!\n",
    "\n",
    "#### 설치할 라이브러리\n",
    "- 개인 컴퓨터에서 이 파일을 실행하는 경우 아래 라이브러리들을 설치해야 합니다.\n",
    "\n",
    "```Python\n",
    "!pip install transformers==4.35.0\n",
    "!pip install quanto==0.0.11\n",
    "!pip install torch==2.1.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5-FLAN\n",
    "- 강의에서는 ElutherAI의 Pythia 모델을 보여주었지만, 본 노트북 파일에는 T5-FLAN 모델로 실습을 진행합니다. 더 적은 자원을 필요로 하는 모델을 정해준 것 같습니다.\n",
    "\n",
    "T5-FLAN 모델을 사용하기 위해서 추가적으로 설치해야하는 라이브러리가 있습니다.\n",
    "```Python\n",
    "!pip install sentencepiece==0.2.0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력 텍스트를 토큰화하고, 이를 모델에 입력으로 제공한 뒤, 얻은 출력 값을 decoding하면 텍스트가 출력됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "input_text = \"Hello, my name is \"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델의 크기를 계산하는 함수를 이용하여 그 크기를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from helper import compute_module_sizes\n",
    "module_sizes = compute_module_sizes(model)\n",
    "print(f\"The model size is {module_sizes[''] * 1e-9} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize the model (8-bit precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본 강의에서는 quanto라는 라이브러리의 quantize, freeze 함수를 이용합니다.\n",
    "\n",
    "quantize는 말 그대로 양자화를 수행하되 중간(intermediate) 상태로 만들어 주는 것이고, freeze는 이 정보를 바탕으로 실제 양자화를 적용한 결과를 반환합니다.\n",
    "\n",
    "여기서는 activations에는 양자화를 적용하지 않는 것이 코드에서 확인됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from quanto import quantize, freeze\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "quantize(model, weights=torch.int8, activations=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze the model\n",
    "- 이 과정은 약간의 메모리를 필요로 합니다. \n",
    "- freeze를 실행하면 model 내부의 가중치 자료형이 변경되고 이에 따라 요구되는 메모리의 양도 줄어드는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "freeze(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존 대비 약 1/3 가량으로 모델의 크기가 줄어든 것이 확인됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "module_sizes = compute_module_sizes(model)\n",
    "print(f\"The model size is {module_sizes[''] * 1e-9} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try running inference on the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "input_text = \"Hello, my name is \"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이후는 강의 관련 내용들로, 로컬에서 직접 모델을 돌려보고 싶은 분들만 참고해보시면 좋겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqmyZYGbbO8A"
   },
   "source": [
    "## Note: Quantizing the model used in the lecture video will not work due to classroom hardware limitations.\n",
    "- Here is the code that Marc, the instructor is walking through.  \n",
    "- It will likely run on your local computer if you have 8GB of memory, which is usually the minimum for personal computers.\n",
    "  - To run locally, you can download the notebook and the helper.py file by clicking on the \"Jupyter icon\" at the top of the notebook and navigating the file directory of this classroom.  Also download the requirements.txt to install all the required libraries.\n",
    "\n",
    "### Without Quantization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load [EleutherAI/pythia-410m](https://huggingface.co/EleutherAI/pythia-410m) model and tokenizer.\n",
    "\n",
    "```Python\n",
    "from transformers import AutoModelForCausalLM\n",
    "model_name = \"EleutherAI/pythia-410m\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             low_cpu_mem_usage=True)\n",
    "print(model.gpt_neox)\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a start of a (`text`) sentence which you'd like the model to complete.\n",
    "```Python\n",
    "text = \"Hello my name is\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "outputs\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute the model's size using the helper function, `compute_module_sizes`.\n",
    "```Python\n",
    "from helper import compute_module_sizes\n",
    "module_sizes = compute_module_sizes(model)\n",
    "print(f\"The model size is {module_sizes[''] * 1e-9} GB\")\n",
    "print(model.gpt_neox.layers[0].attention.dense.weight)\n",
    "```\n",
    "**Note:** The weights are in `fp32`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BC809CYugOp"
   },
   "source": [
    "### 8-bit Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "from quanto import quantize, freeze\n",
    "import torch\n",
    "\n",
    "quantize(model, weights=torch.int8, activations=None)\n",
    "# after performing quantization\n",
    "print(model.gpt_neox)\n",
    "print(model.gpt_neox.layers[0].attention.dense.weight)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The \"freeze\" function requires more memory than is available in this classroom.\n",
    "- This code will run on a machine that has 8GB of memory, and so it will likely work if you run this code on your local machine.\n",
    "\n",
    "```Python\n",
    "# freeze the model\n",
    "freeze(model)\n",
    "print(model.gpt_neox.layers[0].attention.dense.weight)\n",
    "\n",
    "# get model size after quantization\n",
    "module_sizes = compute_module_sizes(model)\n",
    "print(f\"The model size is {module_sizes[''] * 1e-9} GB\")\n",
    "\n",
    "# run inference after quantizing the model\n",
    "outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing \"linear quantization\" to \"downcasting\"\n",
    "\n",
    "To recap the difference between the \"linear quantization\" method in this lesson with the \"downcasting\" method in the previous lesson:\n",
    "\n",
    "- When downcasting a model, you convert the model's parameters to a more compact data type (bfloat16).  During inference, the model performs its calculations in this data type, and its activations are in this data type.  Downcasting may work with the bfloat16 data type, but the model performance will likely degrade with any smaller data type, and won't work if you convert to an integer data type (like the int8 in this lesson).\n",
    "\n",
    "\n",
    "- In this lesson, you used another quantization method, \"linear quantization\", which enables the quantized model to maintain performance much closer to the original model by converting from the compressed data type back to the original FP32 data type during inference. So when the model makes a prediction, it is performing the matrix multiplications in FP32, and the activations are in FP32.  This enables you to quantize the model in data types smaller than bfloat16, such as int8, in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is just the beginning...\n",
    "- This course is intended to be a beginner-friendly introduction to the field of quantization. 🐣\n",
    "- If you'd like to learn more about quantization, please stay tuned for another Hugging Face short course that goes into more depth on this topic (launching in a few weeks!) 🤗"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Did you like this course?\n",
    "\n",
    "- If you liked this course, could you consider giving a rating and share what you liked? 💕\n",
    "- If you did not like this course, could you also please share what you think could have made it better? 🙏\n",
    "\n",
    "#### A note about the \"Course Review\" page.\n",
    "The rating options are from 0 to 10.\n",
    "- A score of 9 or 10 means you like the course.🤗\n",
    "- A score of 7 or 8 means you feel neutral about the course (neither like nor dislike).🙄\n",
    "- A score of 0,1,2,3,4,5 or 6 all mean that you do not like the course. 😭\n",
    "  - Whether you give a 0 or a 6, these are all defined as \"detractors\" according to the standard measurement called \"Net Promoter Score\". 🧐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOpTGuZzDRr5A8ocSoYIFkP",
   "collapsed_sections": [
    "4V_amrl-xG9D",
    "dODA6rR0z297"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
