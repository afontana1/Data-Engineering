{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d67a4729-cd2f-47e7-a4f6-f84a5677414f",
   "metadata": {},
   "source": [
    "# Basic RAG (Retrieval Augmented Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b880d1ed-3db0-45a1-807e-1b47e9ce1320",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "# ! pip install faiss-cpu \"mistralai>=0.1.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b350586c-f8ca-4013-8840-46460e4ba295",
   "metadata": {},
   "source": [
    "### API key 불러오기\n",
    "이전 파일들을 참고해 주세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4b100be-c2cf-4e07-ba17-07eae31aea35",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from helper import load_mistral_api_key\n",
    "api_key, dlai_endpoint = load_mistral_api_key(ret_key=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8609d5-9f27-4202-b0be-36db34412998",
   "metadata": {},
   "source": [
    "### 데이터 불러오기\n",
    "\n",
    "- https://www.deeplearning.ai/the-batch/ 이 사이트를 방문하셔도 좋습니다.\n",
    "- 혹은 어떤 기사의 URL을 가져오셔도 좋고요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983ce5f6-5eb1-4442-8e04-822bdbd701f4",
   "metadata": {},
   "source": [
    "### BeautifulSoup을 이용하여 기사 parsing하기\n",
    "여기는 그냥 텍스트를 가져온다 생각하고 넘어가겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c01740-72b4-482c-b61e-e272a734f01f",
   "metadata": {
    "height": 217
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "response = requests.get(\n",
    "    \"https://www.deeplearning.ai/the-batch/a-roadmap-explores-how-ai-can-detect-and-mitigate-greenhouse-gases/\"\n",
    ")\n",
    "html_doc = response.text\n",
    "soup = BeautifulSoup(html_doc, \"html.parser\")\n",
    "tag = soup.find(\"div\", re.compile(\"^prose--styled\"))\n",
    "text = tag.text\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491cf036-7bc9-4e96-94ab-737872de531a",
   "metadata": {},
   "source": [
    "### Optionally, 위에서 획득한 텍스트를 파일로 저장해도 됩니다\n",
    "- 다음 강의에서는 텍스트 파일을 챗 인터페이스에 업로드하는 방법을 다룹니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbfa8e2-08af-445b-8134-7395cc693b5b",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "file_name = \"AI_greenhouse_gas.txt\"\n",
    "with open(file_name, 'w') as file:\n",
    "    file.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad1aa61-9e1c-46c8-ae5e-61855df440f9",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "텍스트를 일정한 단위로 잘라내는 것을 뜻합니다.\n",
    "\n",
    "여기서는 512 글자 단위로 텍스트 덩어리를 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8494655e-bd87-49de-8f1d-69ffbc1c256e",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "chunk_size = 512\n",
    "chunks = [text[i : i + chunk_size] for i in range(0, len(text), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78c9936-0c1d-471c-b030-6c45639e7238",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42e3f06-09d6-4186-be0b-6034b0c8330e",
   "metadata": {},
   "source": [
    "### Get embeddings of the chunks\n",
    "각 덩어리의 임베딩 벡터를 획득합니다.\n",
    "\n",
    "이를 위해서 mistral의 embedding 모델을 사용합니다.\n",
    "\n",
    "즉, embedding 모델에 512글자 단위로 구분된 chunk를 입력으로 제공하면 일정한 길이(1024)의 벡터가 반환됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77d9805-7a53-4210-9f80-f4de52285588",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from mistralai.client import MistralClient\n",
    "\n",
    "\n",
    "def get_text_embedding(txt):\n",
    "    client = MistralClient(api_key=api_key, endpoint=dlai_endpoint)\n",
    "    embeddings_batch_response = client.embeddings(model=\"mistral-embed\", input=txt)\n",
    "    return embeddings_batch_response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46503830-6ad5-493e-a629-152721e2d88e",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55396758-c3f3-45b3-b6e7-d4912c0899f2",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca875993-fe6d-42df-811e-a43891cd0350",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "len(text_embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cba33c7-9d1d-44d8-a01e-e30f16be1aac",
   "metadata": {},
   "source": [
    "### 벡터 데이터베이스에 저장하기\n",
    "- 본 강의에서는 [Faiss](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/)라는 라이브러리를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0981a956-5f9c-4ea6-8fb3-a2cc9fe896d2",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "d = text_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(text_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee023ab-b26c-4df5-8a7b-7dd660bfad86",
   "metadata": {},
   "source": [
    "### 유저 쿼리 임베딩하기\n",
    "위에서와 동일한 embedding 모델을 사용하여 유저의 쿼리(텍스트)도 벡터로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894d9764-9da9-4629-8f2a-c9dcaf6ceb8d",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "question = \"What are the ways that AI can reduce emissions in Agriculture?\"\n",
    "question_embeddings = np.array([get_text_embedding(question)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4948cc-6d8b-449f-bc00-abb3591c7222",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "question_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15989e10-d0ec-41be-b6be-fa317565a926",
   "metadata": {},
   "source": [
    "### 쿼리와 가장 비슷한 chunk 탐색하기\n",
    "기존에 embedding한 vector를 기준으로 유저의 query를 embedding한 vector와 비교하여 가장 유사한 것을 찾아냅니다.\n",
    "\n",
    "여기에서는 최대 2개의 chunk를 반환하도록 설정되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c930b378-7aac-434c-881b-ab69d3edb93d",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "D, I = index.search(question_embeddings, k=2)\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73aab584-1dbf-4532-b41e-0403eeeeb567",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
    "print(retrieved_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b802a1e",
   "metadata": {},
   "source": [
    "검색을 통해 획득한 chunk를 `retrieved_chunk`라는 변수에 context로 넣어주게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da042a53-4564-4057-9a60-9b57dffff6a1",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunk}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {question}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d331b4dd",
   "metadata": {},
   "source": [
    "이전에 사용한 것과 동일한 mistral 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e7661e-51e2-4148-a44c-f262e7e85d56",
   "metadata": {
    "height": 268
   },
   "outputs": [],
   "source": [
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "\n",
    "def mistral(user_message, model=\"mistral-small-latest\", is_json=False):\n",
    "    client = MistralClient(api_key=api_key, endpoint=dlai_endpoint)\n",
    "    messages = [ChatMessage(role=\"user\", content=user_message)]\n",
    "\n",
    "    if is_json:\n",
    "        chat_response = client.chat(\n",
    "            model=model, messages=messages, response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "    else:\n",
    "        chat_response = client.chat(model=model, messages=messages)\n",
    "\n",
    "    return chat_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a964d3-0dea-422a-83e6-342a4ab6906b",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "response = mistral(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ce2ffa-c9bf-48a4-a6f3-6727da6a882d",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a653b9c2-d6e7-42f5-88e9-d5dcd376e61e",
   "metadata": {},
   "source": [
    "## RAG + Function calling\n",
    "이와 같은 RAG의 구조를 직전의 강의에서 배운 Function calling과 결합합니다.\n",
    "\n",
    "RAG도 특정 조건을 충족하는 경우에만 활용 가능한, 일종의 'function'처럼 인식시킬 수 있습니다.\n",
    "\n",
    "따라서 텍스트를 일정한 길이의 chunk로 쪼갠 뒤 벡터 데이터베이스를 만들고, 유저 쿼리를 기반으로 유사도가 높은 chunk를 반환한 뒤 적절한 답변을 생성하는 일련의 과정을 하나의 function으로 만들어 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41aac3a-20b4-4e33-ac58-f245577141f8",
   "metadata": {
    "height": 455
   },
   "outputs": [],
   "source": [
    "def qa_with_context(text, question, chunk_size=512):\n",
    "    # split document into chunks\n",
    "    chunks = [text[i : i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    # load into a vector database\n",
    "    text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])\n",
    "    d = text_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    index.add(text_embeddings)\n",
    "    # create embeddings for a question\n",
    "    question_embeddings = np.array([get_text_embedding(question)])\n",
    "    # retrieve similar chunks from the vector database\n",
    "    D, I = index.search(question_embeddings, k=2)\n",
    "    retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
    "    # generate response based on the retrieve relevant text chunks\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Context information is below.\n",
    "    ---------------------\n",
    "    {retrieved_chunk}\n",
    "    ---------------------\n",
    "    Given the context information and not prior knowledge, answer the query.\n",
    "    Query: {question}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    response = mistral(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb4467f-0db8-4247-8150-8746a4630432",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "I.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1bcc8d-b957-4167-b1e9-1353a6301762",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "I.tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c39775",
   "metadata": {},
   "source": [
    "본 예시에서는 하나의 function을 아래에서 쓰고 있지만, 활용하고자 하는 function의 종류가 많을수록 유용한 기능입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23d8ef9-36d4-4912-8303-d2fe3860d7c6",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "names_to_functions = {\"qa_with_context\": functools.partial(qa_with_context, text=text)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae3717b-37e6-40b3-93b1-cfd023b59079",
   "metadata": {
    "height": 336
   },
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"qa_with_context\",\n",
    "            \"description\": \"Answer user question by retrieving relevant context\", # 수정 가능\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"question\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"user question\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"question\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e442fa-5cca-4eb1-9c3f-24276fe4f75c",
   "metadata": {
    "height": 251
   },
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "What are the ways AI can mitigate climate change in transportation?\n",
    "\"\"\"\n",
    "\n",
    "client = MistralClient(api_key=api_key, endpoint=dlai_endpoint)\n",
    "\n",
    "response = client.chat(\n",
    "    model=\"mistral-large-latest\",\n",
    "    messages=[ChatMessage(role=\"user\", content=question)],\n",
    "    tools=tools,\n",
    "    tool_choice=\"any\", # auto, none\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d679bd5a",
   "metadata": {},
   "source": [
    "tools에 정의한 'description'과 response의 'tool_choice'에 따라 결과가 천차만별일 수 있습니다.\n",
    "\n",
    "우선 'tool_choice'가 any로 설정된 경우 유저의 query와 상관 없이 반드시 함수를 호출하게 됩니다.\n",
    "\n",
    "위 예시에서 주어진 question은 우리가 위에서 정의한 RAG를 활용할 필요가 전혀 없음에도 불구하고 이를 억지로 호출하여 엉뚱한 답변이 생성되는 것이죠.\n",
    "\n",
    "이를 방지하기 위해 tool_choice를 'auto'로 설정하는 것이 권장됩니다.\n",
    "\n",
    "또한 언어모델이 description을 바탕으로 보다 구체적이고 명확한 상황에서 tool 사용을 결정할 수 있도록 하는 것이 좋습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d349dd7-0138-4857-9bcb-69ed151cb1b8",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "tool_function = response.choices[0].message.tool_calls[0].function\n",
    "tool_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca751c08-e6e7-46a4-8e4c-a30407853cfc",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "tool_function.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08910b72-2aaa-4393-a35a-5ed2671b8faf",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "args = json.loads(tool_function.arguments)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409f6a67-2787-424e-8b8d-92fc9b66bdf9",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "function_result = names_to_functions[tool_function.name](**args)\n",
    "function_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2d1982-a899-4ad5-a5de-2a33d46cd311",
   "metadata": {},
   "source": [
    "## More about RAG\n",
    "chunking과 retrieval 방법에 대한 디테일한 내용을 공부해보고 싶다면 아래 강의들을 참고해 보세요:\n",
    "- [Advanced Retrieval for AI with Chroma](https://learn.deeplearning.ai/courses/advanced-retrieval-for-ai/lesson/1/introduction)\n",
    "  - Sentence window retrieval\n",
    "  - Auto-merge retrieval\n",
    "- [Building and Evaluating Advanced RAG Applications](https://learn.deeplearning.ai/courses/building-evaluating-advanced-rag)\n",
    "  - Query Expansion\n",
    "  - Cross-encoder reranking\n",
    "  - Training and utilizing Embedding Adapters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9932106-163e-45f4-85db-d6b373cf5bbd",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
