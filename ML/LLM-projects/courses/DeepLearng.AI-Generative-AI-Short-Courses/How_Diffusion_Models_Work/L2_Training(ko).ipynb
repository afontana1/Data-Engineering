{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "876e0348-c280-46bf-8802-ccabb04dcfcc",
   "metadata": {},
   "source": [
    "# Lab 2, Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4b19dd",
   "metadata": {},
   "source": [
    "이번 강의에서는 주어진 데이터를 기반으로 모델을 직접 학습하고자 합니다.  \n",
    "16x16 사이즈의 데이터에 노이즈를 더한 것이 원래 어떤 그림이었는지를 맞추는 방식으로 학습합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700e687c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "from diffusion_utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0d229a",
   "metadata": {},
   "source": [
    "# Setting Things Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340741ea",
   "metadata": {},
   "source": [
    "아래 코드는 지난 강의와 완전 동일합니다.  \n",
    "DDPM(Denoising Diffusino Probabilistic Models) 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23507e17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ContextUnet(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat=256, n_cfeat=10, height=28):  # cfeat - context features\n",
    "        super(ContextUnet, self).__init__()\n",
    "\n",
    "        # 입력 채널의 수, 중간 피쳐맵의 수, 클래스의 수\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_cfeat = n_cfeat\n",
    "        self.h = height  # h == w 라고 가정합니다. 단, 28,24,20처럼, 반드시 4로 나누어 떨어져야 합니다. \n",
    "\n",
    "        # 초기 컨볼루션 레이어를 초기화합니다.\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
    "\n",
    "        # U-Net을 두 단계에 걸쳐 다운샘플링합니다.\n",
    "        self.down1 = UnetDown(n_feat, n_feat)        # down1 #[10, 256, 8, 8]\n",
    "        self.down2 = UnetDown(n_feat, 2 * n_feat)    # down2 #[10, 256, 4,  4]\n",
    "        \n",
    "        # 기존: self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n",
    "        self.to_vec = nn.Sequential(nn.AvgPool2d((4)), nn.GELU())\n",
    "\n",
    "        # 한 층의 fully connected neural network에 타입 스텝과 context label을 삽입합니다.\n",
    "        self.timeembed1 = EmbedFC(1, 2*n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 1*n_feat)\n",
    "        self.contextembed1 = EmbedFC(n_cfeat, 2*n_feat)\n",
    "        self.contextembed2 = EmbedFC(n_cfeat, 1*n_feat)\n",
    "\n",
    "        # 세 단계에 걸쳐 U-Net을 업샘플링합니다.\n",
    "        self.up0 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, self.h//4, self.h//4), # up-sample  \n",
    "            nn.GroupNorm(8, 2 * n_feat), # normalize                       \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
    "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
    "\n",
    "        # 마지막 컨볼루션 레이어를 입력 이미지의 채널 수와 일치할 수 있도록 초기화합니다.\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1), # 피쳐맵의 수를 줄입니다.   #in_channels, out_channels, kernel_size, stride=1, padding=0\n",
    "            nn.GroupNorm(8, n_feat), # 정규화\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1), # 입력과 동일한 채널로 맞춰줍니다.\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, c=None):\n",
    "        \"\"\"\n",
    "        x : (batch, n_feat, h, w) : input image\n",
    "        t : (batch, n_cfeat)      : time step\n",
    "        c : (batch, n_classes)    : context label\n",
    "        \"\"\"\n",
    "        # x는 입력 이미지, c는 context label, t는 타임 스텝입니다. \n",
    "\n",
    "        # 초기 컨볼루션 레이어를 통해 입력 이미지를 전달합니다.\n",
    "        x = self.init_conv(x)\n",
    "        # 다운 샘플링을 통해 결과를 전달합니다.\n",
    "        down1 = self.down1(x)       #[10, 256, 8, 8]\n",
    "        down2 = self.down2(down1)   #[10, 256, 4, 4]\n",
    "        \n",
    "        # 피쳐맵을 벡터로 전환하고 활성화함수를 적용합니다.\n",
    "        hiddenvec = self.to_vec(down2)\n",
    "        \n",
    "        # 만약 context_mask가 1이라면 마스크를 지웁니다.\n",
    "        if c is None:\n",
    "            c = torch.zeros(x.shape[0], self.n_cfeat).to(x)\n",
    "            \n",
    "        # context와 타임 스텝을 삽입합니다.\n",
    "        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)     # (batch, 2*n_feat, 1,1)\n",
    "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
    "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
    "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
    "        #print(f\"uunet forward: cemb1 {cemb1.shape}. temb1 {temb1.shape}, cemb2 {cemb2.shape}. temb2 {temb2.shape}\")\n",
    "\n",
    "\n",
    "        up1 = self.up0(hiddenvec)\n",
    "        up2 = self.up1(cemb1*up1 + temb1, down2)  # 임베딩을 더하고 곱해줍니다.\n",
    "        up3 = self.up2(cemb2*up2 + temb2, down1)\n",
    "        out = self.out(torch.cat((up3, x), 1))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb43af7f",
   "metadata": {},
   "source": [
    "모델에 필요한 하이퍼 파라미터를 세팅합니다.  \n",
    "이번에는 학습용 하이퍼 파라미터가 추가되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c3a942",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "# diffusion hyperparameters\n",
    "timesteps = 500\n",
    "beta1 = 1e-4\n",
    "beta2 = 0.02\n",
    "\n",
    "# network hyperparameters\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else torch.device('cpu'))\n",
    "n_feat = 64 # 64 hidden dimension feature\n",
    "n_cfeat = 5 # context vector is of size 5\n",
    "height = 16 # 16x16 image\n",
    "save_dir = './weights/'\n",
    "\n",
    "# training hyperparameters\n",
    "batch_size = 100\n",
    "n_epoch = 32\n",
    "lrate=1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a705d0a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# construct DDPM noise schedule\n",
    "b_t = (beta2 - beta1) * torch.linspace(0, 1, timesteps + 1, device=device) + beta1\n",
    "a_t = 1 - b_t\n",
    "ab_t = torch.cumsum(a_t.log(), dim=0).exp()    \n",
    "ab_t[0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc9001e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# construct model\n",
    "nn_model = ContextUnet(in_channels=3, n_feat=n_feat, n_cfeat=n_cfeat, height=height).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e34ea64",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25073394",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 데이터셋을 불러오고 optimizer를 구성합니다.\n",
    "dataset = CustomDataset(\"./sprites_1788_16x16.npy\", \"./sprite_labels_nc_1788_16x16.npy\", transform, null_context=False)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "optim = torch.optim.Adam(nn_model.parameters(), lr=lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac0a655",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helper function: perturbs an image to a specified noise level\n",
    "def perturb_input(x, t, noise):\n",
    "    return ab_t.sqrt()[t, None, None, None] * x + (1 - ab_t[t, None, None, None]) * noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2004c5d7-4c6a-456d-844f-af8e98cd6b8e",
   "metadata": {},
   "source": [
    "#### CPU에서 이 코드를 실행하면 몇 시간이 걸릴 수 있습니다. 따라서 이 부분을 스킵하고(직접 학습하지 말고) 아래에 나타난 중간 결과를 확인하는 것이 권장됩니다.\n",
    "그렇더라도 직접 코드를 실행시켜보고 싶다면 셀 타입을 파이썬으로 바꿔서 실행해보세요.\n",
    "본 강의에서 CPU 런타임은 제한되어 있으므로 강의 플랫폼에서 완전히 학습하는 것은 어렵습니다.  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5cee383-a28e-406b-8ab8-1a3e77e8a3a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# training without context code\n",
    "\n",
    "# set into train mode\n",
    "nn_model.train()\n",
    "\n",
    "for ep in range(n_epoch):\n",
    "    print(f'epoch {ep}')\n",
    "    \n",
    "    # linearly decay learning rate\n",
    "    optim.param_groups[0]['lr'] = lrate*(1-ep/n_epoch)\n",
    "    \n",
    "    pbar = tqdm(dataloader, mininterval=2 )\n",
    "    for x, _ in pbar:   # x: images\n",
    "        optim.zero_grad()\n",
    "        x = x.to(device)\n",
    "        \n",
    "        # perturb data\n",
    "        noise = torch.randn_like(x)\n",
    "        t = torch.randint(1, timesteps + 1, (x.shape[0],)).to(device) \n",
    "        x_pert = perturb_input(x, t, noise)\n",
    "        \n",
    "        # use network to recover noise\n",
    "        pred_noise = nn_model(x_pert, t / timesteps)\n",
    "        \n",
    "        # loss is mean squared error between the predicted and true noise\n",
    "        loss = F.mse_loss(pred_noise, noise)\n",
    "        loss.backward()\n",
    "        \n",
    "        optim.step()\n",
    "\n",
    "    # save model periodically\n",
    "    if ep%4==0 or ep == int(n_epoch-1):\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.mkdir(save_dir)\n",
    "        torch.save(nn_model.state_dict(), save_dir + f\"model_{ep}.pth\")\n",
    "        print('saved model at ' + save_dir + f\"model_{ep}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f265f9c6",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa7aa8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helper function; 예측되는 노이즈를 제거하는 함수입니다.\n",
    "# (하지만 이미지가 붕괴하는 것을 방지하기 위해 일부 노이즈는 다시 더합니다)\n",
    "def denoise_add_noise(x, t, pred_noise, z=None):\n",
    "    if z is None:\n",
    "        z = torch.randn_like(x)\n",
    "    noise = b_t.sqrt()[t] * z\n",
    "    mean = (x - pred_noise * ((1 - a_t[t]) / (1 - ab_t[t]).sqrt())) / a_t[t].sqrt()\n",
    "    return mean + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aadaf15-ca49-484a-b00b-d24f072d3d00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 표준 알고리즘을 사용하여 샘플링합니다.\n",
    "@torch.no_grad()\n",
    "def sample_ddpm(n_sample, save_rate=20):\n",
    "    # x_T ~ N(0, 1), sample initial noise\n",
    "    samples = torch.randn(n_sample, 3, height, height).to(device)  \n",
    "\n",
    "    # 그래프를 기리기 위해서 각 스텝을 트래킹합니다.\n",
    "    intermediate = [] \n",
    "    for i in range(timesteps, 0, -1):\n",
    "        print(f'sampling timestep {i:3d}', end='\\r')\n",
    "\n",
    "        # 시간 텐서의 모양을 조정합니다.\n",
    "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
    "\n",
    "        # 일부 노이즈를 골라 더합니다. 만약 i가 1이라면 더하지 않습니다.\n",
    "        z = torch.randn_like(samples) if i > 1 else 0\n",
    "\n",
    "        eps = nn_model(samples, t)    # predict noise e_(x_t,t)\n",
    "        samples = denoise_add_noise(samples, i, eps, z)\n",
    "        if i % save_rate ==0 or i==timesteps or i<8:\n",
    "            intermediate.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    intermediate = np.stack(intermediate)\n",
    "    return samples, intermediate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1445b3bf",
   "metadata": {},
   "source": [
    "이제 epoch별로 결과를 확인해보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa42062-d7f4-49d9-954b-713165f81d19",
   "metadata": {},
   "source": [
    "#### View Epoch 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c30c67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load in model weights and set to eval mode\n",
    "nn_model.load_state_dict(torch.load(f\"{save_dir}/model_0.pth\", map_location=device))\n",
    "nn_model.eval()\n",
    "print(\"Loaded in Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8467c555",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize samples\n",
    "plt.clf()\n",
    "samples, intermediate_ddpm = sample_ddpm(32)\n",
    "animation_ddpm = plot_sample(intermediate_ddpm,32,4,save_dir, \"ani_run\", None, save=False)\n",
    "HTML(animation_ddpm.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9ff242-8488-4cac-8fb0-68cbe3d4197a",
   "metadata": {},
   "source": [
    "#### View Epoch 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91dab98-3434-4e87-9b22-c062f11a724a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load in model weights and set to eval mode\n",
    "nn_model.load_state_dict(torch.load(f\"{save_dir}/model_4.pth\", map_location=device))\n",
    "nn_model.eval()\n",
    "print(\"Loaded in Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb4a136",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize samples\n",
    "plt.clf()\n",
    "samples, intermediate_ddpm = sample_ddpm(32)\n",
    "animation_ddpm = plot_sample(intermediate_ddpm,32,4,save_dir, \"ani_run\", None, save=False)\n",
    "HTML(animation_ddpm.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59d861e-6a09-4c93-92d2-07b2f66b6cbc",
   "metadata": {},
   "source": [
    "#### View Epoch 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a88f6d-f2c8-435a-ab86-349a60bc60e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load in model weights and set to eval mode\n",
    "nn_model.load_state_dict(torch.load(f\"{save_dir}/model_8.pth\", map_location=device))\n",
    "nn_model.eval()\n",
    "print(\"Loaded in Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950d9172-ba9f-450f-be29-4751e4b5030e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize samples\n",
    "plt.clf()\n",
    "samples, intermediate_ddpm = sample_ddpm(32)\n",
    "animation_ddpm = plot_sample(intermediate_ddpm,32,4,save_dir, \"ani_run\", None, save=False)\n",
    "HTML(animation_ddpm.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ae670d-2fe8-4dd5-80b9-648ebde01ac3",
   "metadata": {},
   "source": [
    "#### View Epoch 31 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1c223b-d830-4592-95fa-dea47d48685f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load in model weights and set to eval mode\n",
    "nn_model.load_state_dict(torch.load(f\"{save_dir}/model_31.pth\", map_location=device))\n",
    "nn_model.eval()\n",
    "print(\"Loaded in Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a1a1c4-3fae-4243-8682-80123773681b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize samples\n",
    "plt.clf()\n",
    "samples, intermediate_ddpm = sample_ddpm(32)\n",
    "animation_ddpm = plot_sample(intermediate_ddpm,32,4,save_dir, \"ani_run\", None, save=False)\n",
    "HTML(animation_ddpm.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32cc4f4-d4e7-43d4-90d0-a89e4fbcc28c",
   "metadata": {},
   "source": [
    "# Acknowledgments\n",
    "Sprites by ElvGames, [FrootsnVeggies](https://zrghr.itch.io/froots-and-veggies-culinary-pixels) and  [kyrise](https://kyrise.itch.io/)   \n",
    "This code is modified from, https://github.com/cloneofsimo/minDiffusion   \n",
    "Diffusion model is based on [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) and [Denoising Diffusion Implicit Models](https://arxiv.org/abs/2010.02502)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6b7532-3a24-4d8f-b175-284fd59dc037",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "7fecef7fb14ad1d71e869da5296badaba2a50d2b864b0443ebcc6afac654c29b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
