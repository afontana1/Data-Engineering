{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "958524a2-cb56-439e-850e-032dd10478f2",
   "metadata": {},
   "source": [
    "# Lab 3, Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67bfeaf",
   "metadata": {},
   "source": [
    "컨텍스트라는 표현이 낯설 수는 있지만 여러 정보를 포함하는 개념으로 볼 수 있습니다.  \n",
    "물론 방식은 여러 임베딩 벡터를 단순 concat, 더하기, 곱하기 하는 등 여러 가지가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700e687c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "from diffusion_utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0d229a",
   "metadata": {},
   "source": [
    "# Setting Things Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340741ea",
   "metadata": {},
   "source": [
    "아래 코드는 지난 강의와 완전 동일합니다.  \n",
    "DDPM(Denoising Diffusino Probabilistic Models) 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23507e17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ContextUnet(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat=256, n_cfeat=10, height=28):  # cfeat - context features\n",
    "        super(ContextUnet, self).__init__()\n",
    "\n",
    "        # 입력 채널의 수, 중간 피쳐맵의 수, 클래스의 수\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_cfeat = n_cfeat\n",
    "        self.h = height  # h == w 라고 가정합니다. 단, 28,24,20처럼, 반드시 4로 나누어 떨어져야 합니다. \n",
    "\n",
    "        # 초기 컨볼루션 레이어를 초기화합니다.\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
    "\n",
    "        # U-Net을 두 단계에 걸쳐 다운샘플링합니다.\n",
    "        self.down1 = UnetDown(n_feat, n_feat)        # down1 #[10, 256, 8, 8]\n",
    "        self.down2 = UnetDown(n_feat, 2 * n_feat)    # down2 #[10, 256, 4,  4]\n",
    "        \n",
    "        # 기존: self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n",
    "        self.to_vec = nn.Sequential(nn.AvgPool2d((4)), nn.GELU())\n",
    "\n",
    "        # 한 층의 fully connected neural network에 타입 스텝과 context label을 삽입합니다.\n",
    "        self.timeembed1 = EmbedFC(1, 2*n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 1*n_feat)\n",
    "        self.contextembed1 = EmbedFC(n_cfeat, 2*n_feat)\n",
    "        self.contextembed2 = EmbedFC(n_cfeat, 1*n_feat)\n",
    "\n",
    "        # 세 단계에 걸쳐 U-Net을 업샘플링합니다.\n",
    "        self.up0 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, self.h//4, self.h//4), # up-sample  \n",
    "            nn.GroupNorm(8, 2 * n_feat), # normalize                       \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
    "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
    "\n",
    "        # 마지막 컨볼루션 레이어를 입력 이미지의 채널 수와 일치할 수 있도록 초기화합니다.\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1), # 피쳐맵의 수를 줄입니다.   #in_channels, out_channels, kernel_size, stride=1, padding=0\n",
    "            nn.GroupNorm(8, n_feat), # 정규화\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1), # 입력과 동일한 채널로 맞춰줍니다.\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, c=None):\n",
    "        \"\"\"\n",
    "        x : (batch, n_feat, h, w) : input image\n",
    "        t : (batch, n_cfeat)      : time step\n",
    "        c : (batch, n_classes)    : context label\n",
    "        \"\"\"\n",
    "        # x는 입력 이미지, c는 context label, t는 타임 스텝입니다. \n",
    "\n",
    "        # 초기 컨볼루션 레이어를 통해 입력 이미지를 전달합니다.\n",
    "        x = self.init_conv(x)\n",
    "        # 다운 샘플링을 통해 결과를 전달합니다.\n",
    "        down1 = self.down1(x)       #[10, 256, 8, 8]\n",
    "        down2 = self.down2(down1)   #[10, 256, 4, 4]\n",
    "        \n",
    "        # 피쳐맵을 벡터로 전환하고 활성화함수를 적용합니다.\n",
    "        hiddenvec = self.to_vec(down2)\n",
    "        \n",
    "        # 만약 context_mask가 1이라면 마스크를 지웁니다.\n",
    "        if c is None:\n",
    "            c = torch.zeros(x.shape[0], self.n_cfeat).to(x)\n",
    "            \n",
    "        # context와 타임 스텝을 삽입합니다.\n",
    "        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)     # (batch, 2*n_feat, 1,1)\n",
    "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
    "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
    "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
    "        #print(f\"uunet forward: cemb1 {cemb1.shape}. temb1 {temb1.shape}, cemb2 {cemb2.shape}. temb2 {temb2.shape}\")\n",
    "\n",
    "\n",
    "        up1 = self.up0(hiddenvec)\n",
    "        up2 = self.up1(cemb1*up1 + temb1, down2)  # 임베딩을 더하고 곱해줍니다.\n",
    "        up3 = self.up2(cemb2*up2 + temb2, down1)\n",
    "        out = self.out(torch.cat((up3, x), 1))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb43af7f",
   "metadata": {},
   "source": [
    "모델에 필요한 하이퍼 파라미터를 세팅합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c3a942",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "# diffusion hyperparameters\n",
    "timesteps = 500\n",
    "beta1 = 1e-4\n",
    "beta2 = 0.02\n",
    "\n",
    "# network hyperparameters\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else torch.device('cpu'))\n",
    "n_feat = 64 # 64 hidden dimension feature\n",
    "n_cfeat = 5 # context vector is of size 5\n",
    "height = 16 # 16x16 image\n",
    "save_dir = './weights/'\n",
    "\n",
    "# training hyperparameters\n",
    "batch_size = 100\n",
    "n_epoch = 32\n",
    "lrate=1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a705d0a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# construct DDPM noise schedule\n",
    "b_t = (beta2 - beta1) * torch.linspace(0, 1, timesteps + 1, device=device) + beta1\n",
    "a_t = 1 - b_t\n",
    "ab_t = torch.cumsum(a_t.log(), dim=0).exp()    \n",
    "ab_t[0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc9001e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# construct model\n",
    "nn_model = ContextUnet(in_channels=3, n_feat=n_feat, n_cfeat=n_cfeat, height=height).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ed46d7",
   "metadata": {},
   "source": [
    "# Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092e933a",
   "metadata": {},
   "source": [
    "resnet 모델을 불러오고 optimizer를 재설정해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88afdba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reset neural network\n",
    "nn_model = ContextUnet(in_channels=3, n_feat=n_feat, n_cfeat=n_cfeat, height=height).to(device)\n",
    "\n",
    "# re setup optimizer\n",
    "optim = torch.optim.Adam(nn_model.parameters(), lr=lrate)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ad8f0a4-ca4f-4f46-b060-f6cb7d5ce449",
   "metadata": {
    "tags": []
   },
   "source": [
    "# training with context code\n",
    "# set into train mode\n",
    "nn_model.train()\n",
    "\n",
    "for ep in range(n_epoch):\n",
    "    print(f'epoch {ep}')\n",
    "    \n",
    "    # linearly decay learning rate\n",
    "    optim.param_groups[0]['lr'] = lrate*(1-ep/n_epoch)\n",
    "    \n",
    "    pbar = tqdm(dataloader, mininterval=2 )\n",
    "    for x, c in pbar:   # x: images  c: context\n",
    "        optim.zero_grad()\n",
    "        x = x.to(device)\n",
    "        c = c.to(x)\n",
    "        \n",
    "        # randomly mask out c\n",
    "        context_mask = torch.bernoulli(torch.zeros(c.shape[0]) + 0.9).to(device)\n",
    "        c = c * context_mask.unsqueeze(-1)\n",
    "        \n",
    "        # perturb data\n",
    "        noise = torch.randn_like(x)\n",
    "        t = torch.randint(1, timesteps + 1, (x.shape[0],)).to(device) \n",
    "        x_pert = perturb_input(x, t, noise)\n",
    "        \n",
    "        # use network to recover noise\n",
    "        pred_noise = nn_model(x_pert, t / timesteps, c=c)\n",
    "        \n",
    "        # loss is mean squared error between the predicted and true noise\n",
    "        loss = F.mse_loss(pred_noise, noise)\n",
    "        loss.backward()\n",
    "        \n",
    "        optim.step()\n",
    "\n",
    "    # save model periodically\n",
    "    if ep%4==0 or ep == int(n_epoch-1):\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.mkdir(save_dir)\n",
    "        torch.save(nn_model.state_dict(), save_dir + f\"context_model_{ep}.pth\")\n",
    "        print('saved model at ' + save_dir + f\"context_model_{ep}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ee3c2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load in pretrain model weights and set to eval mode\n",
    "nn_model.load_state_dict(torch.load(f\"{save_dir}/context_model_trained.pth\", map_location=device))\n",
    "nn_model.eval() \n",
    "print(\"Loaded in Context Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf93c0a-46bf-4809-87af-b2ad84f8c515",
   "metadata": {},
   "source": [
    "# Sampling with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d45152-ddb0-4c6f-88de-251bea95dd60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helper function; 예측되는 노이즈를 제거하는 함수입니다.\n",
    "# (하지만 이미지가 붕괴하는 것을 방지하기 위해 일부 노이즈는 다시 더합니다)\n",
    "def denoise_add_noise(x, t, pred_noise, z=None):\n",
    "    if z is None:\n",
    "        z = torch.randn_like(x)\n",
    "    noise = b_t.sqrt()[t] * z\n",
    "    mean = (x - pred_noise * ((1 - a_t[t]) / (1 - ab_t[t]).sqrt())) / a_t[t].sqrt()\n",
    "    return mean + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a053f3fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 표준 알고리즘을 사용하여 샘플링합니다.\n",
    "@torch.no_grad()\n",
    "def sample_ddpm(n_sample, save_rate=20):\n",
    "    # x_T ~ N(0, 1), sample initial noise\n",
    "    samples = torch.randn(n_sample, 3, height, height).to(device)  \n",
    "\n",
    "    # 그래프를 기리기 위해서 각 스텝을 트래킹합니다.\n",
    "    intermediate = [] \n",
    "    for i in range(timesteps, 0, -1):\n",
    "        print(f'sampling timestep {i:3d}', end='\\r')\n",
    "\n",
    "        # 시간 텐서의 모양을 조정합니다.\n",
    "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
    "\n",
    "        # 일부 노이즈를 골라 더합니다. 만약 i가 1이라면 더하지 않습니다.\n",
    "        z = torch.randn_like(samples) if i > 1 else 0\n",
    "\n",
    "        eps = nn_model(samples, t)    # predict noise e_(x_t,t)\n",
    "        samples = denoise_add_noise(samples, i, eps, z)\n",
    "        if i % save_rate ==0 or i==timesteps or i<8:\n",
    "            intermediate.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    intermediate = np.stack(intermediate)\n",
    "    return samples, intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c457149",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 랜덤하게 선택된 컨텍스트에 대해 샘플들을 시각화합니다.\n",
    "plt.clf()\n",
    "ctx = F.one_hot(torch.randint(0, 5, (32,)), 5).to(device=device).float()\n",
    "samples, intermediate = sample_ddpm_context(32, ctx)\n",
    "animation_ddpm_context = plot_sample(intermediate,32,4,save_dir, \"ani_run\", None, save=False)\n",
    "HTML(animation_ddpm_context.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48277806",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_images(imgs, nrow=2):\n",
    "    _, axs = plt.subplots(nrow, imgs.shape[0] // nrow, figsize=(4,2 ))\n",
    "    axs = axs.flatten()\n",
    "    for img, ax in zip(imgs, axs):\n",
    "        img = (img.permute(1, 2, 0).clip(-1, 1).detach().cpu().numpy() + 1) / 2\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31747923",
   "metadata": {},
   "source": [
    "원 핫 인코딩된 벡터로 특징을 표현하는데, 이를 임의로 설정할 수 있고 여러 개를 조합할 수도 있습니다.  \n",
    "각 벡터가 무엇을 의미하게 되는지 잘 살펴보도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae01643",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 유저 정의 컨텍스트\n",
    "# 각자의 원하는 학습 방향에 따라 설정할 수 있습니다.\n",
    "ctx = torch.tensor([\n",
    "    # hero, non-hero, food, spell, side-facing\n",
    "    [1,0,0,0,0],  \n",
    "    [1,0,0,0,0],    \n",
    "    [0,0,0,0,1],\n",
    "    [0,0,0,0,1],    \n",
    "    [0,1,0,0,0],\n",
    "    [0,1,0,0,0],\n",
    "    [0,0,1,0,0],\n",
    "    [0,0,1,0,0],\n",
    "]).float().to(device)\n",
    "samples, _ = sample_ddpm_context(ctx.shape[0], ctx)\n",
    "show_images(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c564ca7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 정의된 컨텍스트 조합하기\n",
    "ctx = torch.tensor([\n",
    "    # hero, non-hero, food, spell, side-facing\n",
    "    [1,0,0,0,0],      #human\n",
    "    [1,0,0.6,0,0],    \n",
    "    [0,0,0.6,0.4,0],  \n",
    "    [1,0,0,0,1],  \n",
    "    [1,1,0,0,0],\n",
    "    [1,0,0,1,0]\n",
    "]).float().to(device)\n",
    "samples, _ = sample_ddpm_context(ctx.shape[0], ctx)\n",
    "show_images(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd10c025-c516-4d17-88a6-3036ea996264",
   "metadata": {},
   "source": [
    "# Acknowledgments\n",
    "Sprites by ElvGames, [FrootsnVeggies](https://zrghr.itch.io/froots-and-veggies-culinary-pixels) and  [kyrise](https://kyrise.itch.io/)   \n",
    "This code is modified from, https://github.com/cloneofsimo/minDiffusion   \n",
    "Diffusion model is based on [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) and [Denoising Diffusion Implicit Models](https://arxiv.org/abs/2010.02502)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1aff1f-4e44-4d32-b191-81bc29f58fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "7fecef7fb14ad1d71e869da5296badaba2a50d2b864b0443ebcc6afac654c29b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
