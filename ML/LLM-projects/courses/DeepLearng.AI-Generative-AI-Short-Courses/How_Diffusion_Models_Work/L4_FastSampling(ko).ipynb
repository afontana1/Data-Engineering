{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f912400",
   "metadata": {},
   "source": [
    "# Lab 4, Fast Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4a15bb",
   "metadata": {},
   "source": [
    "지금까지 sampling을 통해 denoising하는 방식에 대해 알아봤습니다.  \n",
    "하지만 사실 지금까지 공부한 sampling 방식은 굉장히 느립니다.  \n",
    "본 강의에서는 속도를 개선한 sampling 방식을 다룹니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700e687c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "from diffusion_utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0d229a",
   "metadata": {},
   "source": [
    "# Setting Things Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23507e17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ContextUnet(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat=256, n_cfeat=10, height=28):  # cfeat - context features\n",
    "        super(ContextUnet, self).__init__()\n",
    "\n",
    "        # 입력 채널의 수, 중간 피쳐맵의 수, 클래스의 수\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_cfeat = n_cfeat\n",
    "        self.h = height  # h == w 라고 가정합니다. 단, 28,24,20처럼, 반드시 4로 나누어 떨어져야 합니다. \n",
    "\n",
    "        # 초기 컨볼루션 레이어를 초기화합니다.\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
    "\n",
    "        # U-Net을 두 단계에 걸쳐 다운샘플링합니다.\n",
    "        self.down1 = UnetDown(n_feat, n_feat)        # down1 #[10, 256, 8, 8]\n",
    "        self.down2 = UnetDown(n_feat, 2 * n_feat)    # down2 #[10, 256, 4,  4]\n",
    "        \n",
    "        # 기존: self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n",
    "        self.to_vec = nn.Sequential(nn.AvgPool2d((4)), nn.GELU())\n",
    "\n",
    "        # 한 층의 fully connected neural network에 타입 스텝과 context label을 삽입합니다.\n",
    "        self.timeembed1 = EmbedFC(1, 2*n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 1*n_feat)\n",
    "        self.contextembed1 = EmbedFC(n_cfeat, 2*n_feat)\n",
    "        self.contextembed2 = EmbedFC(n_cfeat, 1*n_feat)\n",
    "\n",
    "        # 세 단계에 걸쳐 U-Net을 업샘플링합니다.\n",
    "        self.up0 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, self.h//4, self.h//4), # up-sample  \n",
    "            nn.GroupNorm(8, 2 * n_feat), # normalize                       \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
    "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
    "\n",
    "        # 마지막 컨볼루션 레이어를 입력 이미지의 채널 수와 일치할 수 있도록 초기화합니다.\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1), # 피쳐맵의 수를 줄입니다.   #in_channels, out_channels, kernel_size, stride=1, padding=0\n",
    "            nn.GroupNorm(8, n_feat), # 정규화\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1), # 입력과 동일한 채널로 맞춰줍니다.\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, c=None):\n",
    "        \"\"\"\n",
    "        x : (batch, n_feat, h, w) : input image\n",
    "        t : (batch, n_cfeat)      : time step\n",
    "        c : (batch, n_classes)    : context label\n",
    "        \"\"\"\n",
    "        # x는 입력 이미지, c는 context label, t는 타임 스텝입니다. \n",
    "\n",
    "        # 초기 컨볼루션 레이어를 통해 입력 이미지를 전달합니다.\n",
    "        x = self.init_conv(x)\n",
    "        # 다운 샘플링을 통해 결과를 전달합니다.\n",
    "        down1 = self.down1(x)       #[10, 256, 8, 8]\n",
    "        down2 = self.down2(down1)   #[10, 256, 4, 4]\n",
    "        \n",
    "        # 피쳐맵을 벡터로 전환하고 활성화함수를 적용합니다.\n",
    "        hiddenvec = self.to_vec(down2)\n",
    "        \n",
    "        # 만약 context_mask가 1이라면 마스크를 지웁니다.\n",
    "        if c is None:\n",
    "            c = torch.zeros(x.shape[0], self.n_cfeat).to(x)\n",
    "            \n",
    "        # context와 타임 스텝을 삽입합니다.\n",
    "        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)     # (batch, 2*n_feat, 1,1)\n",
    "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
    "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
    "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
    "        #print(f\"uunet forward: cemb1 {cemb1.shape}. temb1 {temb1.shape}, cemb2 {cemb2.shape}. temb2 {temb2.shape}\")\n",
    "\n",
    "\n",
    "        up1 = self.up0(hiddenvec)\n",
    "        up2 = self.up1(cemb1*up1 + temb1, down2)  # 임베딩을 더하고 곱해줍니다.\n",
    "        up3 = self.up2(cemb2*up2 + temb2, down1)\n",
    "        out = self.out(torch.cat((up3, x), 1))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c3a942",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "# diffusion hyperparameters\n",
    "timesteps = 500\n",
    "beta1 = 1e-4\n",
    "beta2 = 0.02\n",
    "\n",
    "# network hyperparameters\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else torch.device('cpu'))\n",
    "n_feat = 64 # 64 hidden dimension feature\n",
    "n_cfeat = 5 # context vector is of size 5\n",
    "height = 16 # 16x16 image\n",
    "save_dir = './weights/'\n",
    "\n",
    "# training hyperparameters\n",
    "batch_size = 100\n",
    "n_epoch = 32\n",
    "lrate=1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a705d0a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# construct DDPM noise schedule\n",
    "b_t = (beta2 - beta1) * torch.linspace(0, 1, timesteps + 1, device=device) + beta1\n",
    "a_t = 1 - b_t\n",
    "ab_t = torch.cumsum(a_t.log(), dim=0).exp()    \n",
    "ab_t[0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc9001e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# construct model\n",
    "nn_model = ContextUnet(in_channels=3, n_feat=n_feat, n_cfeat=n_cfeat, height=height).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0d59f3",
   "metadata": {},
   "source": [
    "# Fast Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d337feb",
   "metadata": {},
   "source": [
    "기존의 DDPM 대신 DDIM을 사용하여 denoising합니다.  \n",
    "어떤 차이점을 지녔는지에 대해 깊게 공부하고 싶다면 따로 검색해봐야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92257ac9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DDIM를 위한 샘플링 함수를 정의합니다.\n",
    "# DDIM을 사용하여 노이즈를 제거합니다.\n",
    "def denoise_ddim(x, t, t_prev, pred_noise):\n",
    "    ab = ab_t[t]\n",
    "    ab_prev = ab_t[t_prev]\n",
    "    \n",
    "    x0_pred = ab_prev.sqrt() / ab.sqrt() * (x - (1 - ab).sqrt() * pred_noise)\n",
    "    dir_xt = (1 - ab_prev).sqrt() * pred_noise\n",
    "\n",
    "    return x0_pred + dir_xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0f88be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 모델 가중치를 불러오고 평가 모드로 세팅합니다.\n",
    "nn_model.load_state_dict(torch.load(f\"{save_dir}/model_31.pth\", map_location=device))\n",
    "nn_model.eval() \n",
    "print(\"Loaded in Model without context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f49d9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DDIM를 사용하여 빠르게 샘플링하기\n",
    "@torch.no_grad()\n",
    "def sample_ddim(n_sample, n=20):\n",
    "    # x_T ~ N(0, 1), 초기 노이즈를 샘플링합니다.\n",
    "    samples = torch.randn(n_sample, 3, height, height).to(device)  \n",
    "\n",
    "    # 생성된 스텝을 추적하여 그래프로 그리기 위한 배열\n",
    "    intermediate = [] \n",
    "    step_size = timesteps // n\n",
    "    for i in range(timesteps, 0, -step_size):\n",
    "        print(f'sampling timestep {i:3d}', end='\\r')\n",
    "\n",
    "        # 시간 텐서 모양 조정\n",
    "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
    "\n",
    "        eps = nn_model(samples, t)    # predict noise e_(x_t,t)\n",
    "        samples = denoise_ddim(samples, i, i - step_size, eps)\n",
    "        intermediate.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    intermediate = np.stack(intermediate)\n",
    "    return samples, intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8dd59a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 샘플을 시각화합니다.\n",
    "plt.clf()\n",
    "samples, intermediate = sample_ddim(32, n=25)\n",
    "animation_ddim = plot_sample(intermediate,32,4,save_dir, \"ani_run\", None, save=False)\n",
    "HTML(animation_ddim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b055c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 모델 가중치를 불러오고 평가 모드로 세팅합니다.\n",
    "nn_model.load_state_dict(torch.load(f\"{save_dir}/context_model_31.pth\", map_location=device))\n",
    "nn_model.eval() \n",
    "print(\"Loaded in Context Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26ea76d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 컨텍스트를 포함한 fast 샘플링 알고리즘\n",
    "@torch.no_grad()\n",
    "def sample_ddim_context(n_sample, context, n=20):\n",
    "    # x_T ~ N(0, 1), 초기 노이즈 샘플링\n",
    "    samples = torch.randn(n_sample, 3, height, height).to(device)  \n",
    "\n",
    "    # 생성된 스텝을 추적하여 그래프로 그리기 위한 배열\n",
    "    intermediate = [] \n",
    "    step_size = timesteps // n\n",
    "    for i in range(timesteps, 0, -step_size):\n",
    "        print(f'sampling timestep {i:3d}', end='\\r')\n",
    "\n",
    "        # 시간 텐서 재조정\n",
    "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
    "\n",
    "        eps = nn_model(samples, t, c=context)    # predict noise e_(x_t,t)\n",
    "        samples = denoise_ddim(samples, i, i - step_size, eps)\n",
    "        intermediate.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    intermediate = np.stack(intermediate)\n",
    "    return samples, intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdcd4e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 샘플 시각화\n",
    "plt.clf()\n",
    "ctx = F.one_hot(torch.randint(0, 5, (32,)), 5).to(device=device).float()\n",
    "samples, intermediate = sample_ddim_context(32, ctx)\n",
    "animation_ddpm_context = plot_sample(intermediate,32,4,save_dir, \"ani_run\", None, save=False)\n",
    "HTML(animation_ddpm_context.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db7e8fe-7c97-450f-8609-62160d65b478",
   "metadata": {},
   "source": [
    "#### 그렇다면 실제 DDPM과 DDIM의 속도를 비교해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13da999a-ee74-4ed4-a722-6ac3d4cbf3bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helper function; 예측되는 노이즈를 제거하는 함수입니다.\n",
    "# (하지만 이미지가 붕괴하는 것을 방지하기 위해 일부 노이즈는 다시 더합니다)\n",
    "def denoise_add_noise(x, t, pred_noise, z=None):\n",
    "    if z is None:\n",
    "        z = torch.randn_like(x)\n",
    "    noise = b_t.sqrt()[t] * z\n",
    "    mean = (x - pred_noise * ((1 - a_t[t]) / (1 - ab_t[t]).sqrt())) / a_t[t].sqrt()\n",
    "    return mean + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d31547d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 표준 알고리즘을 사용하여 샘플링합니다.\n",
    "@torch.no_grad()\n",
    "def sample_ddpm(n_sample, save_rate=20):\n",
    "    # x_T ~ N(0, 1), sample initial noise\n",
    "    samples = torch.randn(n_sample, 3, height, height).to(device)  \n",
    "\n",
    "    # 그래프를 기리기 위해서 각 스텝을 트래킹합니다.\n",
    "    intermediate = [] \n",
    "    for i in range(timesteps, 0, -1):\n",
    "        print(f'sampling timestep {i:3d}', end='\\r')\n",
    "\n",
    "        # 시간 텐서의 모양을 조정합니다.\n",
    "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
    "\n",
    "        # 일부 노이즈를 골라 더합니다. 만약 i가 1이라면 더하지 않습니다.\n",
    "        z = torch.randn_like(samples) if i > 1 else 0\n",
    "\n",
    "        eps = nn_model(samples, t)    # predict noise e_(x_t,t)\n",
    "        samples = denoise_add_noise(samples, i, eps, z)\n",
    "        if i % save_rate ==0 or i==timesteps or i<8:\n",
    "            intermediate.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    intermediate = np.stack(intermediate)\n",
    "    return samples, intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c002584",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -r 1 sample_ddim(32, n=25)\n",
    "%timeit -r 1 sample_ddpm(32, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ab28dc-74b7-47e3-b8a9-cace21a6a2d7",
   "metadata": {},
   "source": [
    "# Acknowledgments\n",
    "Sprites by ElvGames, [FrootsnVeggies](https://zrghr.itch.io/froots-and-veggies-culinary-pixels) and  [kyrise](https://kyrise.itch.io/)   \n",
    "This code is modified from, https://github.com/cloneofsimo/minDiffusion   \n",
    "Diffusion model is based on [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) and [Denoising Diffusion Implicit Models](https://arxiv.org/abs/2010.02502)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce56fa3-3d85-4305-9eff-0a352a59401c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
