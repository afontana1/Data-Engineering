{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights and Biases (W&B)와 함께 디퓨전 모델 학습하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 노트북에서 우리는 W&B와 함께 디퓨전 모델을 학습하는 방법을 배워볼 것입니다.  \n",
    "[\"How diffusion models work\"](https://www.deeplearning.ai/short-courses/how-diffusion-models-work/) course 강의의 Lab3 노트북을 사용합니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다룰 내용은 다음과 같습니다:\n",
    "- training loss와 metric의 로그를 남기기\n",
    "- 학습 동안 모델로부터 샘플을 추출하여 W&B에 업로드\n",
    "- 모델 체크포인트를 W&B에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from utilities import *\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W&B의 기능을 제대로 이용하기 위해 계정 생성을 추천합니다.  \n",
    "여기에서는 익명 옵션을 켜고 학습을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "wandb.login(anonymous=\"allow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb에 로그로 남을 파라미터들을 저장합니다.\n",
    "DATA_DIR = Path('./data/')\n",
    "SAVE_DIR = Path('./data/weights/')\n",
    "SAVE_DIR.mkdir(exist_ok=True, parents=True)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "config = SimpleNamespace(\n",
    "    # 하이퍼파라미터\n",
    "    num_samples = 30,\n",
    "\n",
    "    # 디퓨전 하이퍼파라미터\n",
    "    timesteps = 500,\n",
    "    beta1 = 1e-4,\n",
    "    beta2 = 0.02,\n",
    "\n",
    "    # 네트워크 하이퍼파라미터\n",
    "    n_feat = 64, # 64 hidden dimension feature\n",
    "    n_cfeat = 5, # context vector is of size 5\n",
    "    height = 16, # 16x16 이미지\n",
    "    \n",
    "    # 학습 하이퍼파라미터\n",
    "    batch_size = 100,\n",
    "    n_epoch = 32,\n",
    "    lrate = 1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDPM noise schedular와 sampler를 셋업합니다. (Diffusion 강의에서와 동일합니다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- perturb_input: 스케줄 상에 정해진 타임스텝에서 이미지에 노이즈를 추가합니다\n",
    "- sample_ddpm_context: DDPM sampler를 사용하여 이미지를 생성합니다. 우리는 학습 동안 모델로부터 샘플을 추출하기 위해 잏 함수를 사용하고, 어떻게 학습이 진행되는지 확인할 것입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddpm sampler 함수를 셋업합니다\n",
    "perturb_input, sample_ddpm_context = setup_ddpm(config.beta1, \n",
    "                                                config.beta2, \n",
    "                                                config.timesteps, \n",
    "                                                DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 구축합니다\n",
    "nn_model = ContextUnet(in_channels=3, \n",
    "                       n_feat=config.n_feat, \n",
    "                       n_cfeat=config.n_cfeat, \n",
    "                       height=config.height).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 불러오고 옵티마이저를 설정합니다\n",
    "dataset = CustomDataset.from_np(path=DATA_DIR)\n",
    "dataloader = DataLoader(dataset, \n",
    "                        batch_size=config.batch_size, \n",
    "                        shuffle=True)\n",
    "optim = torch.optim.Adam(nn_model.parameters(), lr=config.lrate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 클래스에서 6개의 샘플과 함께 고정된 context vector를 고릅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Noise vecotr\n",
    "# x_T ~ N(0, 1), sample initial noise\n",
    "noises = torch.randn(config.num_samples, 3, \n",
    "                     config.height, config.height).to(DEVICE)  \n",
    "\n",
    "# 고정된 context vector는 다음으로부터 추출합니다\n",
    "ctx_vector = F.one_hot(torch.tensor([0,0,0,0,0,0,   # hero\n",
    "                                     1,1,1,1,1,1,   # non-hero\n",
    "                                     2,2,2,2,2,2,   # food\n",
    "                                     3,3,3,3,3,3,   # spell\n",
    "                                     4,4,4,4,4,4]), # side-facing \n",
    "                       5).to(DEVICE).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 학습 셀은 CPU로 실행하면 굉장히 오래 걸립니다.  \n",
    "강의에서는 GPU로 학습된 결과를 보여줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 결과는 >> [여기](https://wandb.ai/dlai-course/dlai_sprite_diffusion/runs/pzs3gsyo) << 에서 확인 가능합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb run을 생성합니다\n",
    "run = wandb.init(project=\"dlai_sprite_diffusion\", \n",
    "                 job_type=\"train\", \n",
    "                 config=config)\n",
    "\n",
    "# W&B로부터 config를 가져와 전달합니다\n",
    "config = wandb.config\n",
    "\n",
    "for ep in tqdm(range(config.n_epoch), leave=True, total=config.n_epoch):\n",
    "    # 학습 모드 설정\n",
    "    nn_model.train()\n",
    "    optim.param_groups[0]['lr'] = config.lrate*(1-ep/config.n_epoch)\n",
    "    \n",
    "    pbar = tqdm(dataloader, leave=False)\n",
    "    for x, c in pbar:   # x: images  c: context\n",
    "        optim.zero_grad()\n",
    "        x = x.to(DEVICE)\n",
    "        c = c.to(DEVICE)   \n",
    "        context_mask = torch.bernoulli(torch.zeros(c.shape[0]) + 0.8).to(DEVICE)\n",
    "        c = c * context_mask.unsqueeze(-1)        \n",
    "        noise = torch.randn_like(x)\n",
    "        t = torch.randint(1, config.timesteps + 1, (x.shape[0],)).to(DEVICE) \n",
    "        x_pert = perturb_input(x, t, noise)      \n",
    "        pred_noise = nn_model(x_pert, t / config.timesteps, c=c)      \n",
    "        loss = F.mse_loss(pred_noise, noise)\n",
    "        loss.backward()    \n",
    "        optim.step()\n",
    "\n",
    "        wandb.log({\"loss\": loss.item(),\n",
    "                   \"lr\": optim.param_groups[0]['lr'],\n",
    "                   \"epoch\": ep})\n",
    "\n",
    "\n",
    "    # 모델을 주기적으로 저장합니다\n",
    "    if ep%4==0 or ep == int(config.n_epoch-1):\n",
    "        nn_model.eval()\n",
    "        ckpt_file = SAVE_DIR/f\"context_model.pth\"\n",
    "        torch.save(nn_model.state_dict(), ckpt_file)\n",
    "\n",
    "        artifact_name = f\"{wandb.run.id}_context_model\"\n",
    "        at = wandb.Artifact(artifact_name, type=\"model\")\n",
    "        at.add_file(ckpt_file)\n",
    "        wandb.log_artifact(at, aliases=[f\"epoch_{ep}\"])\n",
    "\n",
    "        samples, _ = sample_ddpm_context(nn_model, \n",
    "                                         noises, \n",
    "                                         ctx_vector[:config.num_samples])\n",
    "        wandb.log({\n",
    "            \"train_samples\": [\n",
    "                wandb.Image(img) for img in samples.split(1)\n",
    "            ]})\n",
    "        \n",
    "# W&B run을 종료합니다\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습이 종료된 후 대시보드를 들어가보면 추출된 샘플 이미지들을 학습 단계별로 확인할 수 있습니다.  \n",
    "학습이 진행됨에 따라 모델이 점점 더 정확한 이미지를 생성하는 것을 알 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
