{"components":{"comp-compose-params-for-model-tuner":{"executorLabel":"exec-compose-params-for-model-tuner","inputDefinitions":{"parameters":{"adapter_length":{"defaultValue":0,"description":"Adapter length.","isOptional":true,"parameterType":"NUMBER_INTEGER"},"display_name":{"description":"The display name of the model tuner.","parameterType":"STRING"},"enable_checkpoint_selection":{"defaultValue":"default","description":"One of 'true', 'false', or 'default'. If set\nto 'true' and if evaluation_data_uri is set, select the best tuned\ncheckpoint. If 'default' is set, checkpoint selection is enabled\nfor bison@001 models, and disabled for others.","isOptional":true,"parameterType":"STRING"},"enable_early_stopping":{"defaultValue":true,"description":"If set to True, and if evaluation_data_uri is set,\nthen early stopping will be enabled. If early stopping is enabled, the\nmax recommended train_steps is 1,000.","isOptional":true,"parameterType":"BOOLEAN"},"encryption_spec_key_name":{"defaultValue":"","description":"Customer-managed encryption key. If this is set,\nthen all resources created by the CustomJob will be encrypted with the\nprovided encryption key. Note that this is not supported for TPU at\nthe moment.","isOptional":true,"parameterType":"STRING"},"evaluation_data_uri":{"defaultValue":"","description":"GCS URI of eval dataset.","isOptional":true,"parameterType":"STRING"},"evaluation_interval":{"defaultValue":20,"description":"Evaluation after every this number of tuning steps.","isOptional":true,"parameterType":"NUMBER_INTEGER"},"learning_rate":{"defaultValue":-1,"description":"Learning rate hyperparameter. When learning rate is less\nthan 0, it uses the default learning rate per tuning method per model.\nThis parameter will be deprecated.","isOptional":true,"parameterType":"NUMBER_DOUBLE"},"learning_rate_multiplier":{"defaultValue":1,"description":"Learning rate multiplier for tuning. Under the\nhood, the actual \"learning rate\" will be learning_rate_multiplier *\nthe \"recommended learning rate\" per model, so customers don't have to\nkeep track of the recommended learning rate for each model or tuning\nmethod.","isOptional":true,"parameterType":"NUMBER_DOUBLE"},"location":{"defaultValue":"us-central1","description":"Region in which the launcher for the tuning job is created (not\nthe region for running the whole pipeline).","isOptional":true,"parameterType":"STRING"},"max_eval_inputs_length":{"defaultValue":-1,"description":"Max inputs length from eval data. If the max of\nmax_train_inputs_length and max_eval_inputs_length is less than 0 or\nmore than the default inputs length, which represents the max inputs\nlength supported, use the default inputs length.","isOptional":true,"parameterType":"NUMBER_INTEGER"},"max_eval_outputs_length":{"defaultValue":-1,"description":"Max outputs length from eval data. If the max\nof max_train_outputs_length and max_eval_outputs_length is less than 0\nor more than the default outputs length, which represents the max\noutputs length supported, use the default outputs length.","isOptional":true,"parameterType":"NUMBER_INTEGER"},"max_train_inputs_length":{"defaultValue":-1,"description":"Max inputs length from train data. If the max of\nmax_train_inputs_length and max_eval_inputs_length is less than 0 or\nmore than the default inputs length, which represents the max inputs\nlength supported, use the default inputs length.","isOptional":true,"parameterType":"NUMBER_INTEGER"},"max_train_outputs_length":{"defaultValue":-1,"description":"Max outputs length from train data. If the max\nof max_train_outputs_length and max_eval_outputs_length is less than 0\nor more than the default outputs length, which represents the max\noutputs length supported, use the default outputs length.","isOptional":true,"parameterType":"NUMBER_INTEGER"},"model":{"defaultValue":"text-bison@001","description":"Model type to use.","isOptional":true,"parameterType":"STRING"},"pipeline_region":{"defaultValue":"{{$.pipeline_google_cloud_location}}","description":"The region the pipeline runs in.","isOptional":true,"parameterType":"STRING"},"tpu_training_skip_cmek":{"defaultValue":false,"description":"If True, skip CMEK setup for TPU training.","isOptional":true,"parameterType":"BOOLEAN"},"train_data_uri":{"description":"File or file pattern containing train data.","parameterType":"STRING"},"train_steps":{"defaultValue":0,"description":"Number of training steps.","isOptional":true,"parameterType":"NUMBER_INTEGER"},"tuning_method":{"defaultValue":"tune_v2","description":"The model tuning method. Available methods: tune,\ntune_v2.","isOptional":true,"parameterType":"STRING"},"use_tpu":{"defaultValue":true,"description":"If True, use TPU. Otherwise use GPU.","isOptional":true,"parameterType":"BOOLEAN"}}},"outputDefinitions":{"parameters":{"location":{"parameterType":"STRING"},"payload_first_part":{"parameterType":"STRING"},"payload_second_part":{"parameterType":"STRING"}}}},"comp-condition-1":{"dag":{"tasks":{"tensorboard-uploader":{"cachingOptions":{"enableCache":true},"componentRef":{"name":"comp-tensorboard-uploader"},"inputs":{"artifacts":{"metrics_directory_uri":{"componentInputArtifact":"pipelinechannel--tuning-graph-tensorboard_metrics"}},"parameters":{"experiment_name":{"runtimeValue":{"constant":"large-language-model-tuner-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}"}},"location":{"runtimeValue":{"constant":"{{$.pipeline_google_cloud_location}}"}},"project":{"componentInputParameter":"pipelinechannel--project"},"tensorboard_id":{"componentInputParameter":"pipelinechannel--tensorboard_resource_id"}}},"taskInfo":{"name":"tensorboard-uploader"}}}},"inputDefinitions":{"artifacts":{"pipelinechannel--tuning-graph-tensorboard_metrics":{"artifactType":{"schemaTitle":"system.Artifact","schemaVersion":"0.0.1"}}},"parameters":{"pipelinechannel--project":{"parameterType":"STRING"},"pipelinechannel--tensorboard_resource_id":{"parameterType":"STRING"},"pipelinechannel--validate-pipeline-with_tensorboard":{"parameterType":"BOOLEAN"}}}},"comp-convert-jsonl-to-tfrecord":{"executorLabel":"exec-convert-jsonl-to-tfrecord","inputDefinitions":{"parameters":{"dataset_uri":{"parameterType":"STRING"},"max_examples":{"defaultValue":0,"isOptional":true,"parameterType":"NUMBER_INTEGER"},"min_examples":{"defaultValue":10,"isOptional":true,"parameterType":"NUMBER_INTEGER"}}},"outputDefinitions":{"parameters":{"output_dir":{"parameterType":"LIST"},"output_file":{"parameterType":"STRING"}}}},"comp-convert-jsonl-to-tfrecord-2":{"executorLabel":"exec-convert-jsonl-to-tfrecord-2","inputDefinitions":{"parameters":{"dataset_uri":{"parameterType":"STRING"},"max_examples":{"defaultValue":0,"isOptional":true,"parameterType":"NUMBER_INTEGER"},"min_examples":{"defaultValue":10,"isOptional":true,"parameterType":"NUMBER_INTEGER"}}},"outputDefinitions":{"parameters":{"output_dir":{"parameterType":"LIST"},"output_file":{"parameterType":"STRING"}}}},"comp-create-endpoint-and-deploy-model":{"executorLabel":"exec-create-endpoint-and-deploy-model","inputDefinitions":{"parameters":{"deploy_model":{"defaultValue":true,"description":"Whether to deploy the model to an endpoint. Default\nis ``True``. If ``False``, the model will not be deployed and output\nartifacts will contain empty strings.","isOptional":true,"parameterType":"BOOLEAN"},"display_name":{"description":"Name of the model (shown in Model Registry).","parameterType":"STRING"},"encryption_spec_key_name":{"defaultValue":"","description":"Customer-managed encryption key.","isOptional":true,"parameterType":"STRING"},"location":{"description":"Location for model upload and deployment.","parameterType":"STRING"},"model_resource_name":{"description":"Path to the created Model on Model Registry.","parameterType":"STRING"},"project":{"description":"Name of the GCP project.","parameterType":"STRING"},"regional_endpoint":{"description":"Regional API endpoint.","parameterType":"STRING"},"service_account":{"defaultValue":"","description":"If set, then a custom service account will be used.","isOptional":true,"parameterType":"STRING"}}},"outputDefinitions":{"parameters":{"create_endpoint_gcp_resources":{"description":"Serialized JSON of GCP resources for\ncreating an endpoint.","parameterType":"STRING"},"deploy_model_gcp_resources":{"description":"Serialized JSON of GCP resources for deploying\nthe model.","parameterType":"STRING"},"endpoint_resource_name":{"description":"Path to the created endpoint on Online Prediction.","parameterType":"STRING"}}}},"comp-deployment-graph":{"dag":{"outputs":{"parameters":{"endpoint_resource_name":{"valueFromParameter":{"outputParameterKey":"endpoint_resource_name","producerSubtask":"create-endpoint-and-deploy-model"}},"model_resource_name":{"valueFromParameter":{"outputParameterKey":"model_resource_name","producerSubtask":"upload-llm-model"}}}},"tasks":{"create-endpoint-and-deploy-model":{"cachingOptions":{"enableCache":true},"componentRef":{"name":"comp-create-endpoint-and-deploy-model"},"dependentTasks":["upload-llm-model"],"inputs":{"parameters":{"display_name":{"componentInputParameter":"model_display_name"},"encryption_spec_key_name":{"componentInputParameter":"encryption_spec_key_name"},"location":{"componentInputParameter":"model_location"},"model_resource_name":{"taskOutputParameter":{"outputParameterKey":"model_resource_name","producerTask":"upload-llm-model"}},"project":{"componentInputParameter":"project"},"regional_endpoint":{"componentInputParameter":"regional_endpoint"},"service_account":{"componentInputParameter":"service_account"}}},"taskInfo":{"name":"create-endpoint-and-deploy-model"}},"upload-llm-model":{"cachingOptions":{"enableCache":true},"componentRef":{"name":"comp-upload-llm-model"},"inputs":{"artifacts":{"artifact_uri":{"componentInputArtifact":"saved_model"}},"parameters":{"encryption_spec_key_name":{"componentInputParameter":"encryption_spec_key_name"},"location":{"componentInputParameter":"model_location"},"model_display_name":{"componentInputParameter":"model_display_name"},"model_reference_name":{"componentInputParameter":"large_model_reference"},"project":{"componentInputParameter":"project"},"regional_endpoint":{"componentInputParameter":"regional_endpoint"}}},"taskInfo":{"name":"upload-llm-model"}}}},"inputDefinitions":{"artifacts":{"saved_model":{"artifactType":{"schemaTitle":"system.Artifact","schemaVersion":"0.0.1"},"description":"Output saved_model of adapter tuning."}},"parameters":{"encryption_spec_key_name":{"defaultValue":"","description":"Customer-managed encryption key.","isOptional":true,"parameterType":"STRING"},"large_model_reference":{"defaultValue":"text-bison@001","description":"Large model reference ID indicating which model to\nuse.","isOptional":true,"parameterType":"STRING"},"model_display_name":{"description":"Name of the model (shown in model registry).","parameterType":"STRING"},"model_location":{"defaultValue":"us-central1","description":"Location for model upload and deployment.","isOptional":true,"parameterType":"STRING"},"project":{"description":"Name of the GCP project.","parameterType":"STRING"},"regional_endpoint":{"defaultValue":"https://us-central1-aiplatform.googleapis.com/ui","description":"Regional API endpoint.","isOptional":true,"parameterType":"STRING"},"service_account":{"defaultValue":"","description":"If set, then a custom service account will be used.","isOptional":true,"parameterType":"STRING"}}},"outputDefinitions":{"parameters":{"endpoint_resource_name":{"parameterType":"STRING"},"model_resource_name":{"parameterType":"STRING"}}}},"comp-export-managed-dataset":{"executorLabel":"exec-export-managed-dataset","inputDefinitions":{"parameters":{"dataset_name":{"parameterType":"STRING"},"dataset_uri":{"parameterType":"STRING"},"regional_endpoint":{"parameterType":"STRING"}}},"outputDefinitions":{"parameters":{"export_location":{"parameterType":"STRING"}}}},"comp-large-language-model-tuner":{"executorLabel":"exec-large-language-model-tuner","inputDefinitions":{"parameters":{"location":{"description":"Region in which the launcher for the tuning job is created (not\nthe region for running the whole pipeline).","parameterType":"STRING"},"payload_first_part":{"description":"The first part of the payload to the Custom Job.","parameterType":"STRING"},"payload_second_part":{"description":"The second part of the payload to the Custom Job.","parameterType":"STRING"},"project":{"description":"Name of the project.","parameterType":"STRING"}}},"outputDefinitions":{"artifacts":{"saved_model":{"artifactType":{"schemaTitle":"system.Artifact","schemaVersion":"0.0.1"},"description":"Artifact for adapter in Private GCS."},"tensorboard_metrics":{"artifactType":{"schemaTitle":"system.Artifact","schemaVersion":"0.0.1"},"description":"Artifact for saving TensorBoard metrics."}},"parameters":{"gcp_resources":{"description":"Serialized gcp_resources proto.","parameterType":"STRING"}}}},"comp-preprocess-dataset":{"executorLabel":"exec-preprocess-dataset","inputDefinitions":{"parameters":{"dataset_uri":{"description":"File or file pattern containing training data.","parameterType":"STRING"},"default_context":{"defaultValue":"","description":"If chat model specified, default_context will be applied\nto each training example.","isOptional":true,"parameterType":"STRING"},"model":{"defaultValue":"text-bison@001","description":"Model type to use.","isOptional":true,"parameterType":"STRING"}}},"outputDefinitions":{"parameters":{"output_dir":{"description":"GCS directory where output file(s) will be stored.","parameterType":"LIST"},"output_file":{"parameterType":"STRING"}}}},"comp-preprocess-dataset-2":{"executorLabel":"exec-preprocess-dataset-2","inputDefinitions":{"parameters":{"dataset_uri":{"description":"File or file pattern containing training data.","parameterType":"STRING"},"default_context":{"defaultValue":"","description":"If chat model specified, default_context will be applied\nto each training example.","isOptional":true,"parameterType":"STRING"},"model":{"defaultValue":"text-bison@001","description":"Model type to use.","isOptional":true,"parameterType":"STRING"}}},"outputDefinitions":{"parameters":{"output_dir":{"description":"GCS directory where output file(s) will be stored.","parameterType":"LIST"},"output_file":{"parameterType":"STRING"}}}},"comp-tensorboard-uploader":{"executorLabel":"exec-tensorboard-uploader","inputDefinitions":{"artifacts":{"metrics_directory_uri":{"artifactType":{"schemaTitle":"system.Artifact","schemaVersion":"0.0.1"}}},"parameters":{"experiment_name":{"parameterType":"STRING"},"location":{"parameterType":"STRING"},"project":{"parameterType":"STRING"},"tensorboard_id":{"parameterType":"STRING"}}},"outputDefinitions":{"parameters":{"tensorboard_uri":{"parameterType":"STRING"}}}},"comp-tuning-graph":{"dag":{"outputs":{"artifacts":{"saved_model":{"artifactSelectors":[{"outputArtifactKey":"saved_model","producerSubtask":"large-language-model-tuner"}]},"tensorboard_metrics":{"artifactSelectors":[{"outputArtifactKey":"tensorboard_metrics","producerSubtask":"large-language-model-tuner"}]}}},"tasks":{"compose-params-for-model-tuner":{"cachingOptions":{"enableCache":true},"componentRef":{"name":"comp-compose-params-for-model-tuner"},"dependentTasks":["convert-jsonl-to-tfrecord","convert-jsonl-to-tfrecord-2"],"inputs":{"parameters":{"display_name":{"runtimeValue":{"constant":"large-language-model-tuner-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}"}},"enable_checkpoint_selection":{"componentInputParameter":"enable_checkpoint_selection"},"enable_early_stopping":{"componentInputParameter":"enable_early_stopping"},"encryption_spec_key_name":{"componentInputParameter":"encryption_spec_key_name"},"evaluation_data_uri":{"taskOutputParameter":{"outputParameterKey":"output_file","producerTask":"convert-jsonl-to-tfrecord-2"}},"evaluation_interval":{"componentInputParameter":"evaluation_interval"},"learning_rate":{"componentInputParameter":"learning_rate"},"learning_rate_multiplier":{"componentInputParameter":"learning_rate_multiplier"},"location":{"componentInputParameter":"adapter_training_location"},"model":{"componentInputParameter":"model"},"tpu_training_skip_cmek":{"componentInputParameter":"tpu_training_skip_cmek"},"train_data_uri":{"taskOutputParameter":{"outputParameterKey":"output_file","producerTask":"convert-jsonl-to-tfrecord"}},"train_steps":{"componentInputParameter":"train_steps"},"tuning_method":{"componentInputParameter":"tuning_method"},"use_tpu":{"componentInputParameter":"use_tpu"}}},"taskInfo":{"name":"compose-params-for-model-tuner"}},"convert-jsonl-to-tfrecord":{"cachingOptions":{"enableCache":true},"componentRef":{"name":"comp-convert-jsonl-to-tfrecord"},"dependentTasks":["preprocess-dataset","vertex-pipelines-prompt-validation"],"inputs":{"parameters":{"dataset_uri":{"taskOutputParameter":{"outputParameterKey":"output_file","producerTask":"preprocess-dataset"}}}},"taskInfo":{"name":"convert-jsonl-to-tfrecord"}},"convert-jsonl-to-tfrecord-2":{"cachingOptions":{"enableCache":true},"componentRef":{"name":"comp-convert-jsonl-to-tfrecord-2"},"dependentTasks":["preprocess-dataset-2"],"inputs":{"parameters":{"dataset_uri":{"taskOutputParameter":{"outputParameterKey":"output_file","producerTask":"preprocess-dataset-2"}},"max_examples":{"runtimeValue":{"constant":256}}}},"taskInfo":{"name":"convert-eval-jsonl-to-tfrecord"}},"export-managed-dataset":{"cachingOptions":{"enableCache":true},"componentRef":{"name":"comp-export-managed-dataset"},"inputs":{"parameters":{"dataset_name":{"componentInputParameter":"dataset_name"},"dataset_uri":{"componentInputParameter":"dataset_uri"},"regional_endpoint":{"componentInputParameter":"regional_endpoint"}}},"taskInfo":{"name":"export-managed-dataset"}},"large-language-model-tuner":{"cachingOptions":{"enableCache":true},"componentRef":{"name":"comp-large-language-model-tuner"},"dependentTasks":["compose-params-for-model-tuner"],"inputs":{"parameters":{"location":{"componentInputParameter":"adapter_training_location"},"payload_first_part":{"taskOutputParameter":{"outputParameterKey":"payload_first_part","producerTask":"compose-params-for-model-tuner"}},"payload_second_part":{"taskOutputParameter":{"outputParameterKey":"payload_second_part","producerTask":"compose-params-for-model-tuner"}},"project":{"componentInputParameter":"project"}}},"taskInfo":{"name":"large-language-model-tuner"}},"preprocess-dataset":{"cachingOptions":{"enableCache":true},"componentRef":{"name":"comp-preprocess-dataset"},"dependentTasks":["export-managed-dataset"],"inputs":{"parameters":{"dataset_uri":{"taskOutputParameter":{"outputParameterKey":"export_location","producerTask":"export-managed-dataset"}},"default_context":{"componentInputParameter":"default_context"},"model":{"componentInputParameter":"model"}}},"taskInfo":{"name":"preprocess-dataset"}},"preprocess-dataset-2":{"cachingOptions":{"enableCache":true},"componentRef":{"name":"comp-preprocess-dataset-2"},"inputs":{"parameters":{"dataset_uri":{"componentInputParameter":"evaluation_data_uri"},"default_context":{"componentInputParameter":"default_context"},"model":{"componentInputParameter":"model"}}},"taskInfo":{"name":"preprocess-eval-dataset"}},"vertex-pipelines-prompt-validation":{"cachingOptions":{"enableCache":true},"componentRef":{"name":"comp-vertex-pipelines-prompt-validation"},"dependentTasks":["preprocess-dataset"],"inputs":{"parameters":{"fail_on_warning":{"runtimeValue":{"constant":false}},"model_id":{"componentInputParameter":"model"},"prompt_dataset":{"taskOutputParameter":{"outputParameterKey":"output_file","producerTask":"preprocess-dataset"}},"rai_validation_enabled":{"runtimeValue":{"constant":true}}}},"taskInfo":{"name":"vertex-pipelines-prompt-validation"}}}},"inputDefinitions":{"parameters":{"adapter_training_location":{"defaultValue":"europe-west4","description":"Location adapter training.","isOptional":true,"parameterType":"STRING"},"dataset_name":{"defaultValue":"","description":"Name of training dataset if importing from managed dataset.\nOnly this or `dataset_uri` should be set.","isOptional":true,"parameterType":"STRING"},"dataset_uri":{"defaultValue":"","description":"GCS URI of training dataset. Only this or `dataset_name`\nshould be set.","isOptional":true,"parameterType":"STRING"},"default_context":{"defaultValue":"","description":"If a chat model is specified, `default_context` will be\napplied to each training example.","isOptional":true,"parameterType":"STRING"},"enable_checkpoint_selection":{"defaultValue":"default","description":"One of 'true', 'false', or 'default'. If set\nto 'true' and if evaluation_data_uri is set, select the best tuned\ncheckpoint. If 'default' is set, checkpoint selection is enabled\nfor bison@001 models, and disabled for others.","isOptional":true,"parameterType":"STRING"},"enable_early_stopping":{"defaultValue":true,"description":"If set to True, and if evaluation_data_uri is set,\nthen early stopping will be enabled. If early stopping is enabled, the\nmax recommended train_steps is 1,000.","isOptional":true,"parameterType":"BOOLEAN"},"encryption_spec_key_name":{"defaultValue":"","description":"Customer-managed encryption key.","isOptional":true,"parameterType":"STRING"},"evaluation_data_uri":{"defaultValue":"","description":"GCS URI of eval dataset.","isOptional":true,"parameterType":"STRING"},"evaluation_interval":{"defaultValue":20,"description":"Evaluation after every this number of tuning steps.","isOptional":true,"parameterType":"NUMBER_INTEGER"},"learning_rate":{"defaultValue":-1,"description":"Learning rate hyperparameter for tuning. If this value is\nnegative, then the default learning rate per model per tuning_method\nwill be used. This parameter will be deprecated.","isOptional":true,"parameterType":"NUMBER_DOUBLE"},"learning_rate_multiplier":{"defaultValue":1,"description":"Learning rate multiplier for tuning. Under the\nhood, the actual \"learning rate\" will be learning_rate_multiplier *\nthe \"recommended learning rate\" per model, so customers don't have to\nkeep track of the recommended learning rate for each model or tuning\nmethod.","isOptional":true,"parameterType":"NUMBER_DOUBLE"},"model":{"defaultValue":"text-bison@001","description":"Model name indicating which model to tuning.","isOptional":true,"parameterType":"STRING"},"project":{"description":"Name of the GCP project.","parameterType":"STRING"},"regional_endpoint":{"defaultValue":"https://us-central1-aiplatform.googleapis.com/ui","description":"Regional API endpoint.","isOptional":true,"parameterType":"STRING"},"tpu_training_skip_cmek":{"defaultValue":false,"description":"If set to True, CMEK will be ignored during TPU\ntraining step.","isOptional":true,"parameterType":"BOOLEAN"},"train_steps":{"defaultValue":300,"description":"Number of training steps for tuning step.","isOptional":true,"parameterType":"NUMBER_INTEGER"},"tuning_method":{"defaultValue":"tune_v2","description":"The adapter tuning method. Available methods: tune,\ntune_v2.","isOptional":true,"parameterType":"STRING"},"use_tpu":{"defaultValue":true,"description":"If set, adapter tuning will be running on TPU.","isOptional":true,"parameterType":"BOOLEAN"}}},"outputDefinitions":{"artifacts":{"saved_model":{"artifactType":{"schemaTitle":"system.Artifact","schemaVersion":"0.0.1"}},"tensorboard_metrics":{"artifactType":{"schemaTitle":"system.Artifact","schemaVersion":"0.0.1"}}}}},"comp-upload-llm-model":{"executorLabel":"exec-upload-llm-model","inputDefinitions":{"artifacts":{"artifact_uri":{"artifactType":{"schemaTitle":"system.Artifact","schemaVersion":"0.0.1"},"description":"KFP Artifact for adapter."}},"parameters":{"encryption_spec_key_name":{"defaultValue":"","description":"Customer-managed encryption key.","isOptional":true,"parameterType":"STRING"},"location":{"description":"Location for model upload and deployment.","parameterType":"STRING"},"model_display_name":{"description":"Name of the model (shown in Model Registry).","parameterType":"STRING"},"model_reference_name":{"description":"Large model reference name.","parameterType":"STRING"},"project":{"description":"Name of the GCP project.","parameterType":"STRING"},"regional_endpoint":{"description":"Regional API endpoint.","parameterType":"STRING"},"upload_model":{"defaultValue":true,"description":"Whether to upload the model to the Model Registry. Default\nis ``True``. If ``False``, the model will not be uploaded and output\nartifacts will contain empty strings.","isOptional":true,"parameterType":"BOOLEAN"}}},"outputDefinitions":{"parameters":{"gcp_resources":{"description":"Serialized JSON of `gcp_resources`.","parameterType":"STRING"},"model_resource_name":{"description":"Path to the created Model on Model Registry.","parameterType":"STRING"}}}},"comp-validate-pipeline":{"executorLabel":"exec-validate-pipeline","inputDefinitions":{"parameters":{"accelerator_type":{"defaultValue":"","description":"One of 'TPU' or 'GPU'. If 'TPU' is specified, tuning\nruns in europe-west4, else in us-central1.","isOptional":true,"parameterType":"STRING"},"api_endpoint":{"defaultValue":"aiplatform.googleapis.com/ui","description":"API endpoint.","isOptional":true,"parameterType":"STRING"},"encryption_spec_key_name":{"defaultValue":"","description":"If set, CMEK will be used.","isOptional":true,"parameterType":"STRING"},"evaluation_data_uri":{"defaultValue":"","description":"GCS URI of eval dataset. If evaluation dataset is\nprovided, the best tuned checkpoint will be selected.","isOptional":true,"parameterType":"STRING"},"evaluation_output_root_dir":{"defaultValue":"","description":"Root GCS URI of evaluation outputs.","isOptional":true,"parameterType":"STRING"},"location":{"defaultValue":"{{$.pipeline_google_cloud_location}}","description":"Region in which all the components except for tuning job should\nrun.","isOptional":true,"parameterType":"STRING"},"model":{"defaultValue":"text-bison-001","description":"Large language reference model to use (default: `text-bison-001`).","isOptional":true,"parameterType":"STRING"},"pipeline_region":{"defaultValue":"{{$.pipeline_google_cloud_location}}","description":"The region the pipeline runs in.","isOptional":true,"parameterType":"STRING"},"tensorboard_resource_id":{"defaultValue":"","description":"The Vertex AI TensorBoard Resource/Instance ID.\nIf this parameter is set, then a Vertex AI TensorBoard Experiment will\nbe launched.","isOptional":true,"parameterType":"STRING"},"tpu_training_skip_cmek":{"defaultValue":false,"description":"If True, skip CMEK setup for TPU training.","isOptional":true,"parameterType":"BOOLEAN"},"tuning_method":{"defaultValue":"tune","description":"Tuning method.","isOptional":true,"parameterType":"STRING"}}},"outputDefinitions":{"parameters":{"adapter_training_location":{"parameterType":"STRING"},"model_at_version":{"parameterType":"STRING"},"model_dash_version":{"parameterType":"STRING"},"model_location":{"parameterType":"STRING"},"regional_endpoint":{"parameterType":"STRING"},"service_account":{"parameterType":"STRING"},"use_tpu":{"parameterType":"BOOLEAN"},"with_batch_prediction_and_evaluation":{"parameterType":"BOOLEAN"},"with_tensorboard":{"parameterType":"BOOLEAN"}}}},"comp-vertex-pipelines-prompt-validation":{"executorLabel":"exec-vertex-pipelines-prompt-validation","inputDefinitions":{"parameters":{"fail_on_warning":{"description":"If enabled, fails the component on warnings.","parameterType":"BOOLEAN"},"model_id":{"description":"Large Language Model to tune.","parameterType":"STRING"},"prompt_dataset":{"description":"GCS path to the file containing the prompt tuning data.","parameterType":"STRING"},"rai_validation_enabled":{"description":"If enabled, validates prompt data for harmful content.","parameterType":"BOOLEAN"}}}}},"deploymentSpec":{"executors":{"exec-compose-params-for-model-tuner":{"container":{"args":["--executor_input","{{$}}","--function_to_execute","compose_params_for_model_tuner"],"command":["sh","-ec","program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n","\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef compose_params_for_model_tuner(\n    display_name: str,\n    train_data_uri: str,\n    tuning_method: str = 'tune_v2',\n    location: str = 'us-central1',\n    model: str = 'text-bison@001',\n    train_steps: int = 0,\n    adapter_length: int = 0,\n    learning_rate: float = -1,\n    learning_rate_multiplier: float = 1.0,\n    evaluation_data_uri: str = '',\n    evaluation_interval: int = 20,\n    enable_early_stopping: bool = True,\n    use_tpu: bool = True,\n    encryption_spec_key_name: str = '',\n    tpu_training_skip_cmek: bool = False,\n    max_train_inputs_length: int = -1,\n    max_train_outputs_length: int = -1,\n    max_eval_inputs_length: int = -1,\n    max_eval_outputs_length: int = -1,\n    pipeline_region: str = '{{$.pipeline_google_cloud_location}}',\n    enable_checkpoint_selection: str = 'default',\n) -> NamedTuple('outputs',\n                payload_first_part=str,\n                payload_second_part=str,\n                location=str):\n  \"\"\"Composes the parameters for running model tuner.\n\n  Args:\n      display_name: The display name of the model tuner.\n      tuning_method: The model tuning method. Available methods: tune,\n          tune_v2.\n      train_data_uri: File or file pattern containing train data.\n      location: Region in which the launcher for the tuning job is created (not\n          the region for running the whole pipeline).\n      model: Model type to use.\n      train_steps: Number of training steps.\n      adapter_length: Adapter length.\n      learning_rate: Learning rate hyperparameter. When learning rate is less\n          than 0, it uses the default learning rate per tuning method per model.\n          This parameter will be deprecated.\n      learning_rate_multiplier: Learning rate multiplier for tuning. Under the\n          hood, the actual \"learning rate\" will be learning_rate_multiplier *\n          the \"recommended learning rate\" per model, so customers don't have to\n          keep track of the recommended learning rate for each model or tuning\n          method.\n      evaluation_data_uri: GCS URI of eval dataset.\n      evaluation_interval: Evaluation after every this number of tuning steps.\n      enable_early_stopping: If set to True, and if evaluation_data_uri is set,\n          then early stopping will be enabled. If early stopping is enabled, the\n          max recommended train_steps is 1,000.\n      use_tpu: If True, use TPU. Otherwise use GPU.\n      encryption_spec_key_name: Customer-managed encryption key. If this is set,\n          then all resources created by the CustomJob will be encrypted with the\n          provided encryption key. Note that this is not supported for TPU at\n          the moment.\n      tpu_training_skip_cmek: If True, skip CMEK setup for TPU training.\n      max_train_inputs_length: Max inputs length from train data. If the max of\n          max_train_inputs_length and max_eval_inputs_length is less than 0 or\n          more than the default inputs length, which represents the max inputs\n          length supported, use the default inputs length.\n      max_train_outputs_length: Max outputs length from train data. If the max\n          of max_train_outputs_length and max_eval_outputs_length is less than 0\n          or more than the default outputs length, which represents the max\n          outputs length supported, use the default outputs length.\n      max_eval_inputs_length: Max inputs length from eval data. If the max of\n          max_train_inputs_length and max_eval_inputs_length is less than 0 or\n          more than the default inputs length, which represents the max inputs\n          length supported, use the default inputs length.\n      max_eval_outputs_length: Max outputs length from eval data. If the max\n          of max_train_outputs_length and max_eval_outputs_length is less than 0\n          or more than the default outputs length, which represents the max\n          outputs length supported, use the default outputs length.\n      pipeline_region: The region the pipeline runs in.\n      enable_checkpoint_selection: One of 'true', 'false', or 'default'. If set\n          to 'true' and if evaluation_data_uri is set, select the best tuned\n          checkpoint. If 'default' is set, checkpoint selection is enabled\n          for bison@001 models, and disabled for others.\n\n  Returns:\n      outputs: Structured parameters for creating a Custom Job to run the\n          adapter training job.\n  \"\"\"\n  import json\n  import logging\n  from typing import Any\n  import dataclasses\n  import sys\n\n  latest_models = frozenset(['text-bison', 'chat-bison', 'code-bison',\n                             'codechat-bison'])\n\n  @dataclasses.dataclass(frozen=True, eq=True)\n  class TunedModel:\n    model: str\n    accelerator: str\n    tuning_method: str = 'tune_v2'\n\n  @dataclasses.dataclass(frozen=True)\n  class TuneParams:\n    train_steps: int = 300\n    learning_rate: float = -1\n    adapter_length: int = 1\n    inputs_length: int = 8192\n    outputs_length: int = 1024\n    per_core_batch_size: Any = None\n    ici_mesh_shape: Any = None\n    enable_checkpoint_selection: bool = False\n\n  class GpuTuneParams(TuneParams):\n\n    def __init__(self, per_core_batch_size=None, ici_mesh_shape=None, **kwargs):\n      super().__init__(\n          per_core_batch_size=per_core_batch_size or 1,\n          ici_mesh_shape=ici_mesh_shape or [1, 1, 8],\n          **kwargs,\n      )\n\n  class TpuTuneParams(TuneParams):\n\n    def __init__(self, per_core_batch_size=None, ici_mesh_shape=None, **kwargs):\n      super().__init__(\n          per_core_batch_size=per_core_batch_size or 0.375,\n          ici_mesh_shape=ici_mesh_shape or [1, 4, 16],\n          **kwargs,\n      )\n  legacy_tune_params_tpu = TpuTuneParams(\n      train_steps=1000,\n      adapter_length=20,\n      inputs_length=4096,\n      outputs_length=1024,\n      per_core_batch_size=0.125,\n      ici_mesh_shape=[1, 64, 1],\n      learning_rate=3.0,\n  )\n  legacy_tune_params_gpu = GpuTuneParams(\n      train_steps=1000,\n      adapter_length=20,\n      inputs_length=2048,\n      outputs_length=512,\n      learning_rate=3.0,\n  )\n  default_per_model_params = {\n      TunedModel(\n          tuning_method='tune',\n          model='text-bison-001',\n          accelerator='tpu',\n      ): legacy_tune_params_tpu,\n      TunedModel(\n          tuning_method='tune',\n          model='text-bison-001',\n          accelerator='gpu',\n      ): legacy_tune_params_gpu,\n      TunedModel(\n          tuning_method='tune',\n          model='text-bison@001',\n          accelerator='tpu',\n      ): legacy_tune_params_tpu,\n      TunedModel(\n          tuning_method='tune',\n          model='text-bison@001',\n          accelerator='gpu',\n      ): legacy_tune_params_gpu,\n      TunedModel(\n          tuning_method='tune_v2',\n          model='text-bison@001',\n          accelerator='tpu',\n      ): TpuTuneParams(enable_checkpoint_selection=True),\n      TunedModel(\n          tuning_method='tune_v2',\n          model='text-bison@001',\n          accelerator='gpu',\n      ): GpuTuneParams(enable_checkpoint_selection=True),\n      TunedModel(\n          tuning_method='tune_v2',\n          model='code-bison@001',\n          accelerator='tpu',\n      ): TpuTuneParams(\n          inputs_length=6144,\n          outputs_length=2048,\n          enable_checkpoint_selection=True,\n      ),\n      TunedModel(\n          tuning_method='tune_v2',\n          model='code-bison@001',\n          accelerator='gpu',\n      ): GpuTuneParams(\n          inputs_length=6144,\n          outputs_length=2048,\n          enable_checkpoint_selection=True,\n      ),\n      TunedModel(\n          tuning_method='tune_v2',\n          model='chat-bison@001',\n          accelerator='tpu',\n      ): TpuTuneParams(enable_checkpoint_selection=True),\n      TunedModel(\n          tuning_method='tune_v2',\n          model='chat-bison@001',\n          accelerator='gpu',\n      ): GpuTuneParams(enable_checkpoint_selection=True),\n      TunedModel(\n          tuning_method='tune_v2',\n          model='codechat-bison@001',\n          accelerator='tpu',\n      ): TpuTuneParams(\n          inputs_length=6144,\n          outputs_length=2048,\n          enable_checkpoint_selection=True,\n      ),\n      TunedModel(\n          tuning_method='tune_v2',\n          model='codechat-bison@001',\n          accelerator='gpu',\n      ): GpuTuneParams(\n          inputs_length=6144,\n          outputs_length=2048,\n          enable_checkpoint_selection=True,\n      ),\n      TunedModel(\n          tuning_method='tune_v2',\n          model='text-bison',\n          accelerator='tpu',\n      ): TpuTuneParams(per_core_batch_size=0.125),\n      TunedModel(\n          tuning_method='tune_v2',\n          model='text-bison',\n          accelerator='gpu',\n      ): GpuTuneParams(per_core_batch_size=0.375),\n      TunedModel(\n          tuning_method='tune_v2',\n          model='chat-bison',\n          accelerator='tpu',\n      ): TpuTuneParams(per_core_batch_size=0.125),\n      TunedModel(\n          tuning_method='tune_v2',\n          model='chat-bison',\n          accelerator='gpu',\n      ): GpuTuneParams(per_core_batch_size=0.375),\n      TunedModel(\n          tuning_method='tune_v2',\n          model='code-bison',\n          accelerator='tpu',\n      ): TpuTuneParams(per_core_batch_size=0.125, inputs_length=6144,\n                       outputs_length=2048),\n      TunedModel(\n          tuning_method='tune_v2',\n          model='code-bison',\n          accelerator='gpu',\n      ): GpuTuneParams(per_core_batch_size=0.375, inputs_length=6144,\n                       outputs_length=2048),\n      TunedModel(\n          tuning_method='tune_v2',\n          model='codechat-bison',\n          accelerator='tpu',\n      ): TpuTuneParams(per_core_batch_size=0.125, inputs_length=6144,\n                       outputs_length=2048),\n      TunedModel(\n          tuning_method='tune_v2',\n          model='codechat-bison',\n          accelerator='gpu',\n      ): GpuTuneParams(per_core_batch_size=0.375, inputs_length=6144,\n                       outputs_length=2048),\n  }\n\n  try:\n    if tuning_method not in ['tune', 'tune_v2']:\n      raise ValueError(f'Unsupported tuning method [{tuning_method}].')\n\n    default_params = default_per_model_params.get(\n        TunedModel(\n            tuning_method=tuning_method,\n            model=model,\n            accelerator='tpu' if use_tpu else 'gpu',\n        ),\n        TpuTuneParams() if use_tpu else GpuTuneParams(),\n    )\n    max_inputs_length = max(max_train_inputs_length, max_eval_inputs_length)\n    inputs_length = (\n        min(max_inputs_length, default_params.inputs_length)\n        if max_inputs_length >= 0 else default_params.inputs_length\n    )\n    max_outputs_length = max(max_train_outputs_length, max_eval_outputs_length)\n    outputs_length = (\n        min(max_outputs_length, default_params.outputs_length)\n        if max_outputs_length >= 0 else default_params.outputs_length\n    )\n\n    executor_line = '--executor_input={{<PLACE_HOLDER>}}'\n\n    image_uri_prefix = 'us-docker.pkg.dev/vertex-ai-restricted/llm/llm-pets'\n    args = [\n        tuning_method,\n        f'--model={model}',\n        '--task=generation',\n        f'--train_data_path={train_data_uri}',\n        '--feature_keys=input_text',\n        '--label_key=output_text',\n        (\n            f\"\"\"--train_steps={train_steps if train_steps > 0\n                              else default_params.train_steps}\"\"\"\n        ),\n        f'--inputs_length={inputs_length}',\n        f'--outputs_length={outputs_length}',\n        f'--ici_mesh_shape={default_params.ici_mesh_shape}',\n        f'--per_core_batch_size={default_params.per_core_batch_size}',\n        (\n            f\"\"\"--adapter_length={adapter_length if adapter_length > 0\n                                  else default_params.adapter_length}\"\"\"\n        ),\n    ]\n\n    if 'tune' == tuning_method:\n      if learning_rate >= 0:\n        args.append(f'--learning_rate={learning_rate * learning_rate_multiplier}')\n      elif learning_rate_multiplier != 1.0:\n        args.append(f\"\"\"--learning_rate={\n                    default_params.learning_rate * learning_rate_multiplier}\"\"\")\n    elif 'tune_v2' == tuning_method:\n      args.append(f'--train_evaluation_interval={evaluation_interval}')\n      if evaluation_data_uri:\n        args.append(f'--validation_data_path={evaluation_data_uri}')\n        if enable_early_stopping:\n          args.append(\n              '--early_stopping=metric_name='\n              '/eval_fraction_of_correct_next_step_preds,mode=max,patience=8,'\n              'relative_tolerance=0.001'\n          )\n        checkpoint_selector = '{}'\n        if enable_checkpoint_selection.lower() == 'true' or (\n            enable_checkpoint_selection.lower() == 'default'\n            and default_params.enable_checkpoint_selection\n        ):\n          checkpoint_selector = json.dumps(\n              {\n                  'checkpoint_selector_cls': 'best_single_metric',\n                  'metric_name': '/eval_fraction_of_correct_next_step_preds',\n                  'metric_mode': 'max',\n              },\n              sort_keys=True,\n          )\n        args.append(f'--checkpoint_selector={checkpoint_selector}')\n      if learning_rate >= 0:\n        args.append(f'--learning_rate={learning_rate}')\n      if learning_rate_multiplier >= 0:\n        args.append(f'--learning_rate_multiplier={learning_rate_multiplier}')\n\n    args.append(executor_line)\n\n    if use_tpu:\n      machine_spec = {\n          'machine_type': 'cloud-tpu',\n          'accelerator_type': 'TPU_V3',\n          'accelerator_count': 64,\n      }\n      disk_spec = {'boot_disk_type': 'pd-ssd', 'boot_disk_size_gb': 100}\n      if 'tune' == tuning_method:\n        image_uri = f'{image_uri_prefix}-tpu:v1.0.0'\n      else:  # Default to tune_v2\n        image_uri = f'{image_uri_prefix}-tpu:v2.2.0'\n        if model in latest_models:\n          image_uri = f'{image_uri_prefix}-tpu-experiment:v1.0.0'\n    else:\n      machine_spec = {\n          'machine_type': 'a2-ultragpu-8g',\n          'accelerator_type': 'NVIDIA_A100_80GB',\n          'accelerator_count': 8,\n      }\n      disk_spec = {'boot_disk_type': 'pd-ssd', 'boot_disk_size_gb': 500}\n      if 'tune' == tuning_method:\n        image_uri = f'{image_uri_prefix}-gpu:v1.0.0'\n      else:  # Default to tune_v2\n        image_uri = f'{image_uri_prefix}-gpu:v2.2.0'\n        if model in latest_models:\n          image_uri = f'{image_uri_prefix}-gpu-experiment:v1.0.0'\n\n    custom_job_payload = {\n        'display_name': display_name,\n        'job_spec': {\n            'worker_pool_specs': [{\n                'replica_count': 1,\n                'machine_spec': machine_spec,\n                'disk_spec': disk_spec,\n                'container_spec': {'image_uri': image_uri, 'args': args},\n            }]\n        },\n    }\n\n    if tuning_method == 'tune_v2':\n      custom_job_payload['job_spec'][\n          'protected_artifact_location_id'\n      ] = pipeline_region\n\n    # Set up CMEK\n    if encryption_spec_key_name:\n      if use_tpu:\n        if tpu_training_skip_cmek:\n          logging.warning(\n              'CMEK will not be set for TPU training. If this is not expected, '\n              'please run with GPU.'\n          )\n        else:\n          raise ValueError(\n              'encryption_spec_key_name (CMEK) is not supported for TPU '\n              'training.'\n          )\n      else:\n        custom_job_payload['encryption_spec'] = {\n            'kms_key_name': encryption_spec_key_name\n        }\n\n    custom_job_payload_str = json.dumps(custom_job_payload)\n    custom_job_payload_list = custom_job_payload_str.rsplit(\n        f'\"{executor_line}\"', maxsplit=1\n    )\n    outputs = NamedTuple(\n        'outputs', payload_first_part=str, payload_second_part=str, location=str\n    )\n    return outputs(\n        payload_first_part=custom_job_payload_list[0],\n        payload_second_part=custom_job_payload_list[1],\n        location=location,\n    )\n  except Exception as e:  # pylint: disable=broad-exception-caught\n    if isinstance(e, ValueError):\n      raise\n    logging.exception(str(e))\n    sys.exit(13)\n\n"],"image":"gcr.io/ml-pipeline/google-cloud-pipeline-components:2.3.1"}},"exec-convert-jsonl-to-tfrecord":{"container":{"args":["--dataset-uri","{{$.inputs.parameters['dataset_uri']}}","--output-dir","{{$.outputs.parameters['output_dir'].output_file}}","--min-examples","{{$.inputs.parameters['min_examples']}}","--max-examples","{{$.inputs.parameters['max_examples']}}","----output-paths","{{$.outputs.parameters['output_file'].output_file}}"],"command":["sh","-ec","program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n","def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\ndef convert_jsonl_to_tfrecord(\n    dataset_uri ,\n    output_dir ,  # pytype: disable=invalid-annotation\n    min_examples ,\n    max_examples ,\n)    :\n  \"\"\"Converts incoming JSONL files into TFRecord.\n\n  Args:\n    dataset_uri: URI containing JSONL input data.\n    output_dir: GCS directory where output file(s) will be stored.\n    min_examples: The minimum number of examples required for training or evaluation.\n    max_examples: The maximum number of examples for training or evaluation dataset. If 0, no\n      upper limit.\n\n  Returns:\n    Namedtuple of the TFRecord URI under `output_file` key.\n  \"\"\"\n  # KFP component dependencies must be imported in function body.\n  import collections\n  import json\n  import logging\n  import os\n  import tensorflow as tf\n  import sys\n\n  # All code, including these constants, must be defined within the function in\n  # order to be included in the resulting component.\n  # pylint: disable=invalid-name\n  GCS_ROOT_PREFIX = '/gcs/'\n  GS_PREFIX = 'gs://'\n  JSONL_SUFFIX = '.jsonl'\n  TFRECORD_SUFFIX = '.tfrecord'\n  INPUT_TEXT_KEY = 'input_text'\n  OUTPUT_TEXT_KEY = 'output_text'\n  DATA_KEY = 'data'\n  # pylint: enable=invalid-name\n\n  def extract_jsonl_row(writer, json_instance_data):\n    input_feature = tf.train.Feature(\n        bytes_list=tf.train.BytesList(\n            value=[bytes(json_instance_data[INPUT_TEXT_KEY], 'utf-8')]\n        )\n    )\n    output_feature = tf.train.Feature(\n        bytes_list=tf.train.BytesList(\n            value=[bytes(json_instance_data[OUTPUT_TEXT_KEY], 'utf-8')]\n        )\n    )\n    example = tf.train.Example(\n        features=tf.train.Features(\n            feature={\n                'input_text': input_feature,\n                'output_text': output_feature,\n            }\n        )\n    )\n    writer.write(example.SerializeToString())\n\n  try:\n    output_tuple = collections.namedtuple('Outputs', ['output_file'])\n    if not dataset_uri:\n      return output_tuple('')\n\n    output_file_path = ''\n    with tf.io.gfile.GFile(dataset_uri) as jsonl_file:\n      output_file_path = os.path.join(output_dir, os.path.basename(dataset_uri))\n      if output_file_path.endswith(JSONL_SUFFIX):\n        output_file_path = (\n            output_file_path[: -len(JSONL_SUFFIX)] + TFRECORD_SUFFIX\n        )\n      # Extra makedirs call to ensure all dirs are present.\n      # Without this the leaf dir may not be created.\n      os.makedirs(output_dir, exist_ok=True)\n      num_entries = 0\n\n      # Check if file has one JSON training example per line, or has an object\n      # whose data field contains all the examples.\n      with tf.io.TFRecordWriter(output_file_path) as writer:\n        for line_number, dataset_line in enumerate(jsonl_file):\n          try:\n            json_instance = json.loads(dataset_line)\n          except json.JSONDecodeError as e:\n            raise ValueError(\n                f'Cannot decode training example at line {line_number}:'\n                f' {dataset_line}'\n            ) from e\n\n          keys = list(json_instance.keys())\n          if DATA_KEY in keys:\n            json_instance_data = json_instance[DATA_KEY]\n            for data_obj in json_instance_data:\n              extract_jsonl_row(writer, data_obj)\n              num_entries += 1\n          else:\n            extract_jsonl_row(writer, json_instance)\n            num_entries += 1\n\n      min_examples = max(1, min_examples)\n      if num_entries < min_examples:\n        raise ValueError(\n            f'Dataset is inaccessible or contains less than {min_examples} '\n            'entries. Please ensure your pipeline can access the dataset and '\n            f'that it is properly formatted. Dataset URI: {dataset_uri}'\n        )\n      if max_examples > 0 and num_entries > max_examples:\n        raise ValueError(\n            f'Dataset exceeded the max number of examples: {max_examples}')\n    # Subsequent components will not understand \"/gcs/\" path (generated by\n    # Vertex Pipelines). Convert to \"gs://\" prefix for compatibility.\n    if output_file_path.startswith(GCS_ROOT_PREFIX):\n      output_file_path = GS_PREFIX + output_file_path[len(GCS_ROOT_PREFIX) :]\n\n    return output_tuple(output_file_path)\n  except Exception as e:  # pylint: disable=broad-exception-caught\n    if isinstance(e, ValueError):\n      raise\n    logging.exception(str(e))\n    sys.exit(13)\n\ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n            str(str_value), str(type(str_value))))\n    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Convert jsonl to tfrecord', description='Converts incoming JSONL files into TFRecord.')\n_parser.add_argument(\"--dataset-uri\", dest=\"dataset_uri\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--min-examples\", dest=\"min_examples\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--max-examples\", dest=\"max_examples\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-dir\", dest=\"output_dir\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = convert_jsonl_to_tfrecord(**_parsed_args)\n\n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],"image":"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest"}},"exec-convert-jsonl-to-tfrecord-2":{"container":{"args":["--dataset-uri","{{$.inputs.parameters['dataset_uri']}}","--output-dir","{{$.outputs.parameters['output_dir'].output_file}}","--min-examples","{{$.inputs.parameters['min_examples']}}","--max-examples","{{$.inputs.parameters['max_examples']}}","----output-paths","{{$.outputs.parameters['output_file'].output_file}}"],"command":["sh","-ec","program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n","def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\ndef convert_jsonl_to_tfrecord(\n    dataset_uri ,\n    output_dir ,  # pytype: disable=invalid-annotation\n    min_examples ,\n    max_examples ,\n)    :\n  \"\"\"Converts incoming JSONL files into TFRecord.\n\n  Args:\n    dataset_uri: URI containing JSONL input data.\n    output_dir: GCS directory where output file(s) will be stored.\n    min_examples: The minimum number of examples required for training or evaluation.\n    max_examples: The maximum number of examples for training or evaluation dataset. If 0, no\n      upper limit.\n\n  Returns:\n    Namedtuple of the TFRecord URI under `output_file` key.\n  \"\"\"\n  # KFP component dependencies must be imported in function body.\n  import collections\n  import json\n  import logging\n  import os\n  import tensorflow as tf\n  import sys\n\n  # All code, including these constants, must be defined within the function in\n  # order to be included in the resulting component.\n  # pylint: disable=invalid-name\n  GCS_ROOT_PREFIX = '/gcs/'\n  GS_PREFIX = 'gs://'\n  JSONL_SUFFIX = '.jsonl'\n  TFRECORD_SUFFIX = '.tfrecord'\n  INPUT_TEXT_KEY = 'input_text'\n  OUTPUT_TEXT_KEY = 'output_text'\n  DATA_KEY = 'data'\n  # pylint: enable=invalid-name\n\n  def extract_jsonl_row(writer, json_instance_data):\n    input_feature = tf.train.Feature(\n        bytes_list=tf.train.BytesList(\n            value=[bytes(json_instance_data[INPUT_TEXT_KEY], 'utf-8')]\n        )\n    )\n    output_feature = tf.train.Feature(\n        bytes_list=tf.train.BytesList(\n            value=[bytes(json_instance_data[OUTPUT_TEXT_KEY], 'utf-8')]\n        )\n    )\n    example = tf.train.Example(\n        features=tf.train.Features(\n            feature={\n                'input_text': input_feature,\n                'output_text': output_feature,\n            }\n        )\n    )\n    writer.write(example.SerializeToString())\n\n  try:\n    output_tuple = collections.namedtuple('Outputs', ['output_file'])\n    if not dataset_uri:\n      return output_tuple('')\n\n    output_file_path = ''\n    with tf.io.gfile.GFile(dataset_uri) as jsonl_file:\n      output_file_path = os.path.join(output_dir, os.path.basename(dataset_uri))\n      if output_file_path.endswith(JSONL_SUFFIX):\n        output_file_path = (\n            output_file_path[: -len(JSONL_SUFFIX)] + TFRECORD_SUFFIX\n        )\n      # Extra makedirs call to ensure all dirs are present.\n      # Without this the leaf dir may not be created.\n      os.makedirs(output_dir, exist_ok=True)\n      num_entries = 0\n\n      # Check if file has one JSON training example per line, or has an object\n      # whose data field contains all the examples.\n      with tf.io.TFRecordWriter(output_file_path) as writer:\n        for line_number, dataset_line in enumerate(jsonl_file):\n          try:\n            json_instance = json.loads(dataset_line)\n          except json.JSONDecodeError as e:\n            raise ValueError(\n                f'Cannot decode training example at line {line_number}:'\n                f' {dataset_line}'\n            ) from e\n\n          keys = list(json_instance.keys())\n          if DATA_KEY in keys:\n            json_instance_data = json_instance[DATA_KEY]\n            for data_obj in json_instance_data:\n              extract_jsonl_row(writer, data_obj)\n              num_entries += 1\n          else:\n            extract_jsonl_row(writer, json_instance)\n            num_entries += 1\n\n      min_examples = max(1, min_examples)\n      if num_entries < min_examples:\n        raise ValueError(\n            f'Dataset is inaccessible or contains less than {min_examples} '\n            'entries. Please ensure your pipeline can access the dataset and '\n            f'that it is properly formatted. Dataset URI: {dataset_uri}'\n        )\n      if max_examples > 0 and num_entries > max_examples:\n        raise ValueError(\n            f'Dataset exceeded the max number of examples: {max_examples}')\n    # Subsequent components will not understand \"/gcs/\" path (generated by\n    # Vertex Pipelines). Convert to \"gs://\" prefix for compatibility.\n    if output_file_path.startswith(GCS_ROOT_PREFIX):\n      output_file_path = GS_PREFIX + output_file_path[len(GCS_ROOT_PREFIX) :]\n\n    return output_tuple(output_file_path)\n  except Exception as e:  # pylint: disable=broad-exception-caught\n    if isinstance(e, ValueError):\n      raise\n    logging.exception(str(e))\n    sys.exit(13)\n\ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n            str(str_value), str(type(str_value))))\n    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Convert jsonl to tfrecord', description='Converts incoming JSONL files into TFRecord.')\n_parser.add_argument(\"--dataset-uri\", dest=\"dataset_uri\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--min-examples\", dest=\"min_examples\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--max-examples\", dest=\"max_examples\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-dir\", dest=\"output_dir\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = convert_jsonl_to_tfrecord(**_parsed_args)\n\n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],"image":"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest"}},"exec-create-endpoint-and-deploy-model":{"container":{"args":["--executor_input","{{$}}","--function_to_execute","create_endpoint_and_deploy_model"],"command":["sh","-ec","program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n","\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef create_endpoint_and_deploy_model(\n    project: str,\n    location: str,\n    model_resource_name: str,\n    display_name: str,\n    regional_endpoint: str,\n    endpoint_resource_name: dsl.OutputPath(str),\n    create_endpoint_gcp_resources: dsl.OutputPath(str),\n    deploy_model_gcp_resources: dsl.OutputPath(str),\n    encryption_spec_key_name: str = '',\n    service_account: str = '',\n    deploy_model: bool = True,\n):\n  \"\"\"Creates a vertex endpoint and deploy the specified model.\n\n  Args:\n      project: Name of the GCP project.\n      location: Location for model upload and deployment.\n      model_resource_name: Path to the created Model on Model Registry.\n      display_name: Name of the model (shown in Model Registry).\n      regional_endpoint: Regional API endpoint.\n      encryption_spec_key_name: Customer-managed encryption key.\n      service_account: If set, then a custom service account will be used.\n      deploy_model: Whether to deploy the model to an endpoint. Default\n          is ``True``. If ``False``, the model will not be deployed and output\n          artifacts will contain empty strings.\n\n  Returns:\n      endpoint_resource_name: Path to the created endpoint on Online Prediction.\n      create_endpoint_gcp_resources: Serialized JSON of GCP resources for\n          creating an endpoint.\n      deploy_model_gcp_resources: Serialized JSON of GCP resources for deploying\n          the model.\n  \"\"\"\n  import json\n  import logging\n  import os\n  import sys\n  from typing import Any, Dict\n\n  try:\n    from google_cloud_pipeline_components.container.v1.gcp_launcher import lro_remote_runner\n  except ImportError:\n    from google_cloud_pipeline_components.google_cloud_pipeline_components.container.v1.gcp_launcher import lro_remote_runner\n\n  def run_lro_remote_runner(\n      url: str, payload: Dict[str, Any], gcp_resources: str) -> Any:\n    remote_runner = lro_remote_runner.LroRemoteRunner(location)\n    lro = remote_runner.create_lro(\n        url, json.dumps(payload), gcp_resources)\n    return remote_runner.poll_lro(lro=lro)\n\n  try:\n    os.makedirs(os.path.dirname(endpoint_resource_name), exist_ok=True)\n\n    if not deploy_model:\n      with open(endpoint_resource_name, 'w') as fout:\n        fout.write('')\n      return\n\n    regional_endpoint = regional_endpoint.rstrip('/')\n\n    create_endpoint_payload = {\n        'displayName': display_name,\n    }\n\n    pipeline_labels_str = os.getenv('VERTEX_AI_PIPELINES_RUN_LABELS')\n    if pipeline_labels_str:\n      create_endpoint_payload['labels'] = json.loads(pipeline_labels_str)\n\n    if encryption_spec_key_name:\n      create_endpoint_payload['encryption_spec'] = {\n          'kms_key_name': encryption_spec_key_name\n      }\n\n    create_endpoint_lro = run_lro_remote_runner(\n        url=(f'{regional_endpoint}/projects/{project}/locations/{location}'\n             '/endpoints'),\n        payload=create_endpoint_payload,\n        gcp_resources=create_endpoint_gcp_resources)\n\n    response_endpoint = create_endpoint_lro['response']['name']\n    with open(endpoint_resource_name, 'w') as fout:\n      fout.write(response_endpoint)\n\n    logging.info(\n        'Endpoint created successfully. Deploying model %s to endpoint',\n        model_resource_name)\n\n    deploy_model_payload = {\n        'deployedModel': {\n            'model': model_resource_name,\n            'displayName': display_name,\n            'automaticResources': {\n                'minReplicaCount': 1,\n                'maxReplicaCount': 1\n            }\n        }\n    }\n    if service_account:\n      deploy_model_payload['deployedModel']['service_account'] = service_account\n\n    _ = run_lro_remote_runner(\n        url=f'{regional_endpoint}/{response_endpoint}:deployModel',\n        payload=deploy_model_payload,\n        gcp_resources=deploy_model_gcp_resources)\n\n    logging.info('Model deployed successfully!')\n  except Exception as e:  # pylint: disable=broad-exception-caught\n    if isinstance(e, ValueError):\n      raise\n    logging.exception(str(e))\n    sys.exit(13)\n\n"],"image":"gcr.io/ml-pipeline/google-cloud-pipeline-components:2.3.1"}},"exec-export-managed-dataset":{"container":{"command":["sh","-c","set -e -x\nDATASET_NAME=\"$0\"\nDATASET_URI=\"$1\"\nREGIONAL_ENDPOINT=\"$2\"\nEXPORT_LOCATION=\"$3\"\n\nif [ -z \"${DATASET_URI}\" ] && [ -z \"${DATASET_NAME}\" ]\nthen\n  echo \"One of dataset_uri and dataset_name must be specified.\"\n  exit 1\nfi\n\nif [ -n \"${DATASET_URI}\" ] && [ -n \"${DATASET_NAME}\" ]\nthen\n  echo \"Only one of dataset_uri and dataset_name may be set.\"\n  exit 1\nfi\n\n# Construct export output dir; the file containing the dataset location is also located here.\nEXPORT_DIR=\"$(dirname \"$EXPORT_LOCATION\")\"\nmkdir -p $EXPORT_DIR\nEXPORT_DIR_GCS=$(echo $EXPORT_DIR | sed 's/^\\/gcs\\//gs:\\/\\//g')\n\nif [ -n \"${DATASET_URI}\" ]\nthen\n  echo \"Dataset export not required for an unmanaged file.\"\n  echo -n $DATASET_URI > \"$EXPORT_LOCATION\"\n  exit 0\nfi\n\nEXPORT_LRO=$(curl -X POST \\\n  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n  -H \"Content-Type: application/json\" \\\n  \"${REGIONAL_ENDPOINT}/${DATASET_NAME}:export\" \\\n  -d \"{\n    'exportConfig': {\n      'gcsDestination': {\n        'outputUriPrefix': '$EXPORT_DIR_GCS'\n      },\n      'exportUse': 'ADAPTER_TUNING'\n    }\n  }\")\nLRO_NAME=$(echo $EXPORT_LRO | python3 -c \"import sys, json; print(json.load(sys.stdin)['name'])\")\nPOLL_ENDPOINT=\"${REGIONAL_ENDPOINT}/${LRO_NAME}\"\n\nwhile true\ndo\n  POLL=$(curl -X GET -H \"Authorization: Bearer $(gcloud auth print-access-token)\" -H \"Content-Type: application/json\" $POLL_ENDPOINT)\n  echo $POLL\n  STATUS=$(echo $POLL | python3 -c \"import sys, json; print(json.load(sys.stdin)['metadata']['genericMetadata']['state'])\")\n  if [ \"$STATUS\" = \"DONE\" ]\n  then\n    ERROR=$(echo $POLL | python3 -c \"import sys, json; from collections import defaultdict; d = defaultdict(str); d.update(json.load(sys.stdin)); print(d['error'])\")\n    if [ ${#ERROR} -gt 0 ]\n    then\n      exit 13\n    fi\n    RESPONSE_EXPORTED_FILE=$(echo $POLL | python3 -c \"import sys, json; from collections import defaultdict; d = defaultdict(str); d.update(json.load(sys.stdin)); print(d['response']['exportedFiles'][0])\")\n    echo -n $RESPONSE_EXPORTED_FILE > \"$EXPORT_LOCATION\"\n    exit 0\n  fi\n  sleep 10\ndone\n","{{$.inputs.parameters['dataset_name']}}","{{$.inputs.parameters['dataset_uri']}}","{{$.inputs.parameters['regional_endpoint']}}","{{$.outputs.parameters['export_location'].output_file}}"],"image":"gcr.io/google.com/cloudsdktool/cloud-sdk:latest"}},"exec-large-language-model-tuner":{"container":{"args":["--type","CustomJob","--project","{{$.inputs.parameters['project']}}","--location","{{$.inputs.parameters['location']}}","--gcp_resources","{{$.outputs.parameters['gcp_resources'].output_file}}","--payload","{\"Concat\": [\"{{$.inputs.parameters['payload_first_part']}}\", \"\\\"--executor_input={{$.json_escape[1]}}\\\"\", \"{{$.inputs.parameters['payload_second_part']}}\"]}"],"command":["python3","-u","-m","google_cloud_pipeline_components.container.v1.custom_job.launcher"],"image":"gcr.io/ml-pipeline/google-cloud-pipeline-components:1.0.24"}},"exec-preprocess-dataset":{"container":{"args":["--executor_input","{{$}}","--function_to_execute","preprocess_dataset"],"command":["sh","-ec","program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n","\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef preprocess_dataset(\n    dataset_uri: str,\n    output_dir: dsl.OutputPath(list),\n    default_context: str = '',\n    model: str = 'text-bison@001',\n) -> NamedTuple('Outputs', [('output_file', str)]):\n  \"\"\"Preprocesses dataset according to model and dataset.\n\n  Args:\n      dataset_uri: File or file pattern containing training data.\n      default_context: If chat model specified, default_context will be applied\n        to each training example.\n      model: Model type to use.\n\n  Returns:\n      output_dir: GCS directory where output file(s) will be stored.\n      Outputs: Namedtuple of the processed URI under `output_file` key.\n  \"\"\"\n  # KFP component dependencies must be imported in function body.\n  import collections\n  import json\n  import logging\n  import os\n  import sys\n\n  # All code, including these constants, must be defined within the function in\n  # order to be included in the resulting component.\n  # pylint: disable=invalid-name\n  GCS_ROOT_PREFIX = '/gcs/'\n  GS_PREFIX = 'gs://'\n  INPUT_TEXT_KEY = 'input_text'\n  OUTPUT_TEXT_KEY = 'output_text'\n  CONTEXT_KEY = 'context'\n  MESSAGES_KEY = 'messages'\n  GLOBAL_PREFIX = 'Only answer after [assistant] and never reply as [user]:'\n  CONTEXT_PREFIX = '[SYSTEM]:'\n  AUTHOR_USER = 'user'\n  AUTHOR_ASSISTANT = 'assistant'\n  USER_PREFIX = '[user]:'\n  ASSISTANT_PREFIX = '[assistant]:'\n  AUTHOR_ENCODING_PREFIX_MAPPING = {\n      AUTHOR_USER: USER_PREFIX,\n      AUTHOR_ASSISTANT: ASSISTANT_PREFIX,\n  }\n\n  def _preprocess_one_conversation(context, messages, output_file):\n    \"\"\"Preprocesses one chat messages dataset example.\"\"\"\n    if not messages or len(messages) <= 1:\n      raise ValueError(\n          'Chat messages length should greater than 1. Please include a'\n          ' `messages` field in each line of dataset.'\n      )\n    for message in messages:\n      author = message.get('author')\n      content = message.get('content')\n      if not author or author not in [AUTHOR_USER, AUTHOR_ASSISTANT]:\n        raise ValueError(\n            f'The `author` of each message needs to be from `{AUTHOR_USER}` or'\n            f' `{AUTHOR_ASSISTANT}`.'\n        )\n      if not content:\n        raise ValueError('The `content` of each message needs to be non-empty.')\n\n    per_conversation_context = (\n        [CONTEXT_PREFIX + context + '\\n'] if context else []\n    )\n    message_history = []\n    for message in messages:\n      author = message.get('author')\n      content = message.get('content')\n      if author == AUTHOR_ASSISTANT:\n        encoding_list = (\n            [GLOBAL_PREFIX]\n            + per_conversation_context\n            + message_history\n            + [ASSISTANT_PREFIX]\n        )\n        input_text = '\\n'.join(encoding_list)\n        json.dump(\n            {INPUT_TEXT_KEY: input_text, OUTPUT_TEXT_KEY: content}, output_file\n        )\n        output_file.write('\\n')\n\n      message_history.append(\n          f'{AUTHOR_ENCODING_PREFIX_MAPPING[author]}{content}'\n      )\n\n  def _preprocess_chat_dataset(dataset_uri, default_context, output_file_path):\n    # Changes gs:// to /gcs/ since we use open() and GCS Fuse.\n    if dataset_uri.startswith(GS_PREFIX):\n      dataset_uri = dataset_uri.replace(GS_PREFIX, GCS_ROOT_PREFIX, 1)\n\n    with open(dataset_uri) as jsonl_file:\n      with open(output_file_path, 'w') as output_file:\n        for dataset_line in jsonl_file:\n          dataset_instance = json.loads(dataset_line)\n\n          # Check if per example context exists, if not, use default_context\n          context = dataset_instance.get(CONTEXT_KEY, default_context)\n          messages = dataset_instance.get(MESSAGES_KEY)\n\n          _preprocess_one_conversation(context, messages, output_file)\n\n  try:\n    output_tuple = collections.namedtuple('Outputs', ['output_file'])\n\n    # Extra makedirs call to ensure all dirs are present.\n    # Without this the leaf dir may not be created.\n    os.makedirs(output_dir, exist_ok=True)\n\n    if not dataset_uri:\n      return output_tuple('')\n\n    if default_context and model.startswith('text'):\n      raise ValueError('`default context` only applies to chat models.')\n\n    output_file_path = os.path.join(output_dir, os.path.basename(dataset_uri))\n\n    if 'chat' in model:\n      _preprocess_chat_dataset(dataset_uri, default_context, output_file_path)\n    else:\n      output_file_path = dataset_uri\n\n    # Subsequent components will not understand \"/gcs/\" path (generated by\n    # Vertex Pipelines). Convert to \"gs://\" prefix for compatibility.\n    if output_file_path.startswith(GCS_ROOT_PREFIX):\n      output_file_path = GS_PREFIX + output_file_path[len(GCS_ROOT_PREFIX) :]\n\n    return output_tuple(output_file_path)\n  except Exception as e:  # pylint: disable=broad-exception-caught\n    if isinstance(e, ValueError):\n      raise\n    logging.exception(str(e))\n    sys.exit(13)\n\n"],"image":"gcr.io/ml-pipeline/google-cloud-pipeline-components:2.3.1"}},"exec-preprocess-dataset-2":{"container":{"args":["--executor_input","{{$}}","--function_to_execute","preprocess_dataset"],"command":["sh","-ec","program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n","\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef preprocess_dataset(\n    dataset_uri: str,\n    output_dir: dsl.OutputPath(list),\n    default_context: str = '',\n    model: str = 'text-bison@001',\n) -> NamedTuple('Outputs', [('output_file', str)]):\n  \"\"\"Preprocesses dataset according to model and dataset.\n\n  Args:\n      dataset_uri: File or file pattern containing training data.\n      default_context: If chat model specified, default_context will be applied\n        to each training example.\n      model: Model type to use.\n\n  Returns:\n      output_dir: GCS directory where output file(s) will be stored.\n      Outputs: Namedtuple of the processed URI under `output_file` key.\n  \"\"\"\n  # KFP component dependencies must be imported in function body.\n  import collections\n  import json\n  import logging\n  import os\n  import sys\n\n  # All code, including these constants, must be defined within the function in\n  # order to be included in the resulting component.\n  # pylint: disable=invalid-name\n  GCS_ROOT_PREFIX = '/gcs/'\n  GS_PREFIX = 'gs://'\n  INPUT_TEXT_KEY = 'input_text'\n  OUTPUT_TEXT_KEY = 'output_text'\n  CONTEXT_KEY = 'context'\n  MESSAGES_KEY = 'messages'\n  GLOBAL_PREFIX = 'Only answer after [assistant] and never reply as [user]:'\n  CONTEXT_PREFIX = '[SYSTEM]:'\n  AUTHOR_USER = 'user'\n  AUTHOR_ASSISTANT = 'assistant'\n  USER_PREFIX = '[user]:'\n  ASSISTANT_PREFIX = '[assistant]:'\n  AUTHOR_ENCODING_PREFIX_MAPPING = {\n      AUTHOR_USER: USER_PREFIX,\n      AUTHOR_ASSISTANT: ASSISTANT_PREFIX,\n  }\n\n  def _preprocess_one_conversation(context, messages, output_file):\n    \"\"\"Preprocesses one chat messages dataset example.\"\"\"\n    if not messages or len(messages) <= 1:\n      raise ValueError(\n          'Chat messages length should greater than 1. Please include a'\n          ' `messages` field in each line of dataset.'\n      )\n    for message in messages:\n      author = message.get('author')\n      content = message.get('content')\n      if not author or author not in [AUTHOR_USER, AUTHOR_ASSISTANT]:\n        raise ValueError(\n            f'The `author` of each message needs to be from `{AUTHOR_USER}` or'\n            f' `{AUTHOR_ASSISTANT}`.'\n        )\n      if not content:\n        raise ValueError('The `content` of each message needs to be non-empty.')\n\n    per_conversation_context = (\n        [CONTEXT_PREFIX + context + '\\n'] if context else []\n    )\n    message_history = []\n    for message in messages:\n      author = message.get('author')\n      content = message.get('content')\n      if author == AUTHOR_ASSISTANT:\n        encoding_list = (\n            [GLOBAL_PREFIX]\n            + per_conversation_context\n            + message_history\n            + [ASSISTANT_PREFIX]\n        )\n        input_text = '\\n'.join(encoding_list)\n        json.dump(\n            {INPUT_TEXT_KEY: input_text, OUTPUT_TEXT_KEY: content}, output_file\n        )\n        output_file.write('\\n')\n\n      message_history.append(\n          f'{AUTHOR_ENCODING_PREFIX_MAPPING[author]}{content}'\n      )\n\n  def _preprocess_chat_dataset(dataset_uri, default_context, output_file_path):\n    # Changes gs:// to /gcs/ since we use open() and GCS Fuse.\n    if dataset_uri.startswith(GS_PREFIX):\n      dataset_uri = dataset_uri.replace(GS_PREFIX, GCS_ROOT_PREFIX, 1)\n\n    with open(dataset_uri) as jsonl_file:\n      with open(output_file_path, 'w') as output_file:\n        for dataset_line in jsonl_file:\n          dataset_instance = json.loads(dataset_line)\n\n          # Check if per example context exists, if not, use default_context\n          context = dataset_instance.get(CONTEXT_KEY, default_context)\n          messages = dataset_instance.get(MESSAGES_KEY)\n\n          _preprocess_one_conversation(context, messages, output_file)\n\n  try:\n    output_tuple = collections.namedtuple('Outputs', ['output_file'])\n\n    # Extra makedirs call to ensure all dirs are present.\n    # Without this the leaf dir may not be created.\n    os.makedirs(output_dir, exist_ok=True)\n\n    if not dataset_uri:\n      return output_tuple('')\n\n    if default_context and model.startswith('text'):\n      raise ValueError('`default context` only applies to chat models.')\n\n    output_file_path = os.path.join(output_dir, os.path.basename(dataset_uri))\n\n    if 'chat' in model:\n      _preprocess_chat_dataset(dataset_uri, default_context, output_file_path)\n    else:\n      output_file_path = dataset_uri\n\n    # Subsequent components will not understand \"/gcs/\" path (generated by\n    # Vertex Pipelines). Convert to \"gs://\" prefix for compatibility.\n    if output_file_path.startswith(GCS_ROOT_PREFIX):\n      output_file_path = GS_PREFIX + output_file_path[len(GCS_ROOT_PREFIX) :]\n\n    return output_tuple(output_file_path)\n  except Exception as e:  # pylint: disable=broad-exception-caught\n    if isinstance(e, ValueError):\n      raise\n    logging.exception(str(e))\n    sys.exit(13)\n\n"],"image":"gcr.io/ml-pipeline/google-cloud-pipeline-components:2.3.1"}},"exec-tensorboard-uploader":{"container":{"command":["bash","-c","set -e -x\nPROJECT=\"$0\"\nLOCATION=\"$1\"\nTENSORBOARD_ID=\"$2\"\nMETRICS_DIRECTORY_URI=\"$3\"\nEXPERIMENT_NAME=\"$4\"\nTENSORBOARD_URI=\"$5\"\n\nmkdir -p \"$(dirname ${TENSORBOARD_URI})\"\nif [ -z \"${TENSORBOARD_ID}\" ];\nthen\n  echo \"TensorBoard ID is not set. Skip uploading the TensorBoard.\"\n  echo -n \"\" > \"${TENSORBOARD_URI}\"\n  exit 0\nfi\n\nif [ -z \"${PROJECT}\" ]; then\n  echo \"Project is not set.\"\n  exit 1\nelif [ -z \"${LOCATION}\" ]; then\n  echo \"Location is not set.\"\n  exit 1\nelif [ -z \"${METRICS_DIRECTORY_URI}\" ]; then\n  echo \"Metrics directory uri is not set.\"\n  exit 1\nelif [ -z \"${EXPERIMENT_NAME}\" ]; then\n  echo \"Experiment name is not set.\"\n  exit 1\nelif [ -z \"${TENSORBOARD_URI}\" ]; then\n  echo \"TensorBoard URI is not set.\"\n  exit 1\nfi\n\ncase \"${METRICS_DIRECTORY_URI}\" in\n  \"gs://\"*) ;;\n  *)\n    echo \"Invalid metrics directory uri. Metrics directory uri must start with gs://.\"\n    exit 1\n    ;;\nesac\n\nif [[ \"${TENSORBOARD_ID}\" =~ ^[0-9]+$ ]]; then\n  TENSORBOARD_RESOURCE_ID=\"projects/${PROJECT}/locations/${LOCATION}/tensorboards/${TENSORBOARD_ID}\"\nelif [[ \"${TENSORBOARD_ID}\" =~ ^projects/[^/]+/locations/[^/]+/tensorboards/[0-9]+$ ]]; then\n  echo \"Split tensorboard id\"\n  TENSORBOARD_RESOURCE_ID=\"${TENSORBOARD_ID}\"\n  TENSORBOARD_RESOURCE_ARR=(${TENSORBOARD_ID//\\// })\n  PROJECT=${TENSORBOARD_RESOURCE_ARR[1]}\n  LOCATION=${TENSORBOARD_RESOURCE_ARR[3]}\n  TENSORBOARD_ID=${TENSORBOARD_RESOURCE_ARR[5]}\nelse\n  echo '[ERROR]: Invalid format of tensorboard_id. It should be either a '\\\n'string of digits or projects/${PROJECT}/location/${LOCATION}/'\\\n'tensorboards/${TENSORBOARD_ID_DIGITS}'\n  exit 1\nfi\n\nset +e\n\n/opt/conda/bin/tb-gcp-uploader --tensorboard_resource_name \\\n  \"${TENSORBOARD_RESOURCE_ID}\" \\\n  --logdir=\"${METRICS_DIRECTORY_URI}\" \\\n  --experiment_name=\"${EXPERIMENT_NAME}\" \\\n  --one_shot=True\n\nif [ $? -ne 0 ]; then\n  exit 13\nfi\n\nset -e\n\nweb_server_uri=\"tensorboard.googleusercontent.com\"\ntensorboard_resource_name_uri=\"projects+${PROJECT}+locations+${LOCATION}+tensorboards+${TENSORBOARD_ID}+experiments+${EXPERIMENT_NAME}\"\necho -n \"https://${LOCATION}.${web_server_uri}/experiment/${tensorboard_resource_name_uri}\" > \"${TENSORBOARD_URI}\"\n","{{$.inputs.parameters['project']}}","{{$.inputs.parameters['location']}}","{{$.inputs.parameters['tensorboard_id']}}","{{$.inputs.artifacts['metrics_directory_uri'].uri}}","{{$.inputs.parameters['experiment_name']}}","{{$.outputs.parameters['tensorboard_uri'].output_file}}"],"image":"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-11:latest"}},"exec-upload-llm-model":{"container":{"args":["--executor_input","{{$}}","--function_to_execute","upload_llm_model"],"command":["sh","-ec","program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n","\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef upload_llm_model(\n    project: str,\n    location: str,\n    artifact_uri: dsl.Input[dsl.Artifact],\n    model_reference_name: str,\n    model_display_name: str,\n    regional_endpoint: str,\n    model_resource_name: dsl.OutputPath(str),\n    gcp_resources: dsl.OutputPath(str),\n    encryption_spec_key_name: str = '',\n    upload_model: bool = True,\n):\n  \"\"\"Uploads LLM model.\n\n  Args:\n      project: Name of the GCP project.\n      location: Location for model upload and deployment.\n      artifact_uri: KFP Artifact for adapter.\n      model_reference_name: Large model reference name.\n      model_display_name: Name of the model (shown in Model Registry).\n      regional_endpoint: Regional API endpoint.\n      encryption_spec_key_name: Customer-managed encryption key.\n      upload_model: Whether to upload the model to the Model Registry. Default\n          is ``True``. If ``False``, the model will not be uploaded and output\n          artifacts will contain empty strings.\n\n  Returns:\n      model_resource_name: Path to the created Model on Model Registry.\n      gcp_resources: Serialized JSON of `gcp_resources`.\n  \"\"\"\n  import json\n  import logging\n  import os\n  import sys\n  try:\n    from google_cloud_pipeline_components.container.v1.gcp_launcher import lro_remote_runner\n  except ImportError:\n    from google_cloud_pipeline_components.google_cloud_pipeline_components.container.v1.gcp_launcher import lro_remote_runner\n\n  try:\n    os.makedirs(os.path.dirname(model_resource_name), exist_ok=True)\n\n    if not upload_model:\n      with open(model_resource_name, 'w') as fout:\n        fout.write('')\n      return\n\n    pipeline_labels_str = os.getenv('VERTEX_AI_PIPELINES_RUN_LABELS')\n    labels = (\n        json.loads(pipeline_labels_str) if pipeline_labels_str else {})\n    labels['google-vertex-llm-tuning-base-model-id'] = (\n        model_reference_name.replace('@', '-')\n    )\n\n    model_upload_payload = {\n        'model': {\n            'displayName': model_display_name,\n            'largeModelReference': {\n                'name': model_reference_name\n            },\n            'labels': labels,\n            'generatedModelSource': {\n                'genie_source': {\n                    'base_model_uri': ''\n                }\n            },\n            'artifactUri': artifact_uri.uri\n        }\n    }\n    if encryption_spec_key_name:\n      model_upload_payload['model']['encryption_spec'] = {\n          'kms_key_name': encryption_spec_key_name\n      }\n\n    regional_endpoint = regional_endpoint.rstrip('/')\n    upload_model_uri = (\n        f'{regional_endpoint}/projects/{project}/locations/{location}/models:'\n        'upload')\n\n    remote_runner = lro_remote_runner.LroRemoteRunner(location)\n    upload_model_lro = remote_runner.create_lro(\n        upload_model_uri,\n        json.dumps(model_upload_payload),\n        gcp_resources,\n    )\n    upload_model_lro = remote_runner.poll_lro(lro=upload_model_lro)\n    model_resource = upload_model_lro['response']['model']\n    model_version_id = (\n        upload_model_lro['response'].get('model_version_id')\n        or upload_model_lro['response'].get('modelVersionId'))\n    if model_version_id:\n      model_resource += f'@{model_version_id}'\n\n    with open(model_resource_name, 'w') as fout:\n      fout.write(model_resource)\n\n  except Exception as e:  # pylint: disable=broad-exception-caught\n    if isinstance(e, ValueError):\n      raise\n    logging.exception(str(e))\n    sys.exit(13)\n\n"],"image":"gcr.io/ml-pipeline/google-cloud-pipeline-components:2.3.1"}},"exec-validate-pipeline":{"container":{"args":["--executor_input","{{$}}","--function_to_execute","validate_pipeline"],"command":["sh","-ec","program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n","\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef validate_pipeline(\n    location: str = '{{$.pipeline_google_cloud_location}}',\n    model: str = 'text-bison-001',\n    encryption_spec_key_name: str = '',\n    tpu_training_skip_cmek: bool = False,\n    api_endpoint: str = 'aiplatform.googleapis.com/ui',\n    tuning_method: str = 'tune',\n    evaluation_data_uri: str = '',\n    evaluation_output_root_dir: str = '',\n    tensorboard_resource_id: str = '',\n    accelerator_type: str = '',\n    pipeline_region: str = '{{$.pipeline_google_cloud_location}}',\n) -> NamedTuple('outputs',\n                model_dash_version=str,\n                model_at_version=str,\n                use_tpu=bool,\n                adapter_training_location=str,\n                with_tensorboard=bool,\n                with_batch_prediction_and_evaluation=bool,\n                service_account=str,\n                regional_endpoint=str,\n                model_location=str,):\n  \"\"\"Validate and preprocess pipeline parameters.\n\n  Args:\n      location: Region in which all the components except for tuning job should\n          run.\n      model: Large language reference model to use (default: `text-bison-001`).\n      encryption_spec_key_name: If set, CMEK will be used.\n      tpu_training_skip_cmek: If True, skip CMEK setup for TPU training.\n      api_endpoint: API endpoint.\n      tuning_method: Tuning method.\n      evaluation_data_uri: GCS URI of eval dataset. If evaluation dataset is\n          provided, the best tuned checkpoint will be selected.\n      evaluation_output_root_dir: Root GCS URI of evaluation outputs.\n      tensorboard_resource_id: The Vertex AI TensorBoard Resource/Instance ID.\n          If this parameter is set, then a Vertex AI TensorBoard Experiment will\n          be launched.\n      accelerator_type: One of 'TPU' or 'GPU'. If 'TPU' is specified, tuning\n          runs in europe-west4, else in us-central1.\n      pipeline_region: The region the pipeline runs in.\n\n  Returns:\n      outputs: NamedTuple consisting of fields of model_dash_version,\n          model_at_version, use_tpu, adapter_training_location,\n          with_tensorboard, with_batch_prediction_and_evaluation,\n          service_account, regional_endpoint, model_location..\n  \"\"\"\n  import os\n  import logging\n  import sys\n\n  try:\n    if not accelerator_type and 'us-central1' != location:\n      raise ValueError('Tuning is only supported in us-central1.')\n\n    # Convert model version with formats text-bison-001 and text-bison@001.\n    if tuning_method == 'tune' and (\n        'text-bison-001' == model or 'text-bison@001' == model\n    ):\n      model_dash_version = 'text-bison-001'\n      model_at_version = 'text-bison@001'\n    else:\n      model_dash_version = model\n      model_at_version = model\n\n    service_account = os.environ['CLOUD_ML_JOB_SA']\n\n    supported_pipeline_regions = [\n        'europe-west4',\n        'asia-southeast1',\n        'us-west1',\n        'europe-west3',\n        'europe-west2',\n        'asia-northeast1',\n        'us-central1',\n        'us-east4',\n        'us-west4',\n        'northamerica-northeast1',\n        'europe-west9',\n        'europe-west1',\n        'asia-northeast3',\n    ]\n    if pipeline_region not in supported_pipeline_regions:\n      raise ValueError(\n          f'Unsupported pipeline region: {pipeline_region}. Must be one of'\n          f' {supported_pipeline_regions}.'\n      )\n\n    if accelerator_type:\n      if accelerator_type.upper() == 'TPU':\n        use_tpu = True\n        adapter_training_location = 'europe-west4'\n      elif accelerator_type.upper() == 'GPU':\n        use_tpu = False\n        adapter_training_location = 'us-central1'\n      else:\n        raise ValueError(\n            f'Unsupported accelerator_type: {accelerator_type}. Should'\n            ' be one of \"TPU\" or \"GPU\".'\n        )\n    else:\n      if 'europe-west4' == pipeline_region:\n        use_tpu = True\n        adapter_training_location = 'europe-west4'\n      elif 'us-central1' == pipeline_region:\n        use_tpu = False\n        adapter_training_location = 'us-central1'\n      else:\n        raise ValueError(\n            '`accelerator_type` must be specified to run a pipeline in'\n            f' {pipeline_region}.'\n        )\n\n    location = pipeline_region if not location else location\n\n    if use_tpu:\n      if encryption_spec_key_name:\n        if tpu_training_skip_cmek:\n          logging.warning(\n              'CMEK will not be set for TPU training. If this is not expected, '\n              'please run with GPU.'\n          )\n        else:\n          raise ValueError(\n              'encryption_spec_key_name (CMEK) is not supported for TPU '\n              'training at the moment. Please either unset '\n              'encryption_spec_key_name or create your pipeline in us-central1 '\n              'to use GPU instead.'\n          )\n\n    if not use_tpu:\n      if encryption_spec_key_name:\n        if location != 'us-central1' or pipeline_region != 'us-central1':\n          raise ValueError(\n              'encryption_spec_key_name (CMEK) for GPU training is only'\n              ' supported in us-central1. Please either unset'\n              ' encryption_spec_key_name or create your pipeline in and/or'\n              ' specifythe model upload location to be us-central1 instead.'\n          )\n\n    outputs = NamedTuple(\n        'outputs',\n        model_dash_version=str,\n        model_at_version=str,\n        use_tpu=bool,\n        adapter_training_location=str,\n        with_tensorboard=bool,\n        with_batch_prediction_and_evaluation=bool,\n        service_account=str,\n        regional_endpoint=str,\n        model_location=str,\n    )\n    ret = outputs(\n        model_dash_version=model_dash_version,\n        model_at_version=model_at_version,\n        use_tpu=use_tpu,\n        adapter_training_location=adapter_training_location,\n        with_tensorboard=bool(\n            'tune' != tuning_method and tensorboard_resource_id),\n        with_batch_prediction_and_evaluation=bool(\n            evaluation_data_uri and evaluation_output_root_dir),\n        service_account=service_account,\n        regional_endpoint=f'https://{location}-{api_endpoint}',\n        model_location=location,\n    )\n    logging.info('use_tpu = [%s]', ret.use_tpu)\n    return ret\n  except Exception as e:  # pylint: disable=broad-exception-caught\n    if isinstance(e, ValueError):\n      raise\n    logging.exception(str(e))\n    sys.exit(13)\n\n"],"image":"gcr.io/ml-pipeline/google-cloud-pipeline-components:2.3.1"}},"exec-vertex-pipelines-prompt-validation":{"container":{"args":["--type","VertexPromptValidation","--payload",""],"command":["python3","-u","-m","google_cloud_pipeline_components.container.v1.vertex_prompt_validation.executor"],"image":"gcr.io/ml-pipeline/google-cloud-pipeline-components:2.5.0"}}}},"pipelineInfo":{"description":"Runs adapter training pipeline.","name":"tune-large-model"},"root":{"dag":{"outputs":{"parameters":{"endpoint_resource_name":{"valueFromParameter":{"outputParameterKey":"endpoint_resource_name","producerSubtask":"deployment-graph"}},"model_resource_name":{"valueFromParameter":{"outputParameterKey":"model_resource_name","producerSubtask":"deployment-graph"}}}},"tasks":{"condition-1":{"componentRef":{"name":"comp-condition-1"},"dependentTasks":["tuning-graph","validate-pipeline"],"inputs":{"artifacts":{"pipelinechannel--tuning-graph-tensorboard_metrics":{"taskOutputArtifact":{"outputArtifactKey":"tensorboard_metrics","producerTask":"tuning-graph"}}},"parameters":{"pipelinechannel--project":{"componentInputParameter":"project"},"pipelinechannel--tensorboard_resource_id":{"componentInputParameter":"tensorboard_resource_id"},"pipelinechannel--validate-pipeline-with_tensorboard":{"taskOutputParameter":{"outputParameterKey":"with_tensorboard","producerTask":"validate-pipeline"}}}},"taskInfo":{"name":"tensorboard-uploader-condition"},"triggerPolicy":{"condition":"inputs.parameter_values['pipelinechannel--validate-pipeline-with_tensorboard'] == true"}},"deployment-graph":{"cachingOptions":{"enableCache":true},"componentRef":{"name":"comp-deployment-graph"},"dependentTasks":["tuning-graph","validate-pipeline"],"inputs":{"artifacts":{"saved_model":{"taskOutputArtifact":{"outputArtifactKey":"saved_model","producerTask":"tuning-graph"}}},"parameters":{"encryption_spec_key_name":{"componentInputParameter":"encryption_spec_key_name"},"large_model_reference":{"taskOutputParameter":{"outputParameterKey":"model_at_version","producerTask":"validate-pipeline"}},"model_display_name":{"componentInputParameter":"model_display_name"},"model_location":{"taskOutputParameter":{"outputParameterKey":"model_location","producerTask":"validate-pipeline"}},"project":{"componentInputParameter":"project"},"regional_endpoint":{"taskOutputParameter":{"outputParameterKey":"regional_endpoint","producerTask":"validate-pipeline"}},"service_account":{"taskOutputParameter":{"outputParameterKey":"service_account","producerTask":"validate-pipeline"}}}},"taskInfo":{"name":"deployment-graph"}},"tuning-graph":{"cachingOptions":{"enableCache":true},"componentRef":{"name":"comp-tuning-graph"},"dependentTasks":["validate-pipeline"],"inputs":{"parameters":{"adapter_training_location":{"taskOutputParameter":{"outputParameterKey":"adapter_training_location","producerTask":"validate-pipeline"}},"dataset_name":{"componentInputParameter":"dataset_name"},"dataset_uri":{"componentInputParameter":"dataset_uri"},"default_context":{"componentInputParameter":"default_context"},"enable_checkpoint_selection":{"componentInputParameter":"enable_checkpoint_selection"},"enable_early_stopping":{"componentInputParameter":"enable_early_stopping"},"encryption_spec_key_name":{"componentInputParameter":"encryption_spec_key_name"},"evaluation_data_uri":{"componentInputParameter":"evaluation_data_uri"},"evaluation_interval":{"componentInputParameter":"evaluation_interval"},"learning_rate":{"componentInputParameter":"learning_rate"},"learning_rate_multiplier":{"componentInputParameter":"learning_rate_multiplier"},"model":{"taskOutputParameter":{"outputParameterKey":"model_dash_version","producerTask":"validate-pipeline"}},"project":{"componentInputParameter":"project"},"regional_endpoint":{"taskOutputParameter":{"outputParameterKey":"regional_endpoint","producerTask":"validate-pipeline"}},"tpu_training_skip_cmek":{"componentInputParameter":"tpu_training_skip_cmek"},"train_steps":{"componentInputParameter":"train_steps"},"tuning_method":{"componentInputParameter":"tuning_method"},"use_tpu":{"taskOutputParameter":{"outputParameterKey":"use_tpu","producerTask":"validate-pipeline"}}}},"taskInfo":{"name":"tuning-graph"}},"validate-pipeline":{"cachingOptions":{},"componentRef":{"name":"comp-validate-pipeline"},"inputs":{"parameters":{"accelerator_type":{"componentInputParameter":"accelerator_type"},"api_endpoint":{"componentInputParameter":"api_endpoint"},"encryption_spec_key_name":{"componentInputParameter":"encryption_spec_key_name"},"evaluation_data_uri":{"componentInputParameter":"evaluation_data_uri"},"location":{"componentInputParameter":"location"},"model":{"componentInputParameter":"large_model_reference"},"tensorboard_resource_id":{"componentInputParameter":"tensorboard_resource_id"},"tpu_training_skip_cmek":{"componentInputParameter":"tpu_training_skip_cmek"},"tuning_method":{"componentInputParameter":"tuning_method"}}},"taskInfo":{"name":"validate-pipeline"}}}},"inputDefinitions":{"parameters":{"accelerator_type":{"defaultValue":"","description":"One of 'TPU' or 'GPU'. If 'TPU' is specified, tuning\nruns in europe-west4, else in us-central1.","isOptional":true,"parameterType":"STRING"},"api_endpoint":{"defaultValue":"aiplatform.googleapis.com/ui","description":"API endpoint.","isOptional":true,"parameterType":"STRING"},"dataset_name":{"defaultValue":"","description":"Name of training dataset if importing from managed dataset.\nOnly this or `dataset_uri` should be set.","isOptional":true,"parameterType":"STRING"},"dataset_uri":{"defaultValue":"","description":"GCS URI of training dataset. Only this or `dataset_name`\nshould be set.","isOptional":true,"parameterType":"STRING"},"default_context":{"defaultValue":"","description":"If a chat model is specified, `default_context` will be\napplied to each training example.","isOptional":true,"parameterType":"STRING"},"enable_checkpoint_selection":{"defaultValue":"default","description":"One of 'true', 'false', or 'default'. If set\nto 'true' and if evaluation_data_uri is set, select the best tuned\ncheckpoint. If 'default' is set, checkpoint selection is enabled\nfor bison@001 models, and disabled for others.","isOptional":true,"parameterType":"STRING"},"enable_early_stopping":{"defaultValue":true,"description":"If set to True, and if evaluation_data_uri is set,\nthen early stopping will be enabled. If early stopping is enabled, the\nmax recommended train_steps is 1,000.","isOptional":true,"parameterType":"BOOLEAN"},"encryption_spec_key_name":{"defaultValue":"","description":"Customer-managed encryption key.","isOptional":true,"parameterType":"STRING"},"evaluation_data_uri":{"defaultValue":"","description":"GCS URI of eval dataset.","isOptional":true,"parameterType":"STRING"},"evaluation_interval":{"defaultValue":20,"description":"Evaluation after every this number of tuning steps.","isOptional":true,"parameterType":"NUMBER_INTEGER"},"evaluation_output_root_dir":{"defaultValue":"","description":"Root GCS URI of evaluation outputs.","isOptional":true,"parameterType":"STRING"},"large_model_reference":{"defaultValue":"text-bison@001","description":"Large model reference ID indicating which model to\nuse.","isOptional":true,"parameterType":"STRING"},"learning_rate":{"defaultValue":-1,"description":"Learning rate hyperparameter for tuning. If this value is\nnegative, then the default learning rate per model per tuning_method\nwill be used. This parameter will be deprecated.","isOptional":true,"parameterType":"NUMBER_DOUBLE"},"learning_rate_multiplier":{"defaultValue":1,"description":"Learning rate multiplier for tuning. Under the\nhood, the actual \"learning rate\" will be learning_rate_multiplier *\nthe \"recommended learning rate\" per model, so customers don't have to\nkeep track of the recommended learning rate for each model or tuning\nmethod.","isOptional":true,"parameterType":"NUMBER_DOUBLE"},"location":{"defaultValue":"","description":"Location for model upload and deployment.","isOptional":true,"parameterType":"STRING"},"model_display_name":{"description":"Name of the model (shown in model registry).","parameterType":"STRING"},"project":{"description":"Name of the GCP project.","parameterType":"STRING"},"tensorboard_resource_id":{"defaultValue":"","description":"The Vertex AI TensorBoard Resource/Instance ID.\nIf this parameter is set, then a Vertex AI TensorBoard Experiment will\nbe launched.","isOptional":true,"parameterType":"STRING"},"tpu_training_skip_cmek":{"defaultValue":false,"description":"If set to True, CMEK will be ignored during TPU\ntraining step.","isOptional":true,"parameterType":"BOOLEAN"},"train_steps":{"defaultValue":300,"description":"Number of training steps for tuning step.","isOptional":true,"parameterType":"NUMBER_INTEGER"},"tuning_method":{"defaultValue":"tune_v2","description":"The adapter tuning method. Available methods: tune,\ntune_v2.","isOptional":true,"parameterType":"STRING"}}},"outputDefinitions":{"parameters":{"endpoint_resource_name":{"parameterType":"STRING"},"model_resource_name":{"parameterType":"STRING"}}}},"schemaVersion":"2.1.0","sdkVersion":"kfp-2.3.0"}