# Version History ğŸš€

## fastchat-mcp

### v1.2.0 ğŸ§¹

### v1.1.3 ğŸ“¡

- âœ… Full **WebSocket integration** with authentication, middleware, and security.  
- ğŸ”‘ Integration with an **external API** to generate, store, and load tokens connected to a database.  
- âš™ï¸ **Simple, fast, and flexible configuration** of the middleware system directly from the configuration file.  
- ğŸ§© **Basic tokenization** implemented â€” future updates will extend this to support more advanced payloads.  
  - For customizing tokenization settings, please check the [example implementation](../examples/custom_api/api.py).  

### v1.1.2 ğŸ—ƒ

- ğŸ’¬ The chat flow now handles connections to endpoints that interact with databases.  
- âš™ï¸ Database usage is **optional**, fully **configurable**, and designed to be **easy to understand and manage**. [Learn more](DATABASE.md)

### v1.1.1 ğŸ§©

- âœ¨ It is now possible to add additional system prompts to the `Fastchat` module, enabling customization of response style and query processing. *[See example](./USAGE.md#customizing-system-prompts)*  

- ğŸ§© You can now connect additional servers to the `Fastchat` class with a straightforward configuration process. *[See example](../README.md#additional-mcp-servers)*  

### v1.1.0 ğŸ“¡

- ğŸ”„ **Shift from Sequential to Asynchronous Code**: Chat initialization must now be performed asynchronously using: `await chat.initialize();`.

- ğŸŒ **New WebSocket API**: Added a basic API that provides a WebSocket endpoint at: `"https://127.0.0.1:8000/chat/ws"`. Currently in **beta** stage, without an authentication system.

- ğŸš€ **Default Language Model**: The default model is now **`gpt-5-nano`** â€” faster, cheaper, and more accurate in completions.

### v1.0.1 ğŸ”§

- ğŸ“ Improved logging and terminal output for local chat.
- ğŸ¯ Enhanced prompt configuration for selecting prompts from the MCP.

### v1.0.0 ğŸ“Ÿ

- ğŸ“Ÿ The Stdio connection protocol has been implemented. The system now supports stdio-type connections.
- ğŸ‘¾ Access has been granted to utilize any OpenAI language model. Currently, only OpenAI is supported as the LLM provider.
- âš™ï¸ Rename config file from `config.json` to `fastchat.config.json`

### v0.1.3 âŒ›ï¸

- ğŸ”§ Renamed the package to `fastchat-mcp`.
- ğŸ’¬ Enabled real-time chat responses using streaming for final outputs.
- ğŸ› ï¸ Refined parameter handling for tools, adding support for multi-typed arguments.

---
---
---

## mcp-llm-client

> ## Note
>
> Versions prior to `v0.1.3` specifically used the `mcp-llm-client` package. Starting from version `v0.1.3`, the package for this repository has been renamed to `fastchat-mcp`.

### v0.1.2 ğŸ”§

- ğŸ **Bug fixed:** A small error originating from the `dev` development branch has been resolved.
- âœ… Now, if an MCP server is detected, **no error will be thrown because of this**.

---

### v0.1.1 âš™ï¸

- ğŸ” **Full integration with the new OAuth system:** Enhanced security and compatibility.
- âš™ï¸ **Advanced customization:** You can now add **custom headers** directly from the configuration.
- ğŸ·ï¸ **Improved flexibility:** Support for passing `app_name` from the configuration for better identification.

---

### v0.1.0 âœ¨

- ğŸŒŸ Full integration of prompts from MCP servers into the client workflow.
- âš™ï¸ Enhancement of prompt engineering within the repository.
- ğŸ”— Exclusive integration with `mcp.types.PromptMessage`.

---

### v0.0.8 ğŸ¯

- ğŸš€ The prompts for language models (LLMs) were optimized to deliver responses that are more closely aligned with the MCP context.
- ğŸ”„ The service extraction step was merged with the argument creation step, enabling both services and arguments to be identified in a single stage.

---

### v0.0.7 ğŸ”

- ğŸ›¡ï¸ A simple authorization system based on user credential authentication (username and password) was integrated. For further reference, please see [mcp-oauth](https://github.com/rb58853/mcp-oauth).

---

### v0.0.6 ğŸ“¡

- ğŸ“¥ The exposed services have been added to the context of all queries, including those that do not require the use of a specific service. This approach allows for general inquiries regarding the available services.

---

### v0.0.5 ğŸ§©

- ğŸ“‘ The LLM system is structured in steps, with each step being returned to the client making the query. This approach allows for the identification of the current stage within the query process.
- ğŸŒ Efficient language detection has been implemented for queries, enabling responses to be provided based on the detected language.
- ğŸ’¬ The `open_local_chat()` function has been added, making it easy to use a local chat.

---

### v0.0.4 ğŸ“¦

- ğŸ“¥ Package dependencies are incorporated during its initial installation process.

---

### v0.0.1 ğŸ› ï¸

- ğŸš€ Initial implementation of `Fastchat` client
- ğŸ”— Complete integration of `httpstream` protocol ([fasmcp](https://github.com/modelcontextprotocol/python-sdk))
- ğŸŒ Connectivity with multiple servers
- ğŸ”§ Simplified fastchat.config.json file for connection management
- âš¡ Efficient processing of multiple simultaneous requests to tools and resources within a single query
- ğŸ”“ Simple connection without authorization (compatible only with servers that do not require authentication)
