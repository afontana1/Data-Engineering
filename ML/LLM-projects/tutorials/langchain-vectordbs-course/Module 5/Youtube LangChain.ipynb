{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Langchain and Deep Lake for Youtube summarization\n",
    "\n",
    "In this tutorial, we will explore the capabilities of the LangChain library, which offers a suite of tools for working with text data, including document retrieval, text summarization, and language model interactions. We will specifically focus on utilizing LangChain in combination with OpenAI's language model to perform document retrieval and generate concise summaries. In addition, we will be leveraging the Deep Lake's ability to store multi-modal data in the form of text and vector embeddings, to make the retrieval and summarization process more streamlined.\n",
    "\n",
    "## Outline:\n",
    "\n",
    "### Setting up the Environment:\n",
    "\n",
    "Importing the necessary modules from LangChain.\n",
    "Configuring the required components, such as OpenAI for language model interactions.\n",
    "\n",
    "### Document Retrieval:\n",
    "\n",
    "Establishing a vector store using DeepLake for efficient storage and retrieval of text data.\n",
    "Utilizing the retriever functionality to search for relevant documents based on specific criteria.\n",
    "Extracting the retrieved documents for further processing.\n",
    "\n",
    "### Text Summarization:\n",
    "\n",
    "Defining a prompt template to guide the summarization process.\n",
    "Creating a prompt using the PromptTemplate class to structure the summary generation.\n",
    "Applying the OpenAI language model through the LLMChain class to generate summaries.\n",
    "Combining the generated summaries into a cohesive text.\n",
    "\n",
    "### Putting It All Together:\n",
    "\n",
    "Integrating the document retrieval and text summarization steps.\n",
    "Demonstrating how the LangChain library can be leveraged to retrieve relevant documents and generate concise summaries.\n",
    "By the end of this tutorial, you will have a foundational understanding of using LangChain and OpenAI's language model to perform document retrieval and generate summaries. These techniques can be applied to various text analysis tasks, enabling efficient information retrieval and summarization from large text collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = 'YOUR KEY'\n",
    "os.environ['ACTIVELOOP_TOKEN'] = 'YOUR KEY'\n",
    "YOUTUBE_KEY = 'YOUR KEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above imports the `os` module to work with environment variables. Then, it sets the OPENAI_API_KEY, ACTIVELOOP_TOKEN, and YOUTUBE_KEY variables to the corresponding API keys or tokens.\n",
    "\n",
    "To obtain a YouTube API key:\n",
    "1. Go to the [Google Developers Console](https://console.cloud.google.com/).\n",
    "2. Create a new project or select an existing project.\n",
    "3. Enable the \"YouTube Data API v3\" for your project.\n",
    "4. Go to the \"Credentials\" section.\n",
    "5. Create a new API key.\n",
    "6. Copy the generated API key.\n",
    "7. Replace the placeholder in the code snippet with your API key.\n",
    "Remember to keep your API key secure and avoid sharing it publicly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "\n",
    "CHANNEL_ID = 'UCESLZhusAkFfsNsApnjF_Cg'\n",
    "\n",
    "youtube = build('youtube', 'v3', developerKey=YOUTUBE_KEY)\n",
    "\n",
    "urls = []\n",
    "\n",
    "response = youtube.search().list(\n",
    "    part='snippet',\n",
    "    channelId=CHANNEL_ID,\n",
    "    maxResults=10,  # Set the desired number of recent videos\n",
    "    order='date',  # Retrieve videos in reverse chronological order\n",
    "    type='video',\n",
    ").execute()\n",
    "\n",
    "videos = response['items']\n",
    "for video in videos:\n",
    "    video_id = video['id']['videoId']\n",
    "    video_title = video['snippet']['title']\n",
    "\n",
    "    urls.append(video_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then retrieve the IDs and titles of the most recent videos uploaded to a specific YouTube channel using the YouTube Data API.\n",
    "\n",
    "Here's a breakdown:\n",
    "\n",
    "1. Import the necessary functions from the `googleapiclient.discovery` module.\n",
    "2. Set the `CHANNEL_ID` variable to the unique identifier of the YouTube channel.\n",
    "3. Create a `youtube` service object using the `build` function, specifying the YouTube Data API version and API key.\n",
    "4. Initialize an empty list, `urls`, to store video IDs.\n",
    "5. Call `youtube.search().list()` to search for videos, providing parameters such as `part`, `channelId`, `maxResults`, `order`, and `type`.\n",
    "6. Execute the search request using the `execute()` method, obtaining a response.\n",
    "7. Access the list of video items in the response under the `items` key.\n",
    "8. Retrieve the video ID and title from each item and append the video ID to the `urls` list.\n",
    "\n",
    "After execution, the `urls` list will contain the video IDs of the most recent videos from the specified YouTube channel, which can be used for further processing or constructing video URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import YoutubeLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "\n",
    "docs = []\n",
    "for url in urls:\n",
    "    loader = YoutubeLoader.from_youtube_url('https://www.youtube.com/watch?v=' + url)\n",
    "    docs.extend(loader.load_and_split())\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain is then used to split the text into smaller chunks. The code uses the LangChain library to load and split text from YouTube videos into smaller text chunks.\n",
    "\n",
    "A high level understanding of the code is as follows:\n",
    "\n",
    "1. Import the necessary modules from the LangChain library: `YoutubeLoader` for loading YouTube videos and `CharacterTextSplitter` for splitting text.\n",
    "2. Create an empty list, `docs`, to store the loaded documents.\n",
    "3. Iterate over the `urls` list to process each video URL.\n",
    "4. Create a `YoutubeLoader` object by passing the YouTube URL (with the video ID appended) to the `from_youtube_url` method.\n",
    "5. Use the `loader.load_and_split()` method to load the video and split its text into smaller chunks.\n",
    "6. Extend the `docs` list with the loaded and split documents.\n",
    "7. Create a `CharacterTextSplitter` object, specifying the desired `chunk_size` and `chunk_overlap`.\n",
    "8. Use the `split_documents()` method of the `text_splitter` object to split the documents into smaller text chunks.\n",
    "9. The resulting `texts` variable will contain the split text chunks.\n",
    "\n",
    "This code allows for loading and splitting text from YouTube videos, enabling further processing or analysis on smaller text units.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "\n",
    "embeddings = OpenAIEmbeddings(disallowed_special=())\n",
    "db = DeepLake(dataset_path='hub://{YOUR_NAME}/youtube', embedding_function=embeddings)\n",
    "db.add_documents(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This snippet utilizes the LangChain library to perform text embeddings and create a vector store using DeepLake. It adds the split text chunks to the DeepLake vector store for future retrieval and analysis.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "1. Import the necessary modules from the LangChain library: `OpenAIEmbeddings` for text embeddings and `DeepLake` for the vector store.\n",
    "2. Create an `OpenAIEmbeddings` object called `embeddings` with the optional parameter `disallowed_special` set to an empty tuple. This object is responsible for generating embeddings for the text chunks.\n",
    "3. Create a `DeepLake` vector store called `db`, specifying the dataset path as `'hub://{YOUR_NAME}/youtube'`. Replace `{YOUR_NAME}` with your desired name or identifier.\n",
    "4. Add the split text chunks (`texts`) to the `db` vector store using the `add_documents()` method.\n",
    "\n",
    "After executing this code, the `db` vector store will contain the embedded representations of the text chunks from the YouTube videos. This allows for efficient storage and retrieval of the text embeddings for subsequent analysis or similarity matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 10})\n",
    "retrieved_documents = retriever.get_relevant_documents('fed raising rates')\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "    Generate a 100-word summary for the following text:\n",
    "\n",
    "    {text}\n",
    "\n",
    "    To create an accurate summary of this text, please use your own words and pay attention to the main point or idea. \n",
    "    Avoid including any additional information, as that will not accurately represent the source material. \n",
    "    Your summary should include only the key facts and concepts; it should condense the original text into a concise and accurate 100-word summary that clearly communicates the main idea or point of the text.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "chain = LLMChain(llm=OpenAI(temperature=0, model_name = 'text-davinci-003'), prompt=prompt)\n",
    "results = chain.apply([{'text': t} for t in retrieved_documents])\n",
    "combined_text = ('\\n').join([result['text'] for result in results]).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet showcases the utilization of the LangChain library for performing document retrieval and generating a 100-word summary using OpenAI's language model.\n",
    "\n",
    "And the final step to creating a coherent summary of key ideas spread across multiple videos is to:\n",
    "\n",
    "1. Import the necessary modules: `OpenAI` from LangChain for language model interactions, `LLMChain` for the language model chain, and `PromptTemplate` for defining the prompt structure.\n",
    "2. Convert the `db` vector store into a retriever object called `retriever` using the `as_retriever()` method. Specify the retrieval search parameters, such as `\"k\": 10`, to retrieve the top 10 relevant documents.\n",
    "3. Retrieve relevant documents by calling `get_relevant_documents()` on the `retriever` object, providing the query `'fed raising rates'`. The retrieved documents are stored in the `retrieved_documents` variable.\n",
    "4. Define the `prompt_template` variable, which contains a template for generating the summary. It includes a placeholder `{text}` for the actual text.\n",
    "5. Create a `PromptTemplate` object called `prompt` using the template defined above. Specify `\"text\"` as the input variable for the template.\n",
    "6. Create an `LLMChain` object called `chain`, utilizing OpenAI's language model (`OpenAI`) with a temperature value of 0 and the model name set to `'text-davinci-003'`. This chain is responsible for generating the summaries based on the prompt.\n",
    "7. Apply the chain to each retrieved document by calling the `apply()` method on the `chain` object. The input is a list of dictionaries, where each dictionary contains the key `'text'` mapped to the document text.\n",
    "8. Store the generated summaries in the `results` variable.\n",
    "9. Combine the generated summaries into a single string, `combined_text`, by joining the text from each result with a newline character (`\\n`). Leading and trailing whitespaces are removed using `strip()`.\n",
    "\n",
    "After executing this code, the `combined_text` variable will hold multiple concise 100-word summary generated by the language model for the retrieved documents related to the query `'fed raising rates'`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
