{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guide for Scraping Web Content and Storing in DeepLake\n",
    "\n",
    "This guide will walk you through the process of scraping web content and storing it in a DeepLake vector store using the Python script provided. We'll scrape Hugging Face's documentation pages, process the scraped data, and then load it into a DeepLake database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "First, we'll need to import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Environment Variables\n",
    "Next, we'll load the environment variables from the .env file and get the dataset path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "dataset_path = os.environ.get('DEEPLAKE_DATASET_PATH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate OpenAIEmbeddings\n",
    "We create an instance of OpenAIEmbeddings() which we'll use to transform our text data into embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define URL List\n",
    "We define a function get_documentation_urls() which will return a list of relative URLs we intend to scrape. Modify this list as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documentation_urls():\n",
    "    return ['/docs/huggingface_hub/guides/overview', '/docs/huggingface_hub/guides/download']  # list of URLs, can include more if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Full URLs\n",
    "We construct the full URL by appending the relative URL to the base URL using construct_full_url() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_full_url(base_url, relative_url):\n",
    "    # Construct the full URL by appending the relative URL to the base URL\n",
    "    return base_url + relative_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Page Content\n",
    "Using scrape_page_content() function, we send a GET request to the URL, parse the HTML response, extract and clean the desired content from the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page_content(url):\n",
    "    # Send a GET request to the URL and parse the HTML response using BeautifulSoup\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # Extract the desired content from the page (in this case, the body text)\n",
    "    text=soup.body.text.strip()\n",
    "    # Remove non-ASCII characters\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0b-\\x0c\\x0e-\\x1f\\x7f-\\xff]', '', text)\n",
    "    # Remove extra whitespace and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Scraped Content to File\n",
    "We scrape the content from all URLs and write it to a file using scrape_all_content() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_content(base_url, relative_urls, filename):\n",
    "    # Loop through the list of URLs, scrape content, and add it to the content list\n",
    "    content = []\n",
    "    for relative_url in relative_urls:\n",
    "        full_url = construct_full_url(base_url, relative_url)\n",
    "        scraped_content = scrape_page_content(full_url)\n",
    "        content.append(scraped_content.rstrip('\\n'))\n",
    "\n",
    "    # Write the scraped content to a file\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        for item in content:\n",
    "            file.write(\"%s\\n\" % item)\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Documents from a File\n",
    "Next, we define a function load_docs() to load the scraped content from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs(root_dir,filename):\n",
    "    # Create an empty list to hold the documents\n",
    "    docs = []\n",
    "    try:\n",
    "        # Load the file using the TextLoader class and UTF-8 encoding\n",
    "        loader = TextLoader(os.path.join(\n",
    "            root_dir, filename), encoding='utf-8')\n",
    "        # Split the loaded file into separate documents and add them to the list of documents\n",
    "        docs.extend(loader.load_and_split())\n",
    "    except Exception as e:\n",
    "        # If an error occurs during loading, ignore it and return an empty list of documents\n",
    "        pass\n",
    "    # Return the list of documents\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Documents into Chunks\n",
    "We then split the loaded documents into smaller chunks using split_docs() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_docs(docs):\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    return text_splitter.split_documents(docs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Vectors into DeepLake\n",
    "Finally, we load these chunks into a DeepLake database using load_vectors_into_deeplake() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors_into_deeplake(dataset_path, source_chunks):\n",
    "    # Initialize the DeepLake database with the dataset path and embedding function\n",
    "    deeplake_db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "    # Add the text chunks to the database\n",
    "    deeplakedb=deeplake_db.add_texts(source_chunks)\n",
    "    return deeplakedb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function\n",
    "This is then followed by  function runs the entire process by calling the functions we've defined in the correct order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://huggingface.co'\n",
    "# Set the name of the file to which the scraped content will be saved\n",
    "filename='content.txt'\n",
    "# Set the root directory where the content file will be saved\n",
    "root_dir ='./'\n",
    "relative_urls = get_documentation_urls()\n",
    "# Scrape all the content from the relative urls and save it to the content file\n",
    "content = scrape_all_content(base_url, relative_urls,filename)\n",
    "# Load the content from the file\n",
    "docs = load_docs(root_dir,filename)\n",
    "# Split the content into individual documents\n",
    "texts = split_docs(docs)\n",
    "# Create a DeepLake database with the given dataset path and embedding function\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "# Add the individual documents to the database\n",
    "db.add_documents(texts)\n",
    "# Clean up by deleting the content file\n",
    "os.remove(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this guide, we provided an overview of a script designed to scrape web content, specifically from the Hugging Face's documentation pages, and store it in a DeepLake vector store. This script integrates a wide array of operations such as web scraping, text cleaning, document loading, text splitting, and embedding generation, thus making it a comprehensive solution for extracting and preparing data for further analysis or application in machine learning tasks.\n",
    "\n",
    "Using this script as a base, you can adapt it to your needs, for example by scraping different websites or storing the data in a different kind of database. Remember, the key lies in understanding each step, as it will allow you to modify and improve the script according to your needs.\n",
    "\n",
    "Remember to respect the terms and conditions of the website you are scraping and avoid overloading servers with excessive requests. Happy scraping!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
