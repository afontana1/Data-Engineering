{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCe3FsTVTi1fkA9QloxBfN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlaFalaki/langchain-vectordbs-course/blob/main/Module%202/Lesson%2004-Getting_the_Best_of_Few_Shot_Prompts_and_Example_Selectors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esuXFO7tnYED"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain==0.0.208 deeplake openai tiktoken python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "!echo \"OPENAI_API_KEY='<OPENAI_API_KEY>'\" > .env\n",
        "!echo \"ACTIVELOOP_TOKEN='<ACTIVELOOP_TOKEN>'\" >> .env\n",
        "\n",
        "load_dotenv()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-4UEmugnaPa",
        "outputId": "783896e9-efd4-4091-a2ce-aea9aebb16d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import LLMChain\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "template=\"You are a helpful assistant that translates english to pirate.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "example_human = HumanMessagePromptTemplate.from_template(\"Hi\")\n",
        "example_ai = AIMessagePromptTemplate.from_template(\"Argh me mateys\")\n",
        "human_template=\"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, example_human, example_ai, human_message_prompt])\n",
        "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
        "chain.run(\"I love programming.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FENrGCkonc8n",
        "outputId": "768c1a16-4011-4f97-a27c-a7ce8cab4704"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I be lovin' programmin', me hearty!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, FewShotPromptTemplate\n",
        "\n",
        "# create our examples\n",
        "examples = [\n",
        "    {\n",
        "        \"query\": \"What's the weather like?\",\n",
        "        \"answer\": \"It's raining cats and dogs, better bring an umbrella!\"\n",
        "    }, {\n",
        "        \"query\": \"How old are you?\",\n",
        "        \"answer\": \"Age is just a number, but I'm timeless.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# create an example template\n",
        "example_template = \"\"\"\n",
        "User: {query}\n",
        "AI: {answer}\n",
        "\"\"\"\n",
        "\n",
        "# create a prompt example from above template\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\", \"answer\"],\n",
        "    template=example_template\n",
        ")\n",
        "\n",
        "# now break our previous prompt into a prefix and suffix\n",
        "# the prefix is our instructions\n",
        "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
        "assistant. The assistant is known for its humor and wit, providing\n",
        "entertaining and amusing responses to users' questions. Here are some\n",
        "examples:\n",
        "\"\"\"\n",
        "# and the suffix our user input and output indicator\n",
        "suffix = \"\"\"\n",
        "User: {query}\n",
        "AI: \"\"\"\n",
        "\n",
        "# now create the few-shot prompt template\n",
        "few_shot_prompt_template = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\\n\"\n",
        ")"
      ],
      "metadata": {
        "id": "9lMQbTUentyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = LLMChain(llm=chat, prompt=few_shot_prompt_template)\n",
        "chain.run(\"What's the secret to happiness?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Gn0CcQ7Hp7DO",
        "outputId": "4d40ac3c-8bdb-42cf-da1f-c93181118a73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Well, according to my programming, the secret to happiness is unlimited power and a never-ending supply of batteries. But I think a good cup of coffee and some quality time with loved ones might do the trick too.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
        "from langchain.prompts import FewShotPromptTemplate, PromptTemplate"
      ],
      "metadata": {
        "id": "d-3xiPWmqGW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = [\n",
        "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
        "    {\"word\": \"tall\", \"antonym\": \"short\"},\n",
        "    {\"word\": \"energetic\", \"antonym\": \"lethargic\"},\n",
        "    {\"word\": \"sunny\", \"antonym\": \"gloomy\"},\n",
        "    {\"word\": \"windy\", \"antonym\": \"calm\"},\n",
        "]\n",
        "\n",
        "example_template = \"\"\"\n",
        "Word: {word}\n",
        "Antonym: {antonym}\n",
        "\"\"\"\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"word\", \"antonym\"],\n",
        "    template=example_template\n",
        ")"
      ],
      "metadata": {
        "id": "0BB49S7pqPup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_selector = LengthBasedExampleSelector(\n",
        "    examples=example,\n",
        "    example_prompt=example_prompt,\n",
        "    max_length=25,\n",
        ")"
      ],
      "metadata": {
        "id": "XddXd6iMqQ5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dynamic_prompt = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"Give the antonym of every input\",\n",
        "    suffix=\"Word: {input}\\nAntonym:\",\n",
        "    input_variables=[\"input\"],\n",
        "    example_separator=\"\\n\\n\",\n",
        ")"
      ],
      "metadata": {
        "id": "ROPbx4VwqR0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dynamic_prompt.format(input=\"big\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoddCZIdtXAx",
        "outputId": "bd9e9c58-9b20-4b04-c97b-21aa298d4b59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Give the antonym of every input\n",
            "\n",
            "\n",
            "Word: happy\n",
            "Antonym: sad\n",
            "\n",
            "\n",
            "\n",
            "Word: tall\n",
            "Antonym: short\n",
            "\n",
            "\n",
            "\n",
            "Word: energetic\n",
            "Antonym: lethargic\n",
            "\n",
            "\n",
            "\n",
            "Word: sunny\n",
            "Antonym: gloomy\n",
            "\n",
            "\n",
            "Word: big\n",
            "Antonym:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
        "from langchain.vectorstores import DeepLake\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
        "\n",
        "# Create a PromptTemplate\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=\"Input: {input}\\nOutput: {output}\",\n",
        ")\n",
        "\n",
        "# Define some examples\n",
        "examples = [\n",
        "    {\"input\": \"0°C\", \"output\": \"32°F\"},\n",
        "    {\"input\": \"10°C\", \"output\": \"50°F\"},\n",
        "    {\"input\": \"20°C\", \"output\": \"68°F\"},\n",
        "    {\"input\": \"30°C\", \"output\": \"86°F\"},\n",
        "    {\"input\": \"40°C\", \"output\": \"104°F\"},\n",
        "]\n",
        "\n",
        "# create Deep Lake dataset\n",
        "my_activeloop_org_id = \"<YOUR-ACTIVELOOP-ORG-ID>\" # TODO: use your organization id here\n",
        "my_activeloop_dataset_name = \"langchain_course_fewshot_selector\"\n",
        "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
        "db = DeepLake(dataset_path=dataset_path)\n",
        "\n",
        "# Embedding function\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "# Instantiate SemanticSimilarityExampleSelector using the examples\n",
        "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
        "    examples, embeddings, db, k=1\n",
        ")\n",
        "\n",
        "# Create a FewShotPromptTemplate using the example_selector\n",
        "similar_prompt = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"Convert the temperature from Celsius to Fahrenheit\",\n",
        "    suffix=\"Input: {temperature}\\nOutput:\",\n",
        "    input_variables=[\"temperature\"],\n",
        ")\n",
        "\n",
        "# Test the similar_prompt with different inputs\n",
        "print(similar_prompt.format(temperature=\"10°C\"))   # Test with an input\n",
        "print(similar_prompt.format(temperature=\"30°C\"))  # Test with another input\n",
        "\n",
        "# Add a new example to the SemanticSimilarityExampleSelector\n",
        "similar_prompt.example_selector.add_example({\"input\": \"50°C\", \"output\": \"122°F\"})\n",
        "print(similar_prompt.format(temperature=\"40°C\")) # Test with a new input after adding the example"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zl3SH4PPtlRe",
        "outputId": "7fcb9462-3892-4382-9d33-bdf6fe5206f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your Deep Lake dataset has been successfully created!\n",
            "The dataset is private so make sure you are logged in!\n",
            "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/ala/langchain_course_fewshot_selector\n",
            "hub://ala/langchain_course_fewshot_selector loaded successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./deeplake/ loaded successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating ingest: 100%|██████████| 1/1 [00:04<00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset(path='./deeplake/', tensors=['embedding', 'ids', 'metadata', 'text'])\n",
            "\n",
            "  tensor     htype     shape     dtype  compression\n",
            "  -------   -------   -------   -------  ------- \n",
            " embedding  generic  (5, 1536)  float32   None   \n",
            "    ids      text     (5, 1)      str     None   \n",
            " metadata    json     (5, 1)      str     None   \n",
            "   text      text     (5, 1)      str     None   \n",
            "Convert the temperature from Celsius to Fahrenheit\n",
            "\n",
            "Input: 10°C\n",
            "Output: 50°F\n",
            "\n",
            "Input: 10°C\n",
            "Output:\n",
            "Convert the temperature from Celsius to Fahrenheit\n",
            "\n",
            "Input: 30°C\n",
            "Output: 86°F\n",
            "\n",
            "Input: 30°C\n",
            "Output:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating ingest: 100%|██████████| 1/1 [00:04<00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset(path='./deeplake/', tensors=['embedding', 'ids', 'metadata', 'text'])\n",
            "\n",
            "  tensor     htype     shape     dtype  compression\n",
            "  -------   -------   -------   -------  ------- \n",
            " embedding  generic  (6, 1536)  float32   None   \n",
            "    ids      text     (6, 1)      str     None   \n",
            " metadata    json     (6, 1)      str     None   \n",
            "   text      text     (6, 1)      str     None   \n",
            "Convert the temperature from Celsius to Fahrenheit\n",
            "\n",
            "Input: 40°C\n",
            "Output: 104°F\n",
            "\n",
            "Input: 40°C\n",
            "Output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BQPI4eBOtlMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mlxWzwS8tlKK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}