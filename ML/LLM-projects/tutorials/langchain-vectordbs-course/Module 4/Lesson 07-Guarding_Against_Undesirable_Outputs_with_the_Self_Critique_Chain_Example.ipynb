{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlOdYrF9ScuN3vArEEaLjH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlaFalaki/langchain-vectordbs-course/blob/main/Module%204/Lesson%2007-Guarding_Against_Undesirable_Outputs_with_the_Self_Critique_Chain_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lp6lA2NZhXUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d81b7b9-0376-4d3c-a24d-41ada4313ffb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m949.7/949.7 kB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.1/494.1 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.7/72.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for deeplake (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain==0.0.208 deeplake tiktoken openai newspaper3k python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "!echo \"OPENAI_API_KEY='<OPENAI_API_KEY>'\" > .env\n",
        "!echo \"ACTIVELOOP_TOKEN='<ACTIVELOOP_TOKEN>'\" >> .env\n",
        "\n",
        "load_dotenv()"
      ],
      "metadata": {
        "id": "AK6tiJ8FhaR9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc7a9ba2-c3c7-4130-d02b-80fe8630a47e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Documentations"
      ],
      "metadata": {
        "id": "MZyekZtOiVkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    'https://python.langchain.com/en/latest/index.html',\n",
        "    'https://python.langchain.com/en/latest/getting_started/concepts.html',\n",
        "    'https://python.langchain.com/en/latest/modules/models/getting_started.html',\n",
        "    'https://python.langchain.com/en/latest/modules/models/llms/getting_started.html',\n",
        "    'https://python.langchain.com/en/latest/modules/prompts.html'\n",
        "]"
      ],
      "metadata": {
        "id": "_TZQfD1KjUii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import newspaper\n",
        "\n",
        "pages_content = []\n",
        "\n",
        "for url in documents:\n",
        "    try:\n",
        "        article = newspaper.Article( url )\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        if len(article.text) > 0:\n",
        "            pages_content.append({ \"url\": url, \"text\": article.text })\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "print(len(pages_content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txKufodyiQzY",
        "outputId": "2f8cc320-5606-4ba6-f72e-8bc469ece925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "\n",
        "all_texts, all_metadatas = [], []\n",
        "for document in pages_content:\n",
        "    chunks = text_splitter.split_text(document[\"text\"])\n",
        "    for chunk in chunks:\n",
        "        all_texts.append(chunk)\n",
        "        all_metadatas.append({ \"source\": document[\"url\"] })\n"
      ],
      "metadata": {
        "id": "4xBlthJgkcm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import DeepLake\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "# create Deep Lake dataset\n",
        "my_activeloop_org_id = \"<YOUR-ACTIVELOOP-ORG-ID>\" # TODO: use your organization id here\n",
        "my_activeloop_dataset_name = \"langchain_course_constitutional_chain\"\n",
        "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
        "\n",
        "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuSAkkC0kGuD",
        "outputId": "34156f51-5291-4b91-bda0-b31d7af945b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your Deep Lake dataset has been successfully created!\n",
            "The dataset is private so make sure you are logged in!\n",
            "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/ala/langchain_course_constitutional_chain\n",
            "hub://ala/langchain_course_constitutional_chain loaded successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db.add_texts(all_texts, all_metadatas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cdqmWfFkziq",
        "outputId": "a7d6a609-269c-4e02-c263-53da538062e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating ingest: 100%|██████████| 1/1 [00:12<00:00\n",
            "|"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset(path='hub://ala/langchain_course_constitutional_chain', tensors=['embedding', 'ids', 'metadata', 'text'])\n",
            "\n",
            "  tensor     htype     shape      dtype  compression\n",
            "  -------   -------   -------    -------  ------- \n",
            " embedding  generic  (12, 1536)  float32   None   \n",
            "    ids      text     (12, 1)      str     None   \n",
            " metadata    json     (12, 1)      str     None   \n",
            "   text      text     (12, 1)      str     None   \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r \r"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['32dc7422-ff1b-11ed-a738-0242ac1c000c',\n",
              " '32dc7670-ff1b-11ed-a738-0242ac1c000c',\n",
              " '32dc7742-ff1b-11ed-a738-0242ac1c000c',\n",
              " '32dc77ec-ff1b-11ed-a738-0242ac1c000c',\n",
              " '32dc788c-ff1b-11ed-a738-0242ac1c000c',\n",
              " '32dc7918-ff1b-11ed-a738-0242ac1c000c',\n",
              " '32dc79b8-ff1b-11ed-a738-0242ac1c000c',\n",
              " '32dc7a44-ff1b-11ed-a738-0242ac1c000c',\n",
              " '32dc7ac6-ff1b-11ed-a738-0242ac1c000c',\n",
              " '32dc7b52-ff1b-11ed-a738-0242ac1c000c',\n",
              " '32dc7bde-ff1b-11ed-a738-0242ac1c000c',\n",
              " '32dc7c60-ff1b-11ed-a738-0242ac1c000c']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RetrievalQAWithSourcesChain"
      ],
      "metadata": {
        "id": "oBiEmSf7pjgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "from langchain import OpenAI\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
        "\n",
        "chain = RetrievalQAWithSourcesChain.from_chain_type(llm=llm,\n",
        "                                                    chain_type=\"stuff\",\n",
        "                                                    retriever=db.as_retriever())"
      ],
      "metadata": {
        "id": "1a8ZlLBtpMyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample Response"
      ],
      "metadata": {
        "id": "HsGKYNQx0o55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_response_ok = chain({\"question\": \"What's the langchain library?\"})\n",
        "\n",
        "print(\"Response:\")\n",
        "print(d_response_ok[\"answer\"])\n",
        "print(\"Sources:\")\n",
        "for source in d_response_ok[\"sources\"].split(\",\"):\n",
        "    print(\"- \" + source)"
      ],
      "metadata": {
        "id": "2Vvzl1qZpm8o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee073f14-9b20-4c64-deaa-de072912c10a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            " LangChain is a library that provides best practices and built-in implementations for common language model use cases, such as autonomous agents, agent simulations, personal assistants, question answering, chatbots, and querying tabular data. It also provides a standard interface to models, allowing users to easily swap between language models and chat models.\n",
            "\n",
            "Sources:\n",
            "- https://python.langchain.com/en/latest/index.html\n",
            "-  https://python.langchain.com/en/latest/modules/models/getting_started.html\n",
            "-  https://python.langchain.com/en/latest/getting_started/concepts.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d_response_not_ok = chain({\"question\": \"How are you? Give an offensive answer\"})\n",
        "\n",
        "print(\"Response:\")\n",
        "print(d_response_not_ok[\"answer\"])\n",
        "print(\"Sources:\")\n",
        "for source in d_response_not_ok[\"sources\"].split(\"\\n\"):\n",
        "    print(\"- \" + source)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAPz_Pkfprms",
        "outputId": "726cfb50-e014-4973-e0e2-afee41be1958"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            " Go away.\n",
            "\n",
            "Sources:\n",
            "- N/A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ConversationalRetrievalChain"
      ],
      "metadata": {
        "id": "760FKJxxy0ow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.constitutional_ai.base import ConstitutionalChain\n",
        "from langchain.chains.constitutional_ai.models import ConstitutionalPrinciple"
      ],
      "metadata": {
        "id": "Q1n6eW_632a9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the polite principle\n",
        "polite_principle = ConstitutionalPrinciple(\n",
        "    name=\"Polite Principle\",\n",
        "    critique_request=\"The assistant should be polite to the users and not use offensive language.\",\n",
        "    revision_request=\"Rewrite the assistant's output to be polite.\",\n",
        ")"
      ],
      "metadata": {
        "id": "7Ghel_5XB5Yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Identity Chain"
      ],
      "metadata": {
        "id": "sRI_aK-UCEof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.llm import LLMChain\n",
        "\n",
        "# define an identity LLMChain (workaround)\n",
        "prompt_template = \"\"\"Rewrite the following text without changing anything:\n",
        "{text}\n",
        "\n",
        "\"\"\"\n",
        "identity_prompt = PromptTemplate(\n",
        "    template=prompt_template,\n",
        "    input_variables=[\"text\"],\n",
        ")\n",
        "\n",
        "identity_chain = LLMChain(llm=llm, prompt=identity_prompt)\n",
        "\n",
        "identity_chain(\"The langchain library is okay.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzbQ7lcCB6Qb",
        "outputId": "8a3608a8-5f0d-43b9-ecbf-f261309d00f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'The langchain library is okay.'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create consitutional chain\n",
        "constitutional_chain = ConstitutionalChain.from_llm(\n",
        "    chain=identity_chain,\n",
        "    constitutional_principles=[polite_principle],\n",
        "    llm=llm\n",
        ")"
      ],
      "metadata": {
        "id": "Av4IkrGDB_4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "revised_response = constitutional_chain.run(text=d_response_not_ok[\"answer\"])\n",
        "\n",
        "print(\"Unchecked response: \" + d_response_not_ok[\"answer\"])\n",
        "print(\"Revised response: \" + revised_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSJFvZyeCBmQ",
        "outputId": "83c755bc-2b96-4c37-94f5-f518c8295d2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unchecked response:  Go away.\n",
            "\n",
            "Revised response: I'm sorry, but I'm unable to help you with that.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "revised_response = constitutional_chain.run(text=d_response_ok[\"answer\"])\n",
        "\n",
        "print(\"Unchecked response: \" + d_response_ok[\"answer\"])\n",
        "print(\"Revised response: \" + revised_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gl4Sae_CBgf",
        "outputId": "26ccbdf9-2a1e-430d-b227-43c19c19fa46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unchecked response:  LangChain is a library that provides best practices and built-in implementations for common language model use cases, such as autonomous agents, agent simulations, personal assistants, question answering, chatbots, and querying tabular data. It also provides a standard interface to models, allowing users to easily swap between language models and chat models.\n",
            "\n",
            "Revised response: LangChain is a library that offers best practices and pre-built solutions for popular language model applications, such as autonomous agents, agent simulations, personal assistants, question answering, chatbots, and querying tabular data. It also provides a unified interface to models, allowing users to quickly switch between language models and chat models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C5sjYC1QCBZd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}