addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 14}], "Overview of this lecture:", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 15}], "- We will discuss all the primitives needed to train a model.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 16}], "- We will go bottom-up from tensors to models to optimizers to the training loop.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 17}], "- We will pay close attention to *efficiency* in how we use resources.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 19}], "We will account for two types of *resources*:", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 20}], "- Memory (GB)", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 21}], "- Compute (FLOPs)", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 22}], "Accounting is just about doing basic napkin math.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 24}], "We will actually not go over the Transformer.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 25}], "There are excellent expositions:", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 26}], "- Assignment 1 handout", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 26}], "https://github.com/stanford-cs336/spring2024-assignment1-basics/blob/master/cs336_spring2024_assignment1_basics.pdf", {"color": "gray"})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 27}], "- Mathematical description", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 27}], "https://johnthickstun.com/docs/transformers.pdf", {"color": "gray"})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 28}], "- Illustrated Transformer", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 28}], "http://jalammar.github.io/illustrated-transformer/", {"color": "gray"})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 29}], "- Illustrated GPT-2", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 29}], "https://jalammar.github.io/illustrated-gpt2/", {"color": "gray"})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 30}], "Instead, we'll work with simpler models.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 32}], "What knowledge to take away", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 33}], "- Mechanics: straightforward", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 34}], "- Mindset: important to do it", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 35}], "- Intuitions: low-level stuff does transfer to real settings (matrix multiplication), Assignment 1 will look at actual Transformers, but when look at large models, need multi-GPU, need to think about communication", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 39}], "## Memory accounting", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 40}, {"name": "tensors_basics", "filename": "lecture_02.py", "lineno": 65}], "https://pytorch.org/docs/stable/tensors.html", {"color": "gray"})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 40}, {"name": "tensors_basics", "filename": "lecture_02.py", "lineno": 67}], "Tensors are the basic building block for storing everything: parameters, gradients, optimizer state, data, activations.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 40}, {"name": "tensors_basics", "filename": "lecture_02.py", "lineno": 70}], "You can create tensors in multiple ways:", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 40}, {"name": "tensors_basics", "filename": "lecture_02.py", "lineno": 79}], "Don't initialize the values (to save compute):", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 40}, {"name": "tensors_basics", "filename": "lecture_02.py", "lineno": 81}], "...because you want to use some custom logic to set the values later", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 41}, {"name": "tensors_memory", "filename": "lecture_02.py", "lineno": 86}], "Almost everything (parameters, gradients, activations, optimizer states) are stored as floating point numbers.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 41}, {"name": "tensors_memory", "filename": "lecture_02.py", "lineno": 89}], "## float32", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 41}, {"name": "tensors_memory", "filename": "lecture_02.py", "lineno": 89}], "https://en.wikipedia.org/wiki/Single-precision_floating-point_format", {"color": "gray"})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 41}, {"name": "tensors_memory", "filename": "lecture_02.py", "lineno": 90}], "The float32 data type (also known as fp32 or single precision) is the default.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 41}, {"name": "tensors_memory", "filename": "lecture_02.py", "lineno": 91}], "Traditionally, in scientific computing, float32 is the baseline; you could use double precision (float64) in some cases.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 41}, {"name": "tensors_memory", "filename": "lecture_02.py", "lineno": 93}], "In deep learning, you can be a lot sloppier.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 41}, {"name": "tensors_memory", "filename": "lecture_02.py", "lineno": 95}], "Let's examine memory usage of these tensors.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 41}, {"name": "tensors_memory", "filename": "lecture_02.py", "lineno": 96}], "Memory is determined by the (i) number of values and (ii) data type of each value.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 41}, {"name": "tensors_memory", "filename": "lecture_02.py", "lineno": 104}], "One matrix in the feedforward layer of GPT-3:", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 41}, {"name": "tensors_memory", "filename": "lecture_02.py", "lineno": 106}], "...which is a lot!", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 41}, {"name": "tensors_memory", "filename": "lecture_02.py", "lineno": 108}], "## float16", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 41}, {"name": "tensors_memory", "filename": "lecture_02.py", "lineno": 108}], "https://en.wikipedia.org/wiki/Half-precision_floating-point_format", {"color": "gray"})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 41}, {"name": "tensors_memory", "filename": "lecture_02.py", "lineno": 109}], "The float16 data type (also known as fp16 or half precision) cuts down the memory.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 41}, {"name": "tensors_memory", "filename": "lecture_02.py", "lineno": 112}], "However, the dynamic range (especially for small numbers) isn't great.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 41}, {"name": "tensors_memory", "filename": "lecture_02.py", "lineno": 115}], "If this happens when you train, you can get instability.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 41}, {"name": "tensors_memory", "filename": "lecture_02.py", "lineno": 117}], "## bfloat16", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 41}, {"name": "tensors_memory", "filename": "lecture_02.py", "lineno": 117}], "https://en.wikipedia.org/wiki/Bfloat16_floating-point_format", {"color": "gray"})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 41}, {"name": "tensors_memory", "filename": "lecture_02.py", "lineno": 118}], "Google Brian developed bfloat (brain floating point) in 2018 to address this issue.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 41}, {"name": "tensors_memory", "filename": "lecture_02.py", "lineno": 119}], "bfloat16 uses the same memory as float16 but has the same dynamic range as float32!", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 41}, {"name": "tensors_memory", "filename": "lecture_02.py", "lineno": 120}], "The only catch is that the resolution is worse, but this matters less for deep learning.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 41}, {"name": "tensors_memory", "filename": "lecture_02.py", "lineno": 125}], "Let's compare the dynamic ranges and memory usage of the different data types:", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 41}, {"name": "tensors_memory", "filename": "lecture_02.py", "lineno": 129}], "finfo(resolution=1e-06, min=-3.40282e+38, max=3.40282e+38, eps=1.19209e-07, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=float32)", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 41}, {"name": "tensors_memory", "filename": "lecture_02.py", "lineno": 130}], "finfo(resolution=0.001, min=-65504, max=65504, eps=0.000976562, smallest_normal=6.10352e-05, tiny=6.10352e-05, dtype=float16)", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 41}, {"name": "tensors_memory", "filename": "lecture_02.py", "lineno": 131}], "finfo(resolution=0.01, min=-3.38953e+38, max=3.38953e+38, eps=0.0078125, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=bfloat16)", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 41}, {"name": "tensors_memory", "filename": "lecture_02.py", "lineno": 133}], "In 2022, FP8 was standardized, motivated by machine learning workloads.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 41}, {"name": "tensors_memory", "filename": "lecture_02.py", "lineno": 134}], "https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/fp8_primer.html", {"color": "gray"})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 41}, {"name": "tensors_memory", "filename": "lecture_02.py", "lineno": 135}], "100s support two variants of FP8: E4M3 (range [-448, 448]) and E5M2 ([-57344, 57344]).", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 41}, {"name": "tensors_memory", "filename": "lecture_02.py", "lineno": 136}], "Reference:", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 41}, {"name": "tensors_memory", "filename": "lecture_02.py", "lineno": 136}], "https://arxiv.org/pdf/2209.05433.pdf", {"color": "gray"})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 41}, {"name": "tensors_memory", "filename": "lecture_02.py", "lineno": 138}], "- Training with float32 works, but requires lots of memory.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 41}, {"name": "tensors_memory", "filename": "lecture_02.py", "lineno": 139}], "- Training with fp8, float16 and even bfloat16 is risky, and you can get instability.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 41}, {"name": "tensors_memory", "filename": "lecture_02.py", "lineno": 140}], "- Solution (later): use mixed precision training", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 41}, {"name": "tensors_memory", "filename": "lecture_02.py", "lineno": 140}], "<function mixed_precision_training at 0x1502c592d6c0>", {"color": "gray"})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 43}], "## Compute accounting", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 44}, {"name": "tensors_on_gpus", "filename": "lecture_02.py", "lineno": 144}], "By default, tensors are stored in CPU memory.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 44}, {"name": "tensors_on_gpus", "filename": "lecture_02.py", "lineno": 148}], "However, in order to take advantage of the massive parallelism of GPUs, we need to move them to GPU memory.", {})
addImage([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 44}, {"name": "tensors_on_gpus", "filename": "lecture_02.py", "lineno": 150}], "https://www.researchgate.net/publication/338984158/figure/fig2/AS:854027243900928@1580627370716/Communication-between-host-CPU-and-GPU.png", {"width": "50.0%"})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 44}, {"name": "tensors_on_gpus", "filename": "lecture_02.py", "lineno": 152}], "Let's first see if we have any GPUs.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 44}, {"name": "tensors_on_gpus", "filename": "lecture_02.py", "lineno": 157}], "We have 8 GPUs.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 44}, {"name": "tensors_on_gpus", "filename": "lecture_02.py", "lineno": 160}], "Device 0: _CudaDeviceProperties(name='NVIDIA H100 80GB HBM3', major=9, minor=0, total_memory=81230MB, multi_processor_count=132)", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 44}, {"name": "tensors_on_gpus", "filename": "lecture_02.py", "lineno": 160}], "Device 1: _CudaDeviceProperties(name='NVIDIA H100 80GB HBM3', major=9, minor=0, total_memory=81230MB, multi_processor_count=132)", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 44}, {"name": "tensors_on_gpus", "filename": "lecture_02.py", "lineno": 160}], "Device 2: _CudaDeviceProperties(name='NVIDIA H100 80GB HBM3', major=9, minor=0, total_memory=81230MB, multi_processor_count=132)", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 44}, {"name": "tensors_on_gpus", "filename": "lecture_02.py", "lineno": 160}], "Device 3: _CudaDeviceProperties(name='NVIDIA H100 80GB HBM3', major=9, minor=0, total_memory=81230MB, multi_processor_count=132)", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 44}, {"name": "tensors_on_gpus", "filename": "lecture_02.py", "lineno": 160}], "Device 4: _CudaDeviceProperties(name='NVIDIA H100 80GB HBM3', major=9, minor=0, total_memory=81230MB, multi_processor_count=132)", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 44}, {"name": "tensors_on_gpus", "filename": "lecture_02.py", "lineno": 160}], "Device 5: _CudaDeviceProperties(name='NVIDIA H100 80GB HBM3', major=9, minor=0, total_memory=81230MB, multi_processor_count=132)", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 44}, {"name": "tensors_on_gpus", "filename": "lecture_02.py", "lineno": 160}], "Device 6: _CudaDeviceProperties(name='NVIDIA H100 80GB HBM3', major=9, minor=0, total_memory=81230MB, multi_processor_count=132)", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 44}, {"name": "tensors_on_gpus", "filename": "lecture_02.py", "lineno": 160}], "Device 7: _CudaDeviceProperties(name='NVIDIA H100 80GB HBM3', major=9, minor=0, total_memory=81230MB, multi_processor_count=132)", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 44}, {"name": "tensors_on_gpus", "filename": "lecture_02.py", "lineno": 163}], "GPU memory used: 0", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 44}, {"name": "tensors_on_gpus", "filename": "lecture_02.py", "lineno": 165}], "Move the tensor to GPU memory (device 0).", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 44}, {"name": "tensors_on_gpus", "filename": "lecture_02.py", "lineno": 169}], "Create a tensor directly on the GPU:", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 44}, {"name": "tensors_on_gpus", "filename": "lecture_02.py", "lineno": 173}], "GPU memory used (for y and z): 8192", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 178}], "Most tensors are created from performing operations on other tensors.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 179}], "Each operation has some memory and compute consequence.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 181}], "## Storage", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 183}], "Pytorch tensors are really pointers into allocated memory with metadata describing how to get to any element of the tensor.", {})
addImage([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 185}], "https://martinlwx.github.io/img/2D_tensor_strides.png", {"width": "50.0%"})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 186}], "https://pytorch.org/docs/stable/generated/torch.Tensor.stride.html", {"color": "gray"})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 192}], "To go to the next row (dim 0), skip 3 elements.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 195}], "To go to the next column (dim 1), skip 1 element.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 198}], "## Slicing and dicing", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 205}], "Many operations simply provide a different *view* of the tensor.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 206}], "This does not make a copy, and therefore mutations in one tensor affects the other.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 223}], "Check that mutating x also mutates y.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 227}], "Note that some views are non-contiguous entries, which means that further views aren't possible.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 238}], "One can use reshape, which behaves like view if a copy is not needed...", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 242}], "...or else makes a copy (different storage) if needed.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 245}], "Views are free, copies take both (additional) memory and compute.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 247}], "Now for the operations that make a copy...", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 249}], "# Elementwise operations", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 250}], "These operations apply some operation to each element of the tensor and return a (new) tensor of the same shape.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 261}], "## Dropout: zero out each element of the tensor with probability p.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 265}], "Note that dropout rescales so the mean is unchanged", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 268}], "0.3 of the elements should be now zero", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 271}], "`triu` takes the upper triangular part of a matrix.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 272}], "This is useful for computing an causal attention mask, where M[i, j] is the contribution of i to j.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 280}], "## Aggregate operations", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 287}], "By default, mean aggregates over the entire matrix.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 290}], "We can aggregate only over a subset of the dimensions by specifying `dim`.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 293}], "Variance has the same form factor as mean.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 296}], "## Batching", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 298}], "As a general rule, matrix multiplications are very optimized, so the more we can build up things into a single matrix operation, the better.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 300}], "The `stack` operation adds a new dimension indexing the tensor we're stacking.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 301}], "You can use `stack` given a set of data points, create a batch dimension.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 317}], "The `cat` operation concatenates two tensors along some dimension and does not add another dimension", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 320}], "This is useful for combining batching multiple matrix operations (e.g., Q, K, V in attention).", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 333}], "You can also unbatch using `tensor_split`.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 338}], "Squeezing and unsqueezing simply add or a remove a dimension.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 341}], "Unsqueeze adds a dimension.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 344}], "Squeeze removes a dimension.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 347}], "## Matrix multiplication", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 349}], "Finally, the bread and butter of deep learning: matrix multiplication.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 45}, {"name": "tensor_operations", "filename": "lecture_02.py", "lineno": 350}], "Note that the first matrix could have an dimensions (batch, sequence length).", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 358}], "Having gone through all the operations, let us examine their computational cost.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 360}], "A floating-point operation (FLOP) is a basic operation like addition (x + y) or multiplication (x y).", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 363}], "Two terribly confusing acronyms (prounounced the same!):", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 364}], "- FLOPs: floating-point operations (measure of computation done)", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 365}], "- FLOP/s: floating-point operations per second (also written as FLOPS), which is used to measure the speed of hardware.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 367}], "By default, we will talk about FLOPs.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 369}], "## Intuitions", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 371}], "Training GPT-3 took 3.14e23 FLOPs", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 371}], "https://lambdalabs.com/blog/demystifying-gpt-3", {"color": "gray"})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 372}], "Training GPT-4 is speculated to take 2e25 FLOPs", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 372}], "https://patmcguinness.substack.com/p/gpt-4-details-revealed", {"color": "gray"})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 373}], "US executive order: any foundation model trained with >= 1e26 FLOPs must be reported to the government", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 375}], "A100 has a peak performance of 312 teraFLOP/s (3.12e14)", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 376}], "312000000000000.0", {"color": "gray"})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 377}], "8 A100s for 2 weeks: 1.5e21 FLOPs", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 380}], "## Linear model", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 381}], "As motivation, suppose you have a linear model.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 382}], "- We have n points", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 383}], "- Each point is d-dimsional", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 384}], "- The linear model maps each d-dimensional vector to a k outputs", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 393}], "We have one multiplication (x[i][j] * w[j][k]) and one addition per (i, j, k) triple.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 395}], "Therefore the FLOPs is: 8796093022208", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 397}], "## FLOPs of other operations", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 398}], "- Elementwise operation on a m x n matrix requires O(m n) FLOPs.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 399}], "- Addition of two m x n matrices requires m n FLOPs.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 400}], "In general, no other operation that you'd encounter in deep learning is as expensive as matrix multiplication for large enough matrices.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 403}], "Interpretation:", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 404}], "- B is the number of data points", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 405}], "- (D K) is the number of parameters", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 406}], "- FLOPs for forward pass is 2 (# tokens) (# parameters)", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 407}], "It turns out this generalizes to Transformers.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 409}], "How do our FLOPs calculations translate to wall-clock time (seconds)?", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 410}], "Let us time it!", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 413}], "Actual FLOPs/sec (float32): 53916252385557.16", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 415}], "Each GPU has a specification sheet that reports the peak performance.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 416}], "- A100", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 416}], "https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf", {"color": "gray"})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 417}], "- H100", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 417}], "https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet", {"color": "gray"})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 418}], "Note that the FLOP/s depends heavily on the data type!", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 420}], "Promised FLOPs/sec (float32): 67500000000000.0", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 422}], "## Model FLOPs utilization (MFU)", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 424}], "Definition: (actual FLOP/s) / (promised FLOP/s) [ignore communication/overhead]", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 426}], "MFU (float32): 0.7987592946008467", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 427}], "Usually, MFU of >= 0.5 is quite good (and will be higher if matmuls dominate)", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 429}], "Let's do it with bfloat16:", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 435}], "Actual FLOPs/sec (bfloat16): 463852122833734.25", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 438}], "Promised FLOPs/sec (bfloat16): 989500000000000.0", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 441}], "MFU (bfloat16): 0.4687742524848249", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 442}], "Note: comparing bfloat16 to float32, the actual FLOP/s is higher.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 443}], "The MFU here is rather low, probably because the promised FLOPs is optimistic (and seems to rely on sparsity, which we don't have).", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 446}], "Summary", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 447}], "- Matrix multiplications dominate: (2 m n p) FLOPs", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 448}], "- FLOP/s depends on hardware (H100 >> A100) and data type (bfloat16 >> float32", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 46}, {"name": "tensor_operations_flops", "filename": "lecture_02.py", "lineno": 450}], "- Model FLOPs utilization (MFU): (actual FLOP/s) / (promised FLOP/s)", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 47}, {"name": "gradients_basics", "filename": "lecture_02.py", "lineno": 454}], "So far, we've constructed tensors (which correspond to either parameters or data) and passed them through operations (forward).", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 47}, {"name": "gradients_basics", "filename": "lecture_02.py", "lineno": 456}], "Now, we're going to compute the gradient (backward).", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 47}, {"name": "gradients_basics", "filename": "lecture_02.py", "lineno": 458}], "As a simple example, let's consider the simple linear model: y = 0.5 (x * w - 5)^2", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 47}, {"name": "gradients_basics", "filename": "lecture_02.py", "lineno": 461}], "Forward pass: compute loss", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 47}, {"name": "gradients_basics", "filename": "lecture_02.py", "lineno": 469}], "Backward pass: compute gradients", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 48}, {"name": "gradients_flops", "filename": "lecture_02.py", "lineno": 478}], "Let us do count the FLOPs for computing gradients.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 48}, {"name": "gradients_flops", "filename": "lecture_02.py", "lineno": 480}], "Revisit our linear model", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 48}, {"name": "gradients_flops", "filename": "lecture_02.py", "lineno": 490}], "Model: x --w1--> h1 --w2--> h2 -> loss", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 48}, {"name": "gradients_flops", "filename": "lecture_02.py", "lineno": 495}], "Recall the number of forward FLOPs:", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 48}, {"name": "gradients_flops", "filename": "lecture_02.py", "lineno": 495}], "<function tensor_operations_flops at 0x1502c592ca60>", {"color": "gray"})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 48}, {"name": "gradients_flops", "filename": "lecture_02.py", "lineno": 496}], "- Multiply x[i][j] * w1[j][k]", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 48}, {"name": "gradients_flops", "filename": "lecture_02.py", "lineno": 497}], "- Add to h1[i][k]", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 48}, {"name": "gradients_flops", "filename": "lecture_02.py", "lineno": 498}], "- Multiply h1[i][j] * w2[j][k]", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 48}, {"name": "gradients_flops", "filename": "lecture_02.py", "lineno": 499}], "- Add to h2[i][k]", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 48}, {"name": "gradients_flops", "filename": "lecture_02.py", "lineno": 502}], "How many FLOPs is running the backward pass?", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 48}, {"name": "gradients_flops", "filename": "lecture_02.py", "lineno": 507}], "Recall model: x --w1--> h1 --w2--> h2 -> loss", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 48}, {"name": "gradients_flops", "filename": "lecture_02.py", "lineno": 509}], "- h1.grad = d loss / d h1", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 48}, {"name": "gradients_flops", "filename": "lecture_02.py", "lineno": 510}], "- h2.grad = d loss / d h2", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 48}, {"name": "gradients_flops", "filename": "lecture_02.py", "lineno": 511}], "- w1.grad = d loss / d w1", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 48}, {"name": "gradients_flops", "filename": "lecture_02.py", "lineno": 512}], "- w2.grad = d loss / d w2", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 48}, {"name": "gradients_flops", "filename": "lecture_02.py", "lineno": 514}], "Focus on the parameter w2.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 48}, {"name": "gradients_flops", "filename": "lecture_02.py", "lineno": 515}], "Invoke the chain rule.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 48}, {"name": "gradients_flops", "filename": "lecture_02.py", "lineno": 519}], "w2.grad[j,k] = sum_i h1[i,j] * h2.grad[i,k]", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 48}, {"name": "gradients_flops", "filename": "lecture_02.py", "lineno": 523}], "For each (i, j, k), multiply and add.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 48}, {"name": "gradients_flops", "filename": "lecture_02.py", "lineno": 526}], "h1.grad[i,j] = sum_k w2[i,j] * h2[i,k]", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 48}, {"name": "gradients_flops", "filename": "lecture_02.py", "lineno": 530}], "For each (i, j, k), multiply and add.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 48}, {"name": "gradients_flops", "filename": "lecture_02.py", "lineno": 533}], "A nice graphical visualization:", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 48}, {"name": "gradients_flops", "filename": "lecture_02.py", "lineno": 534}], "https://medium.com/@dzmitrybahdanau/the-flops-calculus-of-language-model-training-3b19c1f025e4", {"color": "gray"})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 48}, {"name": "gradients_flops", "filename": "lecture_02.py", "lineno": 536}], "This was for just w2 (D*K parameters).", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 48}, {"name": "gradients_flops", "filename": "lecture_02.py", "lineno": 537}], "Can do it for w1 (D*D parameters) as well (though don't need x.grad).", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 48}, {"name": "gradients_flops", "filename": "lecture_02.py", "lineno": 539}], "Putting it togther:", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 48}, {"name": "gradients_flops", "filename": "lecture_02.py", "lineno": 540}], "- Forward pass: 2 (# data points) (# parameters) FLOPs", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 48}, {"name": "gradients_flops", "filename": "lecture_02.py", "lineno": 541}], "- Backward pass: 4 (# data points) (# parameters) FLOPs", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 48}, {"name": "gradients_flops", "filename": "lecture_02.py", "lineno": 542}], "- Total: 6 (# data points) (# parameters) FLOPs", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 50}], "## Models", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 51}, {"name": "module_parameters", "filename": "lecture_02.py", "lineno": 549}], "Model parameters are stored in PyTorch as `nn.Parameter` objects.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 51}, {"name": "module_parameters", "filename": "lecture_02.py", "lineno": 554}], "## Parameter initialization", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 51}, {"name": "module_parameters", "filename": "lecture_02.py", "lineno": 556}], "Let's see what happens.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 51}, {"name": "module_parameters", "filename": "lecture_02.py", "lineno": 560}], "Note that each element of `output` scales as sqrt(num_inputs): 61.6663703918457.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 51}, {"name": "module_parameters", "filename": "lecture_02.py", "lineno": 561}], "Large values can cause gradients to blow up and cause training to be unstable.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 51}, {"name": "module_parameters", "filename": "lecture_02.py", "lineno": 563}], "We want an initialization that is invariant to `hidden_dim`.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 51}, {"name": "module_parameters", "filename": "lecture_02.py", "lineno": 564}], "To do that, we simply rescale by 1/sqrt(num_inputs)", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 51}, {"name": "module_parameters", "filename": "lecture_02.py", "lineno": 567}], "Now each element of `output` is constant: 0.636482834815979.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 51}, {"name": "module_parameters", "filename": "lecture_02.py", "lineno": 569}], "Up to a constant, this is Xavier initialization.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 51}, {"name": "module_parameters", "filename": "lecture_02.py", "lineno": 570}], "https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf", {"color": "gray"})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 51}, {"name": "module_parameters", "filename": "lecture_02.py", "lineno": 571}], "https://ai.stackexchange.com/questions/30491/is-there-a-proper-initialization-technique-for-the-weight-matrices-in-multi-head", {"color": "gray"})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 52}, {"name": "module_embeddings", "filename": "lecture_02.py", "lineno": 575}], "Embeddings map token sequences (integer indices) to vectors.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 52}, {"name": "module_embeddings", "filename": "lecture_02.py", "lineno": 581}], "Usually, each batch of data has B sequences of length L, where each element is 0, ..., V-1.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 52}, {"name": "module_embeddings", "filename": "lecture_02.py", "lineno": 586}], "We then map each token to an embedding vector.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 53}, {"name": "custom_model", "filename": "lecture_02.py", "lineno": 592}], "Let's build up a simple deep linear model using `nn.Parameter`.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 53}, {"name": "custom_model", "filename": "lecture_02.py", "lineno": 610}], "Remember to move the model to the GPU.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 53}, {"name": "custom_model", "filename": "lecture_02.py", "lineno": 614}], "Run the model on some data.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 55}], "Training loop and best practices", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 56}, {"name": "note_about_randomness", "filename": "lecture_02.py", "lineno": 686}], "Randomness shows up in many places: parameter initialization, dropout, data ordering, etc.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 56}, {"name": "note_about_randomness", "filename": "lecture_02.py", "lineno": 688}], "For reproducibility, we recommend you always pass in a different random seed for each use of randomness.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 56}, {"name": "note_about_randomness", "filename": "lecture_02.py", "lineno": 690}], "Determinism is particularly useful when debugging, so you can hunt down the bug.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 56}, {"name": "note_about_randomness", "filename": "lecture_02.py", "lineno": 692}], "There are three places to set the random seed, which you should do all at once just to be safe.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 57}, {"name": "data_loading", "filename": "lecture_02.py", "lineno": 708}], "In language modeling, data is a sequence of integers (output by the tokenizer).", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 57}, {"name": "data_loading", "filename": "lecture_02.py", "lineno": 710}], "It is convenient to serialize them as numpy arrays (done by the tokenizer).", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 57}, {"name": "data_loading", "filename": "lecture_02.py", "lineno": 714}], "You can load them back as numpy arrays.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 57}, {"name": "data_loading", "filename": "lecture_02.py", "lineno": 715}], "Don't want to load the entire data into memory at once (LLaMA data is 2.8TB).", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 57}, {"name": "data_loading", "filename": "lecture_02.py", "lineno": 716}], "Use memmap to lazily load only the accessed parts into memory.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 57}, {"name": "data_loading", "filename": "lecture_02.py", "lineno": 720}], "A *data loader* generates a batch of sequences for training.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 57}, {"name": "data_loading", "filename": "lecture_02.py", "lineno": 723}, {"name": "get_batch", "filename": "lecture_02.py", "lineno": 659}], "Sample `batch_size` random positions into `data`.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 57}, {"name": "data_loading", "filename": "lecture_02.py", "lineno": 723}, {"name": "get_batch", "filename": "lecture_02.py", "lineno": 663}], "Index into the data.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 57}, {"name": "data_loading", "filename": "lecture_02.py", "lineno": 723}, {"name": "get_batch", "filename": "lecture_02.py", "lineno": 667}], "## Pinned memory", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 57}, {"name": "data_loading", "filename": "lecture_02.py", "lineno": 723}, {"name": "get_batch", "filename": "lecture_02.py", "lineno": 669}], "By default, CPU tensors are in paged memory. We can explicitly pin.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 57}, {"name": "data_loading", "filename": "lecture_02.py", "lineno": 723}, {"name": "get_batch", "filename": "lecture_02.py", "lineno": 672}], "This allows us to copy `x` from CPU into GPU asynchronously.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 57}, {"name": "data_loading", "filename": "lecture_02.py", "lineno": 723}, {"name": "get_batch", "filename": "lecture_02.py", "lineno": 675}], "This allows us to do two things in parallel (not done here):", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 57}, {"name": "data_loading", "filename": "lecture_02.py", "lineno": 723}, {"name": "get_batch", "filename": "lecture_02.py", "lineno": 676}], "- Fetch the next batch of data into CPU", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 57}, {"name": "data_loading", "filename": "lecture_02.py", "lineno": 723}, {"name": "get_batch", "filename": "lecture_02.py", "lineno": 677}], "- Process `x` on the GPU.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 57}, {"name": "data_loading", "filename": "lecture_02.py", "lineno": 723}, {"name": "get_batch", "filename": "lecture_02.py", "lineno": 679}], "https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/", {"color": "gray"})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 57}, {"name": "data_loading", "filename": "lecture_02.py", "lineno": 723}, {"name": "get_batch", "filename": "lecture_02.py", "lineno": 680}], "https://gist.github.com/ZijiaLewisLu/eabdca955110833c0ce984d34eb7ff39?permalink_comment_id=3417135", {"color": "gray"})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 58}, {"name": "optimizer", "filename": "lecture_02.py", "lineno": 763}], "Recall our deep linear model.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 58}, {"name": "optimizer", "filename": "lecture_02.py", "lineno": 769}], "Let's define the AdaGrad optimizer", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 58}, {"name": "optimizer", "filename": "lecture_02.py", "lineno": 770}], "- AdaGrad = SGD + scale by grad^2", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 58}, {"name": "optimizer", "filename": "lecture_02.py", "lineno": 771}], "- RMSProp = AdaGrad + exponentially decaying weighting", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 58}, {"name": "optimizer", "filename": "lecture_02.py", "lineno": 772}], "- Adam = RMSProp + momentum", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 58}, {"name": "optimizer", "filename": "lecture_02.py", "lineno": 773}], "https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf", {"color": "gray"})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 58}, {"name": "optimizer", "filename": "lecture_02.py", "lineno": 775}], "OrderedDict([('layers.0.weight', tensor([[-0.1253, -0.2169,  0.4244,  0.3460],\n        [-0.1580, -1.0576, -0.1781,  0.2186],\n        [ 0.1750,  0.1541,  0.0599,  0.6188],\n        [ 0.5584, -0.1236, -0.5219, -0.6727]], device='cuda:0')), ('layers.1.weight', tensor([[ 0.2833,  0.3968,  0.2994, -0.7775],\n        [-0.1707,  0.9265,  0.2340, -0.0789],\n        [-0.0867,  0.0917,  0.6947,  0.7932],\n        [ 0.4731, -0.4218,  0.4659,  0.6295]], device='cuda:0')), ('final.weight', tensor([[ 0.2044],\n        [ 0.7107],\n        [ 0.0747],\n        [-0.3354]], device='cuda:0'))])", {"color": "gray"})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 58}, {"name": "optimizer", "filename": "lecture_02.py", "lineno": 777}], "Compute gradients", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 58}, {"name": "optimizer", "filename": "lecture_02.py", "lineno": 784}], "Take a step", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 58}, {"name": "optimizer", "filename": "lecture_02.py", "lineno": 786}], "OrderedDict([('layers.0.weight', tensor([[-0.1353, -0.2269,  0.4344,  0.3560],\n        [-0.1680, -1.0676, -0.1681,  0.2286],\n        [ 0.1650,  0.1441,  0.0699,  0.6288],\n        [ 0.5484, -0.1336, -0.5119, -0.6627]], device='cuda:0')), ('layers.1.weight', tensor([[ 0.2733,  0.3868,  0.2894, -0.7675],\n        [-0.1607,  0.9365,  0.2440, -0.0889],\n        [-0.0967,  0.0817,  0.6847,  0.8032],\n        [ 0.4631, -0.4318,  0.4559,  0.6395]], device='cuda:0')), ('final.weight', tensor([[ 0.1944],\n        [ 0.7207],\n        [ 0.0647],\n        [-0.3454]], device='cuda:0'))])", {"color": "gray"})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 58}, {"name": "optimizer", "filename": "lecture_02.py", "lineno": 788}], "Free up the memory (optional)", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 58}, {"name": "optimizer", "filename": "lecture_02.py", "lineno": 791}], "## Memory", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 58}, {"name": "optimizer", "filename": "lecture_02.py", "lineno": 809}], 496, {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 58}, {"name": "optimizer", "filename": "lecture_02.py", "lineno": 811}], "## Compute (for one step)", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 58}, {"name": "optimizer", "filename": "lecture_02.py", "lineno": 813}], 432, {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 58}, {"name": "optimizer", "filename": "lecture_02.py", "lineno": 815}], "## Transformers", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 58}, {"name": "optimizer", "filename": "lecture_02.py", "lineno": 817}], "The accounting for a Transformer is more complicated, but the same idea.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 58}, {"name": "optimizer", "filename": "lecture_02.py", "lineno": 818}], "Assignment 1 will ask you to do that.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 58}, {"name": "optimizer", "filename": "lecture_02.py", "lineno": 820}], "Blog post describing memory usage for Transformer training", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 58}, {"name": "optimizer", "filename": "lecture_02.py", "lineno": 821}], "https://erees.dev/transformer-memory/", {"color": "gray"})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 58}, {"name": "optimizer", "filename": "lecture_02.py", "lineno": 823}], "Blog post descibing FLOPs for a Transformer:", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 58}, {"name": "optimizer", "filename": "lecture_02.py", "lineno": 824}], "https://www.adamcasson.com/posts/transformer-flops", {"color": "gray"})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 59}, {"name": "train_loop", "filename": "lecture_02.py", "lineno": 828}], "Generate data from linear function with weights (0, 1, 2, ..., D-1).", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 59}, {"name": "train_loop", "filename": "lecture_02.py", "lineno": 836}], "Let's do a basic run", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 59}, {"name": "train_loop", "filename": "lecture_02.py", "lineno": 839}], "Do some hyperparameter tuning", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 60}, {"name": "checkpointing", "filename": "lecture_02.py", "lineno": 869}], "Training language models take a long time and certainly will certainly crash.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 60}, {"name": "checkpointing", "filename": "lecture_02.py", "lineno": 870}], "You don't want to lose all your progress.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 60}, {"name": "checkpointing", "filename": "lecture_02.py", "lineno": 872}], "During training, it is useful to periodically save your model and optimizer state to disk.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 60}, {"name": "checkpointing", "filename": "lecture_02.py", "lineno": 878}], "Save the checkpoint:", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 60}, {"name": "checkpointing", "filename": "lecture_02.py", "lineno": 885}], "Load the checkpoint:", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 61}, {"name": "mixed_precision_training", "filename": "lecture_02.py", "lineno": 890}], "Choice of data type (float32, bfloat16, fp8) have tradeoffs.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 61}, {"name": "mixed_precision_training", "filename": "lecture_02.py", "lineno": 891}], "- Higher precision: more accurate/stable, more memory, more compute", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 61}, {"name": "mixed_precision_training", "filename": "lecture_02.py", "lineno": 892}], "- Lower precision: less accurate/stable, less memory, less compute", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 61}, {"name": "mixed_precision_training", "filename": "lecture_02.py", "lineno": 894}], "How can we get the best of both worlds?", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 61}, {"name": "mixed_precision_training", "filename": "lecture_02.py", "lineno": 896}], "Solution: use float32 by default, but use {bfloat16, fp8} when possible.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 61}, {"name": "mixed_precision_training", "filename": "lecture_02.py", "lineno": 898}], "A concrete plan:", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 61}, {"name": "mixed_precision_training", "filename": "lecture_02.py", "lineno": 899}], "- Use {bfloat16, fp8} for the forward pass (activations).", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 61}, {"name": "mixed_precision_training", "filename": "lecture_02.py", "lineno": 900}], "- Use float32 for the rest (parameters, gradients).", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 61}, {"name": "mixed_precision_training", "filename": "lecture_02.py", "lineno": 902}], "2017 paper on mixed precision training", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 61}, {"name": "mixed_precision_training", "filename": "lecture_02.py", "lineno": 903}], "https://arxiv.org/pdf/1710.03740.pdf", {"color": "gray"})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 61}, {"name": "mixed_precision_training", "filename": "lecture_02.py", "lineno": 905}], "Pytorch has an automatic mixed precision (AMP) library.", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 61}, {"name": "mixed_precision_training", "filename": "lecture_02.py", "lineno": 906}], "https://pytorch.org/docs/stable/amp.html", {"color": "gray"})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 61}, {"name": "mixed_precision_training", "filename": "lecture_02.py", "lineno": 907}], "https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/", {"color": "gray"})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 61}, {"name": "mixed_precision_training", "filename": "lecture_02.py", "lineno": 909}], "NVIDIA's Transformer Engine supports FP8 for linear layers", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 61}, {"name": "mixed_precision_training", "filename": "lecture_02.py", "lineno": 910}], "Paper that uses FP8 more pervasively throughout training", {})
addText([{"name": "lecture_02", "filename": "lecture_02.py", "lineno": 61}, {"name": "mixed_precision_training", "filename": "lecture_02.py", "lineno": 910}], "https://arxiv.org/pdf/2310.18313.pdf", {"color": "gray"})
