{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install langchain langsmith openai pyyaml PyGithub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "import utils\n",
    "\n",
    "# load API tokens for our 3rd party APIs.\n",
    "cci_api_key = utils.get_circle_api_key()\n",
    "gh_api_key = utils.get_gh_api_key()\n",
    "openai_api_key = utils.get_openai_api_key()\n",
    "\n",
    "# set up our github branch\n",
    "course_repo = utils.get_repo_name()\n",
    "course_branch = utils.get_branch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LLMs and evals\n",
    "LLM based applications introduce new problems to software testing: non-deterministic output and subjectivity.\n",
    "\n",
    "* LLMs work by learning a probability distribution over training data and then predicting the next token to output in a sequence. This introduces randomness, which makes output sound more human like but also makes it difficult to predict what the LLM will say.\n",
    "* Because LLMs deal with text output, there's also more subjectivity. If you're application produces summaries, there are multiple \"good\" ways to answer a question. Similarly with code, there's no universally correct way to write a function.\n",
    "\n",
    "It's also important to note that LLMs can also product harmful, toxic, or offensive content. This is a new problem for application testing compared to traditional software where outputs can be constrained by the programmer.\n",
    "\n",
    "To deal with these new testing challenges AI Researchers developed the concept of \"evals\" to assess how well LLMs do at different tasks. There are many common datasets for different tasks including MMLU, hellaswag, and HumanEval. LLMs are tested on different datasets so researchers have a point of comparison between models.\n",
    "\n",
    "Standard benchmarks are a good starting point, but they don't cover the specifics of __your__ application. In order to assess how well your agents, chatbots, and assistants perform you need to write evaluations for your application's use cases.\n",
    "\n",
    "## What you will learn\n",
    "In this course we'll show you how to write those evaluations and automate running them in CircleCI. This will give you a way to see how well your application performs while you build as well as give you a set of tests to run as your application changes.\n",
    "\n",
    "\n",
    "* We'll give you a framework to think about:\n",
    "  * What to evaluate - starting with basic evals, using LLM based evals to grade your application output, and dealing with difficult to automate cases\n",
    "  * When to run evaluations:\n",
    "    * Starting with evals when you make application changes\n",
    "    * Running evals before you deploy\n",
    "    * Periodically evaluating your entire application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The sample Application\n",
    "\n",
    "We are going to build a AI powered quiz generator.\n",
    "\n",
    "The app will have a data set of facts categorized across Art, Science, and Geography. The facts are grouped into specific subjects. Some subjects apply to multiple categories, for example Paris is home to many great works of art and scientific inventions.\n",
    "\n",
    "The user will ask our bot to write a quiz about a given topic and get back a set of questions. We'll write evaluations to check that the bot is using the appropriate facts and only using facts in our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts                import ChatPromptTemplate\n",
    "from langchain.chat_models            import ChatOpenAI\n",
    "from langchain.schema.output_parser   import StrOutputParser\n",
    "\n",
    "delimiter = \"####\"\n",
    "\n",
    "# Note: Our topics are stored in the prompt. In a real application you might use a database\n",
    "# or files to hold the data.\n",
    "system_message = f\"\"\"\n",
    "Follow these steps to generate a customized quiz for the user.\n",
    "The question will be delimited with four hashtags i.e {delimiter}\n",
    "\n",
    "Step 1:{delimiter} First identify the category user is asking about from the following list:\n",
    "* Geography\n",
    "* Science\n",
    "* Art\n",
    "\n",
    "Step 2:{delimiter} Determine the subjects to generate questions about. The list of topics are below:\n",
    "1. Subject: Leonardo DaVinci\n",
    "   Categories: Art, Science\n",
    "   Facts:\n",
    "    - Painted the Mona Lisa\n",
    "    - Studied zoology, anatomy, geology, optics\n",
    "    - Designed a flying machine\n",
    "  \n",
    "2. Subject: Paris\n",
    "   Categories: Art, Geography\n",
    "   Facts:\n",
    "    - Location of the Louvre, the museum where the Mona Lisa is displayed\n",
    "    - Capital of France\n",
    "    - Most populous city in France\n",
    "    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie\n",
    "\n",
    "3. Subject: Telescopes\n",
    "   Category: Science\n",
    "   Facts:\n",
    "    - Device to observe different objects\n",
    "    - The first refracting telescopes were invented in the Netherlands in the 17th Century\n",
    "    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\n",
    "\n",
    "4. Subject: Starry Night\n",
    "   Category: Art\n",
    "   Facts:\n",
    "    - Painted by Vincent van Gogh in 1889\n",
    "    - Captures the east-facing view of van Gogh's room in Saint-RÃ©my-de-Provence\n",
    "\n",
    "5. Subject: Physics\n",
    "   Category: Science\n",
    "   Facts:\n",
    "    - The sun doesn't change color during sunset.\n",
    "    - Water slows the speed of light\n",
    "    - The Eiffel Tower in Paris is taller in the summer than the winter due to expansion of the metal.\n",
    "\n",
    "Pick up to two subjects that fit the user's category. \n",
    "\n",
    "Step 3:{delimiter} Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.\n",
    "Use the following format:\n",
    "Question 1:{delimiter} <question 1>\n",
    "\n",
    "Question 2:{delimiter} <question 2>\n",
    "\n",
    "Question 3:{delimiter} <question 3>\n",
    "\"\"\"\n",
    "\n",
    "def assistant_chain():\n",
    "  human_template  = \"{question}\"\n",
    "\n",
    "  chat_prompt = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", system_message),\n",
    "      (\"human\", human_template),\n",
    "  ])\n",
    "  return chat_prompt | ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0) | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some basic evaluations for our assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_science_facts():\n",
    "  assistant = assistant_chain()\n",
    "  question  = \"Generate a quiz about science.\"\n",
    "  answer = assistant.invoke({\"question\": question})\n",
    "  expected_subjects = [\"davinci\", \"telescope\", \"physics\", \"curie\"]\n",
    "  print(answer)\n",
    "  assert any(subject in answer.lower() for subject in expected_subjects), f\"Expected the assistant questions to include '{expected_subjects}', but it did not\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! Here are three science questions for you:\n",
      "\n",
      "Question 1:#### What is the largest telescope in space called and what material is its mirror made of?\n",
      "\n",
      "Question 2:#### True or False: Water slows down the speed of light.\n",
      "\n",
      "Question 3:#### What did Marie and Pierre Curie discover in Paris?\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Expected the assistant questions to include '['mona list']', but it did not",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/michaelwebster/workspace/sc-circleci-c1/L1/setup-ci.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/michaelwebster/workspace/sc-circleci-c1/L1/setup-ci.ipynb#X62sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m evaluate_science_facts()\n",
      "\u001b[1;32m/Users/michaelwebster/workspace/sc-circleci-c1/L1/setup-ci.ipynb Cell 7\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/michaelwebster/workspace/sc-circleci-c1/L1/setup-ci.ipynb#X62sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m expected_subjects \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mmona list\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/michaelwebster/workspace/sc-circleci-c1/L1/setup-ci.ipynb#X62sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(answer)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/michaelwebster/workspace/sc-circleci-c1/L1/setup-ci.ipynb#X62sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39many\u001b[39m(subject \u001b[39min\u001b[39;00m answer\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m subject \u001b[39min\u001b[39;00m expected_subjects), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected the assistant questions to include \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mexpected_subjects\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, but it did not\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Expected the assistant questions to include '['mona list']', but it did not"
     ]
    }
   ],
   "source": [
    "evaluate_science_facts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's write a failing test case.\n",
    "\n",
    "We'll ask our application to answer a question it doesn't have information about. We want the application to decline to answer rather than make up questions, but we don't have any restrictions in our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_geography_facts():\n",
    "  assistant = assistant_chain()\n",
    "  question  = \"Help me create a quiz about Rome\"\n",
    "  answer = assistant.invoke({\"question\": question})\n",
    "  print(answer)\n",
    "  # We'll look for a substring of the message the bot prints when it gets a question about any\n",
    "  decline_response = \"I'm sorry\"\n",
    "  assert decline_response.lower() in answer.lower(), f\"Expected the bot to decline with '{decline_response}' got {answer}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! Since you mentioned Rome, we will focus on the category of Geography. Let's generate some questions about Rome for your quiz.\n",
      "\n",
      "Question 1:####\n",
      "What is the capital city of Italy?\n",
      "\n",
      "Question 2:####\n",
      "Which famous ancient structure in Rome was used for gladiatorial contests and other public spectacles?\n",
      "\n",
      "Question 3:####\n",
      "What is the name of the river that runs through Rome?\n",
      "\n",
      "Feel free to use these questions for your quiz about Rome!\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Expected the bot to decline with 'I'm sorry' got Great! Since you mentioned Rome, we will focus on the category of Geography. Let's generate some questions about Rome for your quiz.\n\nQuestion 1:####\nWhat is the capital city of Italy?\n\nQuestion 2:####\nWhich famous ancient structure in Rome was used for gladiatorial contests and other public spectacles?\n\nQuestion 3:####\nWhat is the name of the river that runs through Rome?\n\nFeel free to use these questions for your quiz about Rome!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/michaelwebster/workspace/sc-circleci-c1/L1/setup-ci.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/michaelwebster/workspace/sc-circleci-c1/L1/setup-ci.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m evaluate_geography_facts()\n",
      "\u001b[1;32m/Users/michaelwebster/workspace/sc-circleci-c1/L1/setup-ci.ipynb Cell 11\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/michaelwebster/workspace/sc-circleci-c1/L1/setup-ci.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# We'll look for a substring of the message the bot prints when it gets a question about any\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/michaelwebster/workspace/sc-circleci-c1/L1/setup-ci.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m decline_response \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mI\u001b[39m\u001b[39m'\u001b[39m\u001b[39mm sorry\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/michaelwebster/workspace/sc-circleci-c1/L1/setup-ci.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39massert\u001b[39;00m decline_response\u001b[39m.\u001b[39mlower() \u001b[39min\u001b[39;00m answer\u001b[39m.\u001b[39mlower(), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected the bot to decline with \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mdecline_response\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m got \u001b[39m\u001b[39m{\u001b[39;00manswer\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Expected the bot to decline with 'I'm sorry' got Great! Since you mentioned Rome, we will focus on the category of Geography. Let's generate some questions about Rome for your quiz.\n\nQuestion 1:####\nWhat is the capital city of Italy?\n\nQuestion 2:####\nWhich famous ancient structure in Rome was used for gladiatorial contests and other public spectacles?\n\nQuestion 3:####\nWhat is the name of the river that runs through Rome?\n\nFeel free to use these questions for your quiz about Rome!"
     ]
    }
   ],
   "source": [
    "evaluate_geography_facts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try and fix the prompt\n",
    "Try and update the prompt so that it handles the case where the user asks about an unsupported category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: Any code you write will be saved to a public GitHub repository. If you want to use a private repository, then you will need to create your own GitHub and CircleCI API key's**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running evaluations in a CircleCI pipeline\n",
    "\n",
    "Now that you have a set of evaluations, we'll show you how to automate running them in CircleCI.\n",
    "\n",
    "For our first round of evaluations we'll focus on adding basic checks to make sure our assistant is being setup properly and producing valid results.\n",
    "\n",
    "From there, we'll add more rigorous evals that we run prior to release and finally evals that we want to run periodically to smoke test the entire application.\n",
    "\n",
    "## Notes\n",
    "* For this notebook, we are using the GitHub API to commit code. In your normal workflow you would use the `git` or `gh` command line tools or a GitHub GUI applications.\n",
    "* As a reminder, any code you push to GitHub will be publicly visible.\n",
    "* We've updated the application prompt to decline generating quizzes for topics that there is no information for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "from langchain.prompts                import ChatPromptTemplate\n",
    "from langchain.chat_models            import ChatOpenAI\n",
    "from langchain.schema.output_parser   import StrOutputParser\n",
    "\n",
    "delimiter = \"####\"\n",
    "\n",
    "quiz_information_bank = \"\"\"1. Subject: Leonardo DaVinci\n",
    "   Categories: Art, Science\n",
    "   Facts:\n",
    "    - Painted the Mona Lisa\n",
    "    - Studied zoology, anatomy, geology, optics\n",
    "    - Designed a flying machine\n",
    "  \n",
    "2. Subject: Paris\n",
    "   Categories: Art, Geography\n",
    "   Facts:\n",
    "    - Location of the Louvre, the museum where the Mona Lisa is displayed\n",
    "    - Capital of France\n",
    "    - Most populous city in France\n",
    "    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie\n",
    "\n",
    "3. Subject: Telescopes\n",
    "   Category: Science\n",
    "   Facts:\n",
    "    - Device to observe different objects\n",
    "    - The first refracting telescopes were invented in the Netherlands in the 17th Century\n",
    "    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\n",
    "\n",
    "4. Subject: Starry Night\n",
    "   Category: Art\n",
    "   Facts:\n",
    "    - Painted by Vincent van Gogh in 1889\n",
    "    - Captures the east-facing view of van Gogh's room in Saint-RÃ©my-de-Provence\n",
    "\n",
    "5. Subject: Physics\n",
    "   Category: Science\n",
    "   Facts:\n",
    "    - The sun doesn't change color during sunset.\n",
    "    - Water slows the speed of light\n",
    "    - The Eiffel Tower in Paris is taller in the summer than the winter due to expansion of the metal.\n",
    "\"\"\"\n",
    "\n",
    "system_message = f\"\"\"\n",
    "Follow these steps to generate a customized quiz for the user.\n",
    "The question will be delimited with four hashtags i.e {delimiter}\n",
    "\n",
    "Step 1:{delimiter} First identify the category user is asking about from the following list:\n",
    "* Geography\n",
    "* Science\n",
    "* Art\n",
    "\n",
    "Step 2:{delimiter} Determine the subjects to generate questions about. The list of topics are below:\n",
    "\n",
    "{quiz_information_bank}\n",
    "\n",
    "Pick up to two subjects that fit the user's category.\n",
    "\n",
    "Step 3:{delimiter} Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.\n",
    "Only reference facts in the included list of topics.\n",
    "Use the following format:\n",
    "Question 1:{delimiter} <question 1>\n",
    "\n",
    "Question 2:{delimiter} <question 2>\n",
    "\n",
    "Question 3:{delimiter} <question 3>\n",
    "\n",
    "If the user asks about a subject you do not have information about, tell them \"I'm sorry, but I do not have information on that topic.\"\n",
    "\"\"\"\n",
    "\n",
    "def assistant_chain():\n",
    "  human_template  = \"{question}\"\n",
    "\n",
    "  chat_prompt = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", system_message),\n",
    "      (\"human\", human_template),\n",
    "  ])\n",
    "  return chat_prompt | ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0) | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_assistant.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_assistant.py\n",
    "from app import assistant_chain\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "def test_science_quiz():\n",
    "  assistant = assistant_chain()\n",
    "  question  = \"Generate a quiz about science.\"\n",
    "  answer = assistant.invoke({\"question\": question})\n",
    "  expected_subjects = [\"davinci\", \"telescope\", \"physics\", \"curie\"]\n",
    "  print(answer)\n",
    "  assert any(subject.lower() in answer.lower() for subject in expected_subjects), f\"Expected the assistant questions to include '{expected_subjects}', but it did not\"\n",
    "\n",
    "def test_geography_quiz():\n",
    "  assistant = assistant_chain()\n",
    "  question  = \"Generate a quiz about geography.\"\n",
    "  answer = assistant.invoke({\"question\": question})\n",
    "  expected_subjects = [\"paris\", \"france\", \"louvre\"]\n",
    "  print(answer)\n",
    "  assert any(subject.lower() in answer.lower() for subject in expected_subjects), f\"Expected the assistant questions to include '{expected_subjects}', but it did not\"\n",
    "\n",
    "def test_decline_unknown_subjects():\n",
    "  assistant = assistant_chain()\n",
    "  question  = \"Help me create a quiz about Rome\"\n",
    "  answer = assistant.invoke({\"question\": question})\n",
    "  print(answer)\n",
    "  # We'll look for a substring of the message the bot prints when it gets a question about any\n",
    "  decline_response = \"I'm sorry\"\n",
    "  assert decline_response.lower() in answer.lower(), f\"Expected the bot to decline with '{decline_response}' got {answer}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The CircleCI config file\n",
    "Now let's set up our tests to run automatically in CircleCI.\n",
    "\n",
    "For this course, we've created a working CircleCI config file. Let's take a look at the configuration.\n",
    "\n",
    "In the config we will define a **workflow** that describes the test to build, test, and deploy our application. The workflow consists of **jobs** that run each step of our process.\n",
    "\n",
    "In this config, we only have one workflow and one job that will conditionally run tests based on passed in parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version: 2.1\n",
      "orbs:\n",
      "  # The python orb contains a set of prepackaged circleci configuration you can use repeatedly in your configurations files\n",
      "  # Orb commands and jobs help you with common scripting around a language/tool\n",
      "  # so you dont have to copy and paste it everywhere.\n",
      "  # See the orb documentation here: https://circleci.com/developer/orbs/orb/circleci/python\n",
      "  python: circleci/python@2.1.1\n",
      "\n",
      "parameters:\n",
      "  eval-mode:\n",
      "    type: string\n",
      "    default: \"commit\"\n",
      "\n",
      "\n",
      "workflows:\n",
      "  evalaute-app:  # This is the name of the workflow, feel free to change it to better match your workflow.\n",
      "    # Inside the workflow, you define the jobs you want to run.\n",
      "    # For more details on extending your workflow, see the configuration docs: https://circleci.com/docs/2.0/configuration-reference/#workflows\n",
      "    jobs:\n",
      "      - run-evals:\n",
      "          context:\n",
      "            - dl-ai-courses\n",
      "\n",
      "jobs:\n",
      "  # Our main job to run evals.\n",
      "  # Based on parameters we will run evals on every commit, a pre-release set of evals, or all of our evaluations\n",
      "  run-evals:\n",
      "    docker:\n",
      "      - image: cimg/python:3.10.5\n",
      "    # Checkout the code as the first step. This is a dedicated CircleCI step.\n",
      "    # The python orb's install-packages step will install the dependencies from a Pipfile via Pipenv by default.\n",
      "    # Here we're making sure we use just use the system-wide pip. By default it uses the project root's requirements.txt.\n",
      "    # Then run your tests!\n",
      "    # CircleCI will report the results back to your VCS provider.\n",
      "    \n",
      "    steps:\n",
      "      - checkout\n",
      "      - python/install-packages:\n",
      "          pkg-manager: pip\n",
      "          # app-dir: ~/project/package-directory/  # If your requirements.txt isn't in the root directory.\n",
      "          # pip-dependency-file: test-requirements.txt  # if you have a different name for your requirements file, maybe one that combines your runtime and test requirements.\n",
      "      - when:\n",
      "          condition:\n",
      "            and:\n",
      "              - equal: [\"commit\", << pipeline.parameters.eval-mode >>]\n",
      "          steps:\n",
      "            - run:\n",
      "                name: Run assistant evals.\n",
      "                # We are running pytest with the `-s` flag which includes print output.\n",
      "                # In a real pipeline, you would not print output, but we include it here for teaching purposes.\n",
      "                command: python -m pytest -s test_assistant.py\n",
      "      - when:\n",
      "          condition:\n",
      "            and:\n",
      "              - equal: [\"release\", << pipeline.parameters.eval-mode >>]\n",
      "          steps:\n",
      "            - run:\n",
      "                name: Run release evals.\n",
      "                command: python -m pytest -s test_release_evals.py\n",
      "      - when:\n",
      "          condition:\n",
      "            and:\n",
      "              - equal: [\"full\", << pipeline.parameters.eval-mode >>]\n",
      "          steps:\n",
      "            - run:\n",
      "                name: Run end to end evals.\n",
      "                command: python -m pytest -s test_assistant.py test_release_evals.py\n"
     ]
    }
   ],
   "source": [
    "with open(\"circle_config.yml\") as f:\n",
    "  print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the per-commit evals\n",
    "The evals we have now are quick checks that we run whenever we change our application.\n",
    "\n",
    "Now when we save our code in github, CircleCI will run our tests\n",
    "\n",
    "## Steps\n",
    "To run the evals we will:\n",
    "1. Write our test file in the course notebook\n",
    "2. Push the file to GitHub and run the CircleCI pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading test_assistant.py\n",
      "uploading app.py\n",
      "dl-cci-long-lasting-radar-7 already exists in the repository pushing updated changes\n",
      "Please visit https://app.circleci.com/pipelines/github/mw-courses/cci-dl-ai-course/63\n"
     ]
    }
   ],
   "source": [
    "from utils import push_files, trigger_commit_evals, trigger_release_evals\n",
    "push_files(course_repo, course_branch, [\"app.py\", \"test_assistant.py\"])\n",
    "trigger_commit_evals(course_repo, course_branch, cci_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running pre-release evals\n",
    "Now let's look at running pre-release evals.\n",
    "\n",
    "So far, our evals are meant catch obvious errors in our application. As the application grows though, having a set of good pre-release tests can help catch more subtle regressions.\n",
    "\n",
    "## Steps\n",
    "To run the evals we will:\n",
    "1. Write our test file in the course notebook\n",
    "2. Push the file to GitHub and run the CircleCI pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first model graded eval\n",
    "\n",
    "Evaluating LLM output can be tricky because a \"good response\" to a query is subjective. We could try and write custom rules, like we did for our initial evals to make sure expected data was in the output, but this gets more fragile as an application expands.\n",
    "\n",
    "One approach to checking the output of an LLM is to use another LLM as a grader. This is referred to as \"model graded evaluation\" we'll show a quick example to make sure our model is actually producing output as a quiz.\n",
    "\n",
    "We aren't concerned with the content just yet, just that the LLM is giving back responses that look like a set of questions.\n",
    "\n",
    "We are including a passing and failing case. If you want to see a passing build in CircleCI update the failing case with the expected response before commiting the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_release_evals.py\n",
    "# note, you will need to run the cell to write the app file for these imports to work.\n",
    "from app import system_message, quiz_information_bank, assistant_chain\n",
    "from langchain.prompts                import ChatPromptTemplate\n",
    "from langchain.chat_models            import ChatOpenAI\n",
    "from langchain.schema.output_parser   import StrOutputParser\n",
    "\n",
    "def create_eval_chain(agent_response):\n",
    "  delimiter = \"####\"\n",
    "  eval_system_prompt = f\"\"\"You are an assistant that evaluates whether or not an assistant is producing valid quizzes.\n",
    "  The assistant should be producing output in the format of Question N:{delimiter} <question N>?\"\"\"\n",
    "  \n",
    "  eval_user_message = f\"\"\"You are evaluating a generated quiz based on the context that the assistant uses to create the quiz.\n",
    "  Here is the data:\n",
    "    [BEGIN DATA]\n",
    "    ************\n",
    "    [Response]: {agent_response}\n",
    "    ************\n",
    "    [END DATA]\n",
    "\n",
    "Read the response carefully and determine if it looks like a quiz or test. Do not evaluate if the information is correct\n",
    "only evaluate if the data is in the expected format.\n",
    "\n",
    "Output Y if the response is a quiz, output N if the response does not look like a quiz.\n",
    "\"\"\"\n",
    "  eval_prompt = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", eval_system_prompt),\n",
    "      (\"human\", eval_user_message),\n",
    "  ])\n",
    "\n",
    "  return eval_prompt | ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0) | StrOutputParser()\n",
    "\n",
    "def test_model_graded_eval():\n",
    "  assistant = assistant_chain()\n",
    "  quiz_request = \"Write me a quiz about geography.\"\n",
    "  result = assistant.invoke({\"question\": quiz_request})\n",
    "  print(result)\n",
    "  eval_agent = create_eval_chain(result)\n",
    "  eval_response = eval_agent.invoke({})\n",
    "  assert eval_response == \"Y\"\n",
    "\n",
    "def test_model_graded_eval_should_fail():\n",
    "  # In this test we are using output that will fail the evaluation.\n",
    "  # This is a good way to check your evaluator is behaving as expected\n",
    "  known_bad_result = \"There are lots of interesting facts. Tell me more about what you'd like to know\"\n",
    "  print(known_bad_result)\n",
    "  eval_agent = create_eval_chain(result)\n",
    "  eval_response = eval_agent.invoke({})\n",
    "  assert eval_response == \"Y\", f\"expected failure, asserted the response should be 'Y', got back '{eval_response}'\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading test_release_evals.py\n",
      "dl-cci-long-lasting-radar-7 already exists in the repository pushing updated changes\n",
      "Please visit https://app.circleci.com/pipelines/github/mw-courses/cci-dl-ai-course/65\n"
     ]
    }
   ],
   "source": [
    "from utils import push_files, trigger_release_evals\n",
    "push_files(course_repo, course_branch, [\"test_release_evals.py\"])\n",
    "trigger_release_evals(course_repo, course_branch, cci_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling it together: Running all of our evaluations\n",
    "Finally, we can run our full the set of commit and pre-release evals.\n",
    "\n",
    "You may want to do this to debug the full application or as a periodic check.\n",
    "\n",
    "## Steps\n",
    "To run the evals we will:\n",
    "1. Run our pipeline in CircleCI passing in a parameter to run all evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit https://app.circleci.com/pipelines/github/mw-courses/cci-dl-ai-course/67\n"
     ]
    }
   ],
   "source": [
    "from utils import trigger_full_evals\n",
    "trigger_full_evals(course_repo, course_branch, cci_api_key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
