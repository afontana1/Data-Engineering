{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Question-Answering Agent Tutorial\n",
    "\n",
    "## Overview\n",
    "This tutorial introduces a basic Question-Answering (QA) agent using LangChain and OpenAI's language model. The agent is designed to understand user queries and provide relevant, concise answers.\n",
    "\n",
    "## Motivation\n",
    "In the era of AI-driven interactions, creating a simple QA agent serves as a fundamental stepping stone towards more complex AI systems. This project aims to:\n",
    "- Demonstrate the basics of AI-driven question-answering\n",
    "- Introduce key concepts in building AI agents\n",
    "- Provide a foundation for more advanced agent architectures\n",
    "\n",
    "## Key Components\n",
    "1. **Language Model**: Utilizes OpenAI's GPT model for natural language understanding and generation.\n",
    "2. **Prompt Template**: Defines the structure and context for the agent's responses.\n",
    "3. **LLMChain**: Combines the language model and prompt template for streamlined processing.\n",
    "\n",
    "## Method Details\n",
    "\n",
    "### 1. Setup and Initialization\n",
    "- Import necessary libraries (LangChain, dotenv)\n",
    "- Load environment variables for API key management\n",
    "- Initialize the OpenAI language model\n",
    "\n",
    "### 2. Defining the Prompt Template\n",
    "- Create a template that instructs the AI on its role and response format\n",
    "- Use the PromptTemplate class to structure the input\n",
    "\n",
    "### 3. Creating the LLMChain\n",
    "- Combine the language model and prompt template into an LLMChain\n",
    "- This chain manages the flow from user input to AI response\n",
    "\n",
    "### 4. Implementing the Question-Answering Function\n",
    "- Define a function that takes a user question as input\n",
    "- Use the LLMChain to process the question and generate an answer\n",
    "\n",
    "### 5. User Interaction\n",
    "- In a Jupyter notebook environment, provide cells for:\n",
    "  - Example usage with a predefined question\n",
    "  - Interactive input for user questions\n",
    "\n",
    "## Conclusion\n",
    "This Simple Question-Answering Agent serves as an entry point into the world of AI agents. By understanding and implementing this basic model, you've laid the groundwork for more sophisticated systems. Future enhancements could include:\n",
    "- Adding memory to maintain context across multiple questions\n",
    "- Integrating external knowledge bases for more informed responses\n",
    "- Implementing more complex decision-making processes\n",
    "\n",
    "As you progress through more advanced tutorials in this repository, you'll build upon these fundamental concepts to create increasingly capable and intelligent AI agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=1000, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a helpful AI assistant. Your task is to answer the user's question to the best of your ability.\n",
    "\n",
    "User's question: {question}\n",
    "\n",
    "Please provide a clear and concise answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the get_answer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(question):\n",
    "    \"\"\"\n",
    "    Get an answer to the given question using the QA chain.\n",
    "    \"\"\"\n",
    "    input_variables = {\"question\": question}\n",
    "    response = qa_chain.invoke(input_variables).content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 6: Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the capital of France?\n",
      "Answer: The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the capital of France?\"\n",
    "answer = get_answer(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive cell for user questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: I'm just a program, so I don't have feelings, but I'm here and ready to help you! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "user_question = input(\"Enter your question: \")\n",
    "user_answer = get_answer(user_question)\n",
    "print(f\"Answer: {user_answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
