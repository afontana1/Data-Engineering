<p><a href="https://www.cnbc.com/2023/04/14/nvidias-h100-ai-chips-selling-for-more-than-40000-on-ebay.html" title="https://www.cnbc.com/2023/04/14/nvidias-h100-ai-chips-selling-for-more-than-40000-on-ebay.html">https://www.cnbc.com/2023/04/14/nvidias-h100-ai-chips-selling-for-more-than-40000-on-ebay.html</a></p><br><br><ul><br>&#9;<li>Nvidia&rsquo;s H100 graphics cards are selling for more than $40,000 on eBay.</li><br>&#9;<li>The high-end chips are still essential for training and deploying AI software.</li><br>&#9;<li>The prices were noted by 3D gaming pioneer and former Meta consulting technology chief John Carmack on Twitter.</li><br></ul><br><br><div><br><p><a href="https://www.cnbc.com/quotes/NVDA/">Nvidia&rsquo;s</a>&nbsp;most-advanced graphics cards are selling for more than $40,000 on&nbsp;<a href="https://www.cnbc.com/quotes/EBAY/">eBay</a>, as demand soars for chips needed to train and deploy artificial intelligence software.</p><br><br><p>The prices for Nvidia&rsquo;s H100 processors were noted by 3D gaming pioneer and former&nbsp;<a href="https://www.cnbc.com/quotes/META/">Meta</a>&nbsp;consulting technology chief John Carmack&nbsp;<a href="https://twitter.com/ID_AA_Carmack/status/1646676148657569792" target="_blank">on Twitter</a>. On Friday, at least eight H100s were listed on eBay at prices ranging from $39,995 to just under $46,000. Some retailers have offered it in the past for&nbsp;<a href="https://wccftech.com/nvidia-h100-80-gb-pcie-accelerator-with-hopper-gpu-is-priced-over-30000-us-in-japan/" target="_blank">around $36,000</a>.</p><br><br><p>The H100, announced last year, is Nvidia&rsquo;s latest flagship AI chip, succeeding the A100, a roughly $10,000 chip that&rsquo;s been called the &ldquo;<a href="https://www.cnbc.com/2023/02/23/nvidias-a100-is-the-10000-chip-powering-the-race-for-ai-.html">workhorse</a>&rdquo; for AI applications.</p><br><br><p>Developers are using the H100 to build so-called large language models (LLMs), which are at the heart of AI applications like OpenAI&rsquo;s ChatGPT. Running those systems is expensive and requires powerful computers to churn through terabytes of data for days or weeks at a time. They also rely on hefty computing power so the AI model can generate text, images or predictions.</p><br><br><p>Training AI models, especially large ones like GPT, requires hundreds of high-end Nvidia GPUs working together.</p><br><br><p><a href="https://www.cnbc.com/quotes/MSFT/">Microsoft</a>&nbsp;spent hundreds of millions of dollars on tens of thousands of Nvidia A100 chips to help&nbsp;<a href="https://www.bloomberg.com/news/articles/2023-03-13/microsoft-built-an-expensive-supercomputer-to-power-openai-s-chatgpt?leadSource=uverify%20wall" target="_blank">build ChatGPT</a>. Nvidia controls the vast majority of the market for AI chips.</p><br><br><p>Nvidia also offers a supercomputer with eight GPUs working together called the DGX. Earlier this year, the company announced new services that would allow companies to rent access to DGX computers for&nbsp;<a href="https://www.theverge.com/23649329/nvidia-dgx-cloud-microsoft-google-oracle-chatgpt-web-browser" target="_blank">$37,000 per month</a>. At that price, the system would use Nvidia&rsquo;s older A100 processors.</p><br><br><p>Nvidia says the H100 is the first chip to be optimized for the specific AI architecture underpinning many of the recent advances in AI, called transformers. Industry experts say the more powerful chips will be necessary to build even bigger and more data-hungry models than those that are currently available.</p><br></div><p>Copyright &#169; Benzinga. All rights reserved. Write to editorial@benzinga.com with any questions about this content. Benzinga does not provide investment advice.</p>