<p><strong>Alphabet Inc</strong>&nbsp;(NASDAQ:<a class="ticker" href="https://www.benzinga.com/stock/GOOG#NASDAQ">GOOG</a>) (NASDAQ:<a class="ticker" href="https://www.benzinga.com/stock/GOOGL#NASDAQ">GOOGL</a>)&nbsp;<strong>Google&nbsp;</strong>on Tuesday elaborated on the supercomputers it uses to train its&nbsp;<a class="editor-rtfLink" href="https://www.benzinga.com/news/23/03/31575293/chatgpt-show-me-the-money-microsoft-to-place-ads-in-chatgpt-powered-bing-search" style="color:#4a6ee0; background:transparent; margin-top:0pt; margin-bottom:0pt" target="_blank">artificial intelligence models</a>.</p>&#13;<br>&#13;<br><p>Google said the systems are faster and more power-efficient than comparable systems from&nbsp;<strong>Nvidia Corp</strong>&nbsp;(NASDAQ:<a class="ticker" href="https://www.benzinga.com/stock/NVDA#NASDAQ">NVDA</a>), Reuters&nbsp;<a class="editor-rtfLink" href="https://www.reuters.com/technology/google-says-its-ai-supercomputer-is-faster-greener-than-nvidia-2023-04-05/" style="color:#4a6ee0; background:transparent; margin-top:0pt; margin-bottom:0pt" target="_blank">reports</a>.</p>&#13;<br>&#13;<br><p>It has designed its custom chip called the Tensor Processing Unit, or TPU which serves more than 90% of the company&#39;s work on AI training, feeding data through models to respond to queries with human-like text or generating images. The Google TPU is now in its fourth generation.</p>&#13;<br>&#13;<br><p>Google detailed how it has strung over 4,000 chips into a supercomputer using its custom-developed optical switches to help connect individual machines.</p>&#13;<br>&#13;<br><p>The company&#39;s&nbsp;<strong>Bard&nbsp;</strong>and&nbsp;<strong>Microsoft Corp</strong>&nbsp;(NASDAQ:<a class="ticker" href="https://www.benzinga.com/stock/MSFT#NASDAQ">MSFT</a>) backed&nbsp;<strong>OpenAI&#39;s ChatGPT</strong>&nbsp;intensified competition among companies that build AI supercomputers as the so-called large language models that power the technologies have exploded in size.</p>&#13;<br>&#13;<br><p>Google trained its largest publicly disclosed language model to date, PaLM, by splitting it across two of the 4,000-chip supercomputers over 50 days.</p>&#13;<br>&#13;<br><p>Google said that its chips are up to 1.7 times faster and 1.9 times more power-efficient for comparably sized systems than a system based on Nvidia&#39;s A100 chip.</p>&#13;<br>&#13;<br><p>Google hinted at working on a new TPU that would compete with the Nvidia H100, with Google Fellow Norm Jouppi telling Reuters that Google has &quot;a healthy pipeline of future chips.&quot;</p>&#13;<br>&#13;<br><p>In March, Microsoft disclosed hunting out ways to string together tens of thousands of Nvidia&#39;s<strong>&nbsp;</strong>A100 graphics chips, the workhorse for training AI models.&nbsp;</p>&#13;<br>&#13;<br><p>OpenAI needed access to complete cloud computing services for long periods as it tried to train an increasingly large set of AI programs called models.</p>&#13;<br>&#13;<br><p>Microsoft executive vice president Scott Guthrie said it cost Microsoft over several hundred million dollars.</p>&#13;<br>&#13;<br><p>It is already at work on the next generation of the AI supercomputer, part of an expanded deal with OpenAI in which Microsoft added $10 billion.</p>&#13;<br>&#13;<br><p>Microsoft is adding the latest Nvidia graphics chip for AI workloads, the H100, and the newest version of Nvidia&#39;s Infiniband networking technology to share&nbsp;<a class="editor-rtfLink" href="https://www.benzinga.com/news/23/03/31321453/from-ideation-to-implementation-how-microsoft-achieved-its-chatgpt-goals" style="color:#4a6ee0; background:transparent; margin-top:0pt; margin-bottom:0pt" target="_blank">data even faster</a>.</p>&#13;<br> <p>Copyright &#169; Benzinga. All rights reserved. Write to editorial@benzinga.com with any questions about this content. Benzinga does not provide investment advice.</p>