{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Blueprints for Text Analysis Using Python**](https://github.com/blueprints-for-text-analytics-python/blueprints-text)  \n",
    "Jens Albrecht, Sidharth Ramachandran, Christian Winkler\n",
    "\n",
    "**If you like the book or the code examples here, please leave a friendly comment on [Amazon.com](https://www.amazon.com/Blueprints-Text-Analytics-Using-Python/dp/149207408X)!**\n",
    "<img src=\"../rating.png\" width=\"100\"/>\n",
    "\n",
    "\n",
    "# Chapter 4:<div class='tocSkip'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Textual Data For Statistics and Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compatibility with spaCy 3.x<div class=\"tocSkip\"/>\n",
    "\n",
    "We adjusted the this notebook to run with spaCy 3.2 and textacy 0.12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remark<div class='tocSkip'/>\n",
    "\n",
    "The code in this notebook differs slightly from the printed book. For example we frequently use pretty print (`pp.pprint`) instead of `print` and `tqdm`'s `progress_apply` instead of Pandas' `apply`. \n",
    "\n",
    "Moreover, several layout and formatting commands, like `figsize` to control figure size or subplot commands are removed in the book.\n",
    "\n",
    "You may also find some lines marked with three hashes ###. Those are not in the book as well as they don't contribute to the concept.\n",
    "\n",
    "All of this is done to simplify the code in the book and put the focus on the important parts instead of formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup<div class='tocSkip'/>\n",
    "\n",
    "Set directory locations. If working on Google Colab: copy files and install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:08:40.350507Z",
     "start_time": "2022-02-11T07:08:40.325173Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    GIT_ROOT = 'https://github.com/blueprints-for-text-analytics-python/blueprints-text/raw/master'\n",
    "    os.system(f'wget {GIT_ROOT}/ch04/setup.py')\n",
    "\n",
    "%run -i setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Python Settings<div class=\"tocSkip\"/>\n",
    "\n",
    "Common imports, defaults for formatting in Matplotlib, Pandas etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:08:51.168742Z",
     "start_time": "2022-02-11T07:08:42.252065Z"
    }
   },
   "outputs": [],
   "source": [
    "%run \"$BASE_DIR/settings.py\"\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "\n",
    "# to print output of all statements and not just the last\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# otherwise text between $ signs will be interpreted as formula and printed in italic\n",
    "pd.set_option('display.html.use_mathjax', False)\n",
    "\n",
    "# path to import blueprints packages\n",
    "sys.path.append(BASE_DIR + '/packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you'll learn and what we build\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Data Preprocessing Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing the Data Set: Reddit Self Posts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data into Pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:08:52.082807Z",
     "start_time": "2022-02-11T07:08:51.253479Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "posts_file = \"rspct.tsv.gz\"\n",
    "posts_file = f\"{BASE_DIR}/data/reddit-selfposts/rspct_autos.tsv.gz\" ### real location\n",
    "posts_df = pd.read_csv(posts_file, sep='\\t')\n",
    "\n",
    "subred_file = \"subreddit_info.csv.gz\"\n",
    "subred_file = f\"{BASE_DIR}/data/reddit-selfposts/subreddit_info.csv.gz\" ### real location\n",
    "subred_df = pd.read_csv(subred_file).set_index(['subreddit'])\n",
    "\n",
    "df = posts_df.join(subred_df, on='subreddit')\n",
    "len(df) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Standardizing Attribute Names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:08:53.317176Z",
     "start_time": "2022-02-11T07:08:53.278328Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:08:53.819211Z",
     "start_time": "2022-02-11T07:08:53.764551Z"
    }
   },
   "outputs": [],
   "source": [
    "column_mapping = {\n",
    "    'id': 'id',\n",
    "    'subreddit': 'subreddit',\n",
    "    'title': 'title',\n",
    "    'selftext': 'text',\n",
    "    'category_1': 'category',\n",
    "    'category_2': 'subcategory',  \n",
    "    'category_3': None, # no data\n",
    "    'in_data': None, # not needed\n",
    "    'reason_for_exclusion': None # not needed\n",
    "}\n",
    "\n",
    "# define remaining columns\n",
    "columns = [c for c in column_mapping.keys() if column_mapping[c] != None]\n",
    "\n",
    "# select and rename those columns\n",
    "df = df[columns].rename(columns=column_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:08:54.357742Z",
     "start_time": "2022-02-11T07:08:54.296583Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df[df['category'] == 'autos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:08:54.798527Z",
     "start_time": "2022-02-11T07:08:54.744532Z"
    }
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:08:55.007653Z",
     "start_time": "2022-02-11T07:08:54.969064Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = None ###\n",
    "df.sample(1, random_state=7).T\n",
    "pd.options.display.max_colwidth = 200 ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading a Data Frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:08:55.673800Z",
     "start_time": "2022-02-11T07:08:55.507564Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_pickle(\"reddit_dataframe.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:05.570184Z",
     "start_time": "2022-02-11T07:08:55.817870Z"
    }
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "db_name = \"reddit-selfposts.db\"\n",
    "con = sqlite3.connect(db_name)\n",
    "df.to_sql(\"posts\", con, index=False, if_exists=\"replace\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:07.042658Z",
     "start_time": "2022-02-11T07:09:05.737789Z"
    }
   },
   "outputs": [],
   "source": [
    "con = sqlite3.connect(db_name)\n",
    "df = pd.read_sql(\"select * from posts\", con)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:07.252367Z",
     "start_time": "2022-02-11T07:09:07.196734Z"
    }
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Text Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:07.473781Z",
     "start_time": "2022-02-11T07:09:07.420741Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "After viewing the [PINKIEPOOL Trailer](https://www.youtu.be/watch?v=ieHRoHUg)\n",
    "it got me thinking about the best match ups.\n",
    "<lb>Here's my take:<lb><lb>[](/sp)[](/ppseesyou) Deadpool<lb>[](/sp)[](/ajsly)\n",
    "Captain America<lb>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:07.708836Z",
     "start_time": "2022-02-11T07:09:07.652054Z"
    }
   },
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Identify Noise with Regular Expressions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:07.936942Z",
     "start_time": "2022-02-11T07:09:07.873892Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "RE_SUSPICIOUS = re.compile(r'[&#<>{}\\[\\]\\\\]')\n",
    "\n",
    "def impurity(text, min_len=10):\n",
    "    \"\"\"returns the share of suspicious characters in a text\"\"\"\n",
    "    if text == None or len(text) < min_len:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(RE_SUSPICIOUS.findall(text))/len(text)\n",
    "\n",
    "print(impurity(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:08.571522Z",
     "start_time": "2022-02-11T07:09:08.114955Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 100 ###\n",
    "# add new column to data frame\n",
    "df['impurity'] = df['text'].progress_apply(impurity, min_len=10)\n",
    "\n",
    "# get the top 3 records\n",
    "df[['text', 'impurity']].sort_values(by='impurity', ascending=False).head(3)\n",
    "pd.options.display.max_colwidth = 200 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:09.565541Z",
     "start_time": "2022-02-11T07:09:08.740547Z"
    }
   },
   "outputs": [],
   "source": [
    "from blueprints.exploration import count_words\n",
    "count_words(df, column='text', preprocess=lambda t: re.findall(r'<[\\w/]*>', t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Noise Removal with Regular Expressions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:09.829932Z",
     "start_time": "2022-02-11T07:09:09.755277Z"
    }
   },
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "def clean(text):\n",
    "    # convert html escapes like &amp; to characters.\n",
    "    text = html.unescape(text) \n",
    "    # tags like <tab>\n",
    "    text = re.sub(r'<[^<>]*>', ' ', text)\n",
    "    # markdown URLs like [Some text](https://....)\n",
    "    text = re.sub(r'\\[([^\\[\\]]*)\\]\\([^\\(\\)]*\\)', r'\\1', text)\n",
    "    # text or code in brackets like [0]\n",
    "    text = re.sub(r'\\[[^\\[\\]]*\\]', ' ', text)\n",
    "    # standalone sequences of specials, matches &# but not #cool\n",
    "    text = re.sub(r'(?:^|\\s)[&#<>{}\\[\\]+|\\\\:-]{1,}(?:\\s|$)', ' ', text)\n",
    "    # standalone sequences of hyphens like --- or ==\n",
    "    text = re.sub(r'(?:^|\\s)[\\-=\\+]{2,}(?:\\s|$)', ' ', text)\n",
    "    # sequences of white spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:12.306785Z",
     "start_time": "2022-02-11T07:09:12.254626Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_text = clean(text)\n",
    "print(clean_text)\n",
    "print(\"Impurity:\", impurity(clean_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:15.899306Z",
     "start_time": "2022-02-11T07:09:12.731714Z"
    }
   },
   "outputs": [],
   "source": [
    "df['clean_text'] = df['text'].progress_map(clean)\n",
    "df['impurity']   = df['clean_text'].apply(impurity, min_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:16.283338Z",
     "start_time": "2022-02-11T07:09:16.174025Z"
    }
   },
   "outputs": [],
   "source": [
    "df[['clean_text', 'impurity']].sort_values(by='impurity', ascending=False) \\\n",
    "                              .head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Character Normalization with textacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:16.608328Z",
     "start_time": "2022-02-11T07:09:16.523502Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"The café “Saint-Raphaël” is loca-\\nted on Côte dʼAzur.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:18.036817Z",
     "start_time": "2022-02-11T07:09:17.910871Z"
    }
   },
   "outputs": [],
   "source": [
    "import textacy\n",
    "import textacy.preprocessing as tprep\n",
    "\n",
    "if textacy.__version__ < '0.11':\n",
    "    # as in book\n",
    "    def normalize(text):\n",
    "        text = tprep.normalize_hyphenated_words(text)\n",
    "        text = tprep.normalize_quotation_marks(text)\n",
    "        text = tprep.normalize_unicode(text)\n",
    "        text = tprep.remove_accents(text)\n",
    "        return text\n",
    "\n",
    "else:\n",
    "    # adjusted to textacy 0.11\n",
    "    def normalize(text):\n",
    "        text = tprep.normalize.hyphenated_words(text)\n",
    "        text = tprep.normalize.quotation_marks(text)\n",
    "        text = tprep.normalize.unicode(text)\n",
    "        text = tprep.remove.accents(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:21.009078Z",
     "start_time": "2022-02-11T07:09:20.963523Z"
    }
   },
   "outputs": [],
   "source": [
    "print(normalize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Pattern-based Data Masking with textacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:23.469380Z",
     "start_time": "2022-02-11T07:09:22.340940Z"
    }
   },
   "outputs": [],
   "source": [
    "from textacy.preprocessing.resources import RE_URL\n",
    "\n",
    "count_words(df, column='clean_text', preprocess=RE_URL.findall).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:23.724446Z",
     "start_time": "2022-02-11T07:09:23.660526Z"
    }
   },
   "outputs": [],
   "source": [
    "if textacy.__version__ < '0.11':\n",
    "    # as in book\n",
    "    replace_urls = textacy.preprocessing.replace_urls\n",
    "else:\n",
    "    replace_urls = textacy.preprocessing.replace.urls\n",
    "\n",
    "text = \"Check out https://spacy.io/usage/spacy-101\"\n",
    "\n",
    "# using default substitution _URL_\n",
    "print(replace_urls(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:34.130462Z",
     "start_time": "2022-02-11T07:09:23.938549Z"
    }
   },
   "outputs": [],
   "source": [
    "df['clean_text'] = df['clean_text'].progress_map(replace_urls)\n",
    "df['clean_text'] = df['clean_text'].progress_map(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:37.397992Z",
     "start_time": "2022-02-11T07:09:34.293431Z"
    }
   },
   "outputs": [],
   "source": [
    "df.rename(columns={'text': 'raw_text', 'clean_text': 'text'}, inplace=True)\n",
    "df.drop(columns=['impurity'], inplace=True)\n",
    "\n",
    "con = sqlite3.connect(db_name)\n",
    "df.to_sql(\"posts_cleaned\", con, index=False, if_exists=\"replace\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:37.681704Z",
     "start_time": "2022-02-11T07:09:37.615851Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "2019-08-10 23:32: @pete/@louis - I don't have a well-designed \n",
    "solution for today's problem. The code of module AC68 should be -1. \n",
    "Have to think a bit... #goodnight ;-) 😩😬\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Tokenization with Regular Expressions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:37.980095Z",
     "start_time": "2022-02-11T07:09:37.867611Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = re.findall(r'\\w\\w+', text)\n",
    "print(*tokens, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:38.419682Z",
     "start_time": "2022-02-11T07:09:38.300825Z"
    }
   },
   "outputs": [],
   "source": [
    "RE_TOKEN = re.compile(r\"\"\"\n",
    "               ( [#]?[@\\w'’\\.\\-\\:]*\\w     # words, hash tags and email adresses\n",
    "               | [:;<]\\-?[\\)\\(3]          # coarse pattern for basic text emojis\n",
    "               | [\\U0001F100-\\U0001FFFF]  # coarse code range for unicode emojis\n",
    "               )\n",
    "               \"\"\", re.VERBOSE)\n",
    "\n",
    "def tokenize(text):\n",
    "    return RE_TOKEN.findall(text)\n",
    "\n",
    "tokens = tokenize(text)\n",
    "print(*tokens, sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization with NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:39.088420Z",
     "start_time": "2022-02-11T07:09:38.644778Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt') ###\n",
    "tokens = nltk.tokenize.word_tokenize(text)\n",
    "print(*tokens, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:39.373425Z",
     "start_time": "2022-02-11T07:09:39.290458Z"
    }
   },
   "outputs": [],
   "source": [
    "# Not in book: Regex Tokenizer\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(RE_TOKEN.pattern, flags=re.VERBOSE)\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(*tokens, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:39.706541Z",
     "start_time": "2022-02-11T07:09:39.594338Z"
    }
   },
   "outputs": [],
   "source": [
    "# Not in book: Tweet Tokenizer\n",
    "tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(*tokens, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:39.997226Z",
     "start_time": "2022-02-11T07:09:39.908964Z"
    }
   },
   "outputs": [],
   "source": [
    "# Not in book: Toktok Tokenizer\n",
    "tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(*tokens, sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations for Tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linguistic Processing with spaCy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating a Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:41.220007Z",
     "start_time": "2022-02-11T07:09:40.211141Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:41.485999Z",
     "start_time": "2022-02-11T07:09:41.404351Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:42.826011Z",
     "start_time": "2022-02-11T07:09:41.735123Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:43.835516Z",
     "start_time": "2022-02-11T07:09:43.011101Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"My best friend Ryan Peters likes fancy adventure games.\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:44.097635Z",
     "start_time": "2022-02-11T07:09:44.034263Z"
    }
   },
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:44.379697Z",
     "start_time": "2022-02-11T07:09:44.304372Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_nlp(doc, include_punct=False):\n",
    "    \"\"\"Generate data frame for visualization of spaCy tokens.\"\"\"\n",
    "    rows = []\n",
    "    for i, t in enumerate(doc):\n",
    "        if not t.is_punct or include_punct:\n",
    "            row = {'token': i,  'text': t.text, 'lemma_': t.lemma_, \n",
    "                   'is_stop': t.is_stop, 'is_alpha': t.is_alpha,\n",
    "                   'pos_': t.pos_, 'dep_': t.dep_, \n",
    "                   'ent_type_': t.ent_type_, 'ent_iob_': t.ent_iob_}\n",
    "            rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows).set_index('token')\n",
    "    df.index.name = None\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:44.670696Z",
     "start_time": "2022-02-11T07:09:44.580331Z"
    }
   },
   "outputs": [],
   "source": [
    "display_nlp(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Customizing Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:45.774014Z",
     "start_time": "2022-02-11T07:09:44.909998Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"@Pete: choose low-carb #food #eat-smart. _url_ ;-) 😋👍\"\n",
    "nlp = spacy.load('en_core_web_sm') ###\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(token, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:47.080936Z",
     "start_time": "2022-02-11T07:09:45.969691Z"
    }
   },
   "outputs": [],
   "source": [
    "import re ###\n",
    "import spacy ###\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, \\\n",
    "                       compile_infix_regex, compile_suffix_regex\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    \n",
    "    # use default patterns except the ones matched by re.search\n",
    "    prefixes = [pattern for pattern in nlp.Defaults.prefixes \n",
    "                if pattern not in ['-', '_', '#']]\n",
    "    suffixes = [pattern for pattern in nlp.Defaults.suffixes\n",
    "                if pattern not in ['_']]\n",
    "    infixes  = [pattern for pattern in nlp.Defaults.infixes\n",
    "                if not re.search(pattern, 'xx-xx')]\n",
    "\n",
    "    return Tokenizer(vocab          = nlp.vocab, \n",
    "                     rules          = nlp.Defaults.tokenizer_exceptions,\n",
    "                     prefix_search  = compile_prefix_regex(prefixes).search,\n",
    "                     suffix_search  = compile_suffix_regex(suffixes).search,\n",
    "                     infix_finditer = compile_infix_regex(infixes).finditer,\n",
    "                     token_match    = nlp.Defaults.token_match)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Working with Stop Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:48.091818Z",
     "start_time": "2022-02-11T07:09:47.246118Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm') ###\n",
    "text = \"Dear Ryan, we need to sit down and talk. Regards, Pete\"\n",
    "doc = nlp(text)\n",
    "\n",
    "non_stop = [t for t in doc if not t.is_stop and not t.is_punct]\n",
    "print(non_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:49.106385Z",
     "start_time": "2022-02-11T07:09:48.251445Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.vocab['down'].is_stop = False\n",
    "nlp.vocab['Dear'].is_stop = True\n",
    "nlp.vocab['Regards'].is_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Not in book: Modifying stop words with a language subclass**\n",
    "\n",
    "Modifying the stop word by changing the vocabulary will probably become deprecated with spaCy 3.0. Instead it is recommended to create a subclass of the respective language like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:49.546013Z",
     "start_time": "2022-02-11T07:09:49.275921Z"
    }
   },
   "outputs": [],
   "source": [
    "# not in book: subclass approach to modify stop word lists\n",
    "# recommended from spaCy version 3.0 onwards\n",
    "from spacy.lang.en import English\n",
    "\n",
    "excluded_stop_words = {'down'}\n",
    "included_stop_words = {'dear', 'regards'}\n",
    "\n",
    "class CustomEnglishDefaults(English.Defaults):\n",
    "    stop_words = English.Defaults.stop_words.copy()\n",
    "    stop_words -= excluded_stop_words\n",
    "    stop_words |= included_stop_words\n",
    "    \n",
    "class CustomEnglish(English):\n",
    "    Defaults = CustomEnglishDefaults\n",
    "\n",
    "nlp = CustomEnglish()\n",
    "\n",
    "text = \"Dear Ryan, we need to sit down and talk. Regards, Pete\"\n",
    "doc = nlp.make_doc(text) # only tokenize\n",
    "    \n",
    "tokens_wo_stop = [token for token in doc ]\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        print(token, end='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:50.979312Z",
     "start_time": "2022-02-11T07:09:49.739327Z"
    }
   },
   "outputs": [],
   "source": [
    "# reset nlp to original\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Extracting Lemmas based on Part-of-Speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:51.355029Z",
     "start_time": "2022-02-11T07:09:51.243518Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"My best friend Ryan Peters likes fancy adventure games.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "print(*[t.lemma_ for t in doc], sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:51.799490Z",
     "start_time": "2022-02-11T07:09:51.687258Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"My best friend Ryan Peters likes fancy adventure games.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "nouns = [t for t in doc if t.pos_ in ['NOUN', 'PROPN']]\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:52.419625Z",
     "start_time": "2022-02-11T07:09:52.340267Z"
    }
   },
   "outputs": [],
   "source": [
    "import textacy\n",
    "\n",
    "tokens = textacy.extract.words(doc, \n",
    "            filter_stops = True,           # default True, no stopwords\n",
    "            filter_punct = True,           # default True, no punctuation\n",
    "            filter_nums = True,            # default False, no numbers\n",
    "            include_pos = ['ADJ', 'NOUN'], # default None = include all\n",
    "            exclude_pos = None,            # default None = exclude none\n",
    "            min_freq = 1)                  # minimum frequency of words\n",
    "\n",
    "print(*[t for t in tokens], sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:09:54.058241Z",
     "start_time": "2022-02-11T07:09:53.999631Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_lemmas(doc, **kwargs):\n",
    "    return [t.lemma_ for t in textacy.extract.words(doc, **kwargs)]\n",
    "\n",
    "lemmas = extract_lemmas(doc, include_pos=['ADJ', 'NOUN'])\n",
    "print(*lemmas, sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Extracting Noun Phrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:11:12.949599Z",
     "start_time": "2022-02-11T07:11:12.889468Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"My best friend Ryan Peters likes fancy adventure games.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "patterns = [\"POS:ADJ POS:NOUN:+\"]\n",
    "\n",
    "if textacy.__version__ < '0.11':\n",
    "    # as in book\n",
    "    spans = textacy.extract.matches(doc, patterns=patterns)\n",
    "else:\n",
    "    # new textacy version\n",
    "    spans = textacy.extract.matches.token_matches(doc, patterns=patterns)\n",
    "    \n",
    "print(*[s.lemma_ for s in spans], sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:11:15.167646Z",
     "start_time": "2022-02-11T07:11:15.109309Z"
    }
   },
   "outputs": [],
   "source": [
    "print(*doc.noun_chunks, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:11:35.569337Z",
     "start_time": "2022-02-11T07:11:35.418520Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_noun_phrases(doc, preceding_pos=['NOUN'], sep='_'):\n",
    "    patterns = []\n",
    "    for pos in preceding_pos:\n",
    "        patterns.append(f\"POS:{pos} POS:NOUN:+\")\n",
    "\n",
    "    if textacy.__version__ < '0.11':\n",
    "        # as in book\n",
    "        spans = textacy.extract.matches(doc, patterns=patterns)\n",
    "    else:\n",
    "        # new textacy version\n",
    "        spans = textacy.extract.matches.token_matches(doc, patterns=patterns)\n",
    "\n",
    "    return [sep.join([t.lemma_ for t in s]) for s in spans]\n",
    "\n",
    "print(*extract_noun_phrases(doc, ['ADJ', 'NOUN']), sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Extracting Named Entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:11:44.855759Z",
     "start_time": "2022-02-11T07:11:44.789334Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"James O'Neill, chairman of World Cargo Inc, lives in San Francisco.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f\"({ent.text}, {ent.label_})\", end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:11:45.705815Z",
     "start_time": "2022-02-11T07:11:45.635390Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:11:46.924574Z",
     "start_time": "2022-02-11T07:11:46.870284Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_entities(doc, include_types=None, sep='_'):\n",
    "\n",
    "    ents = textacy.extract.entities(doc, \n",
    "             include_types=include_types, \n",
    "             exclude_types=None, \n",
    "             drop_determiners=True, \n",
    "             min_freq=1)\n",
    "    \n",
    "    return [sep.join([t.lemma_ for t in e])+'/'+e.label_ for e in ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:11:47.640710Z",
     "start_time": "2022-02-11T07:11:47.589554Z"
    }
   },
   "outputs": [],
   "source": [
    "print(extract_entities(doc, ['PERSON', 'GPE']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction on a Large Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: One Function to Get It All\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:11:50.064456Z",
     "start_time": "2022-02-11T07:11:50.005224Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_nlp(doc):\n",
    "    return {\n",
    "    'lemmas'          : extract_lemmas(doc, \n",
    "                                     exclude_pos = ['PART', 'PUNCT', \n",
    "                                        'DET', 'PRON', 'SYM', 'SPACE'],\n",
    "                                     filter_stops = False),\n",
    "    'adjs_verbs'      : extract_lemmas(doc, include_pos = ['ADJ', 'VERB']),\n",
    "    'nouns'           : extract_lemmas(doc, include_pos = ['NOUN', 'PROPN']),\n",
    "    'noun_phrases'    : extract_noun_phrases(doc, ['NOUN']),\n",
    "    'adj_noun_phrases': extract_noun_phrases(doc, ['ADJ']),\n",
    "    'entities'        : extract_entities(doc, ['PERSON', 'ORG', 'GPE', 'LOC'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:11:51.394808Z",
     "start_time": "2022-02-11T07:11:50.635698Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:11:51.646691Z",
     "start_time": "2022-02-11T07:11:51.565103Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"My best friend Ryan Peters likes fancy adventure games.\"\n",
    "doc = nlp(text)\n",
    "for col, values in extract_nlp(doc).items():\n",
    "    print(f\"{col}: {values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:11:52.892380Z",
     "start_time": "2022-02-11T07:11:52.814408Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp_columns = list(extract_nlp(nlp.make_doc('')).keys())\n",
    "print(nlp_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Using spaCy on a Large Data Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:11:57.045277Z",
     "start_time": "2022-02-11T07:11:54.559387Z"
    }
   },
   "outputs": [],
   "source": [
    "import sqlite3 ###\n",
    "\n",
    "db_name = \"reddit-selfposts.db\"\n",
    "con = sqlite3.connect(db_name)\n",
    "df = pd.read_sql(\"select * from posts_cleaned\", con)\n",
    "con.close()\n",
    "\n",
    "df['text'] = df['title'] + ': ' + df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:11:58.157426Z",
     "start_time": "2022-02-11T07:11:58.100555Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in nlp_columns:\n",
    "    df[col] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On Colab**: Choose \"Runtime\"&rarr;\"Change Runtime Type\"&rarr;\"GPU\" to benefit from the GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:11:59.244480Z",
     "start_time": "2022-02-11T07:11:59.190020Z"
    }
   },
   "outputs": [],
   "source": [
    "if spacy.prefer_gpu():\n",
    "    print(\"Working on GPU.\")\n",
    "else:\n",
    "    print(\"No GPU found, working on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:12:01.665331Z",
     "start_time": "2022-02-11T07:11:59.944873Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=[])\n",
    "nlp.tokenizer = custom_tokenizer(nlp) # optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:12:09.662804Z",
     "start_time": "2022-02-11T07:12:09.596112Z"
    }
   },
   "outputs": [],
   "source": [
    "# full data set takes about 6-8 minutes\n",
    "# for faster processing use a sample like this\n",
    "# df = df.sample(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T07:12:24.571683Z",
     "start_time": "2022-02-11T07:12:13.772508Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "batches = math.ceil(len(df) / batch_size) ###\n",
    "\n",
    "for i in tqdm(range(0, len(df), batch_size), total=batches):\n",
    "    docs = nlp.pipe(df['text'][i:i+batch_size])\n",
    "    \n",
    "    for j, doc in enumerate(docs):\n",
    "        for col, values in extract_nlp(doc).items():\n",
    "            df[col].iloc[i+j] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:18.141805Z",
     "start_time": "2021-02-25T16:49:39.155Z"
    }
   },
   "outputs": [],
   "source": [
    "df[['text', 'lemmas', 'nouns', 'noun_phrases', 'entities']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:18.142811Z",
     "start_time": "2021-02-25T16:49:39.158Z"
    }
   },
   "outputs": [],
   "source": [
    "count_words(df, 'noun_phrases').head(10).plot(kind='barh', figsize=(8,3)).invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persisting the Result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:18.143525Z",
     "start_time": "2021-02-25T16:49:39.163Z"
    }
   },
   "outputs": [],
   "source": [
    "df[nlp_columns] = df[nlp_columns].applymap(lambda items: ' '.join(items))\n",
    "\n",
    "con = sqlite3.connect(db_name) \n",
    "df.to_sql(\"posts_nlp\", con, index=False, if_exists=\"replace\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Note on Execution Time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There is More\n",
    "## Language Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Blueprint (not in book): Language Detection with fastText\n",
    "\n",
    "There are different trained models available on the fastText website. We will be using the smaller model `lid.176.ftz` which has a size of less than 1 MB and is almost as accurate as the large model with 126MB. See <https://fasttext.cc/docs/en/language-identification.html> for instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:18.144359Z",
     "start_time": "2021-02-25T16:49:39.168Z"
    }
   },
   "outputs": [],
   "source": [
    "# download model\n",
    "!wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading, we load the model and make a first prediction with the following lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:18.145762Z",
     "start_time": "2021-02-25T16:49:39.172Z"
    }
   },
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "lang_model = fasttext.load_model(\"lid.176.ftz\")\n",
    "\n",
    "# make a prediction\n",
    "print(lang_model.predict('\"Good morning\" in German is \"Guten Morgen\"', 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:18.146739Z",
     "start_time": "2021-02-25T16:49:39.176Z"
    }
   },
   "outputs": [],
   "source": [
    "lang_model.predict('\"Good morning\" in German is \"Guten Morgen\"', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predict` function takes a Unicode string as its first argument. The second, optional parameter `k` specifies that we want the `k` language labels with the highest probabilities.\n",
    "\n",
    "The model returns labels in the form `__label__<code>`, where code is the ISO 639 language code`<footnote>`See <https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes> for a complete list.`</footnote>` and probabilites for each label. \n",
    "\n",
    "Let's wrap the language identification into a preprocessing function. The function returns the detected language only if the calculated probability is higher than the specified threshold, otherwise, it returns the default language. This is useful for corpora like the hacker news, which is basically an English corpus with some utterances from other languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:18.148015Z",
     "start_time": "2021-02-25T16:49:39.181Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_language(text, threshold=0.8, default='en'):\n",
    "    \n",
    "    # skip language detection for very short texts\n",
    "    if len(text) < 20:\n",
    "        return default\n",
    "\n",
    "    # fasttext requires single line input\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    labels, probas = lang_model.predict(text)\n",
    "    lang = labels[0].replace(\"__label__\", \"\")\n",
    "    proba = probas[0]\n",
    "    \n",
    "    if proba < threshold:\n",
    "        return default\n",
    "    else:\n",
    "        return lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction function can now easily be applied to a data frame to identify the language of each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:18.148894Z",
     "start_time": "2021-02-25T16:49:39.186Z"
    }
   },
   "outputs": [],
   "source": [
    "data = [\"I don't like version 2.0 of Chat4you 😡👎\",   # English\n",
    "        \"Ich mag Version 2.0 von Chat4you nicht 😡👎\", # German\n",
    "        \"Мне не нравится версия 2.0 Chat4you 😡👎\",    # Russian\n",
    "        \"Não gosto da versão 2.0 do Chat4you 😡👎\",    # Portugese\n",
    "        \"मुझे Chat4you का संस्करण 2.0 पसंद नहीं है 😡👎\"]   # Hindi\n",
    "demo_df = pd.Series(data, name='text').to_frame()\n",
    "\n",
    "# create new column\n",
    "demo_df['lang'] = demo_df['text'].apply(predict_language)\n",
    "\n",
    "demo_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the real names for the language codes, we can provide a mapping dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:18.149859Z",
     "start_time": "2021-02-25T16:49:39.190Z"
    }
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/haliaeetus/iso-639/master/data/iso_639-1.csv'\n",
    "lang_df = pd.read_csv(url)\n",
    "lang_df = lang_df[['name', '639-1', '639-2']].melt(id_vars=['name'], var_name='iso', value_name='code')\n",
    "\n",
    "# create dictionary with entries {'code': 'name'}\n",
    "iso639_languages = lang_df.set_index('code')['name'].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we add this to our original data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:18.150403Z",
     "start_time": "2021-02-25T16:49:39.194Z"
    }
   },
   "outputs": [],
   "source": [
    "demo_df['lang_name'] = demo_df['lang'].map(iso639_languages)\n",
    "\n",
    "demo_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spell Checking\n",
    "## Token Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:18.150954Z",
     "start_time": "2021-02-25T16:49:39.198Z"
    }
   },
   "outputs": [],
   "source": [
    "# Not in book: Normalize tokens with a dict\n",
    "token_map = { 'U.S.': 'United_States',\n",
    "              'L.A.': 'Los_Angeles' }\n",
    "\n",
    "def token_normalizer(tokens):\n",
    "    return [token_map.get(t, t) for t in tokens]\n",
    "\n",
    "tokens = \"L.A. is a city in the U.S.\".split()\n",
    "tokens = token_normalizer(tokens)\n",
    "\n",
    "print(*tokens, sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing Remarks and Recommendations\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "265.638px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
