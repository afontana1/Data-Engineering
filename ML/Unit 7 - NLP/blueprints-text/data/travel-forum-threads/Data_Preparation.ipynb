{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Download dataset from the URL: https://www.dropbox.com/s/dcds423fl7fscow/threadDataSet.zip?dl=0\n",
    "        You can also use the command: `wget https://www.dropbox.com/s/dcds423fl7fscow/threadDataSet.zip?dl=0`\n",
    "* Please unzip using the command: `unzip threadDataSet.zip`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Make sure that the unzip happened correctly and the folder structure is as described below:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- After running the unzip command a new directory called threadDataSet would be created containing the following folders:\n",
    "\n",
    "/data/travel-forum-threads/threadDataSet\n",
    "├── human summaries\n",
    "├── selected sentences\n",
    "├── threads as original xml\n",
    "└── threads parsed\n",
    "\n",
    "- There are two directories of interest to us, the first is *threads as original xml* which contains the following directories:\n",
    "\n",
    "/data/travel-forum-threads/threadDataSet/threads as original xml\n",
    "├── batch_one\n",
    "├── batch_two\n",
    "└── gold\n",
    "\n",
    "- Each of the directories contains the original extracted threads as XML files. Each XML file contains all the posts in a thread.\n",
    "- The other directory which is of interest is *human summaries* which contains the following directories:\n",
    "\n",
    "/data/travel-forum-threads/threadDataSet/human summaries/\n",
    "├── batch_one_Annotator_One\n",
    "├── batch_one_Annotator_Two\n",
    "├── batch_two_Annotator_One\n",
    "├── batch_two_Annotator_Two\n",
    "├── gold_Annotator_One\n",
    "└── gold_Annotator_Two\n",
    "\n",
    "- Each of the directories contains the summary of a thread written by a human annotator (there are two used by the authors of the paper) for each original thread\n",
    "- In the steps below, we will create a dataframe that contains all the posts in a thread and then we will add the summary column by Annotator_One."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Fix the XML parsing issue where '&' character is misplaced\n",
    "- If not fixed, this causes the XML parsing to fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "regex = re.compile(r\"&(?!amp;|lt;|gt;)\")\n",
    "\n",
    "thread_directory = \"./threadDataSet/threads as original xml/\"\n",
    "for r, d, f in os.walk(thread_directory):\n",
    "    for file in f:\n",
    "        if '.xml' in file:\n",
    "            with open(r + '/' + file, 'r+', encoding='utf8', errors=\"ignore\") as content:\n",
    "                text = content.read()\n",
    "                text = regex.sub(\"&amp;\", text)\n",
    "                content.seek(0)\n",
    "                content.write(text)\n",
    "                content.truncate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Creating Dataframe from Forum threads\n",
    "* Each thread with all it's posts is in an XML\n",
    "* Each thread is a seperate file\n",
    "* Below method iterates over each file and creates the forum dataframe\n",
    "* Every row in the dataframe is a post or comment in a thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as et\n",
    "\n",
    "def createIssueDF(fileName):\n",
    "    xtree = et.parse(fileName)\n",
    "    xroot = xtree.getroot()\n",
    "    threadID = \"\"\n",
    "    title = \"\"\n",
    "    rows = []\n",
    "    index = 1\n",
    "    columns = [\"userID\", \"Date\", \"postNum\", \"text\"]\n",
    "    for child in xroot:\n",
    "        fileName = os.path.basename(fileName).rstrip('.xml')\n",
    "        if child.tag in ('ThreadID'):\n",
    "            threadID = child.text\n",
    "        elif child.tag in ('Title'):\n",
    "            title = child.text\n",
    "        elif child.tag in ('InitPost','Post'):\n",
    "            if child.find(\"UserID\").text is not None:\n",
    "                userID = child.find(\"UserID\").text.strip()\n",
    "            else:\n",
    "                userID = \"\"\n",
    "            date = child.find(\"Date\").text.strip()\n",
    "            postNum = index\n",
    "            index += 1\n",
    "            if child.find(\"icontent\") is not None:\n",
    "                text = child.find(\"icontent\").text.strip()\n",
    "            else:\n",
    "                text = child.find(\"rcontent\").text.strip()\n",
    "            rows.append({\"userID\":userID, \"Date\":date, \"postNum\":postNum, \"text\":text})\n",
    "\n",
    "    returnDF = pd.DataFrame(rows, columns = columns)\n",
    "    returnDF['Filename'] = fileName\n",
    "    returnDF['ThreadID'] = threadID\n",
    "    returnDF['Title'] = title\n",
    "    \n",
    "    return returnDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_threads_and_summary(thread_directory, summary_directory):\n",
    "    df = pd.DataFrame(columns=['Filename', 'ThreadID', 'Title', 'userID', 'Date', 'postNum', 'text'])\n",
    "    for file in os.listdir(thread_directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        if filename.endswith(\".xml\"): \n",
    "            returnDF = createIssueDF(os.path.join(thread_directory, filename))\n",
    "            df = df.append(returnDF)\n",
    "    df['summary'] = ''\n",
    "    for file in os.listdir(summary_directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(summary_directory, filename), 'r', encoding='utf8', errors=\"ignore\") as f:\n",
    "                lineList = []\n",
    "                for line in f:\n",
    "                    if line.strip() != '':\n",
    "                        lineList.append(line.rstrip('\\n'))\n",
    "            df['summary'][df['Filename'] == filename.rstrip('.txt')] = ''.join(lineList)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Preparing dataset of Gold Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_directory = \"./threadDataSet/threads as original xml/gold\"\n",
    "# Replace path with Annotator_Two to use a different summary\n",
    "summary_directory = \"./threadDataSet/human summaries/gold_Annotator_One\"\n",
    "\n",
    "goldDF = parse_threads_and_summary(thread_directory, summary_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Preparing dataset of Batch_one Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_directory = \"./threadDataSet/threads as original xml/batch_one\"\n",
    "summary_directory = \"./threadDataSet/human summaries/batch_one_Annotator_One\"\n",
    "\n",
    "batchOneDF = parse_threads_and_summary(thread_directory, summary_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Preparing dataset of Batch_two Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_directory = \"./threadDataSet/threads as original xml/batch_two\"\n",
    "summary_directory = \"./threadDataSet/human summaries/batch_two_Annotator_One\"\n",
    "\n",
    "batchTwoDF = parse_threads_and_summary(thread_directory, summary_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Combining all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDF = goldDF.append(batchOneDF)\n",
    "finalDF = finalDF.append(batchTwoDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Checks: Are there any threads with less than 2 posts?\n",
    "\n",
    "- No, so there are no threads that we want to exclude\n",
    "- If yes, it makes sense to exclude these threads since there is nothing to summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: ThreadID, dtype: object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumDF = finalDF.groupby('ThreadID').count().reset_index()\n",
    "cumDF[cumDF['Date'] < 2]['ThreadID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDF.to_csv('travel_threads.csv', sep='|', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
