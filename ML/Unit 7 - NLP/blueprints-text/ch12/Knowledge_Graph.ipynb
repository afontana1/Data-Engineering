{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Blueprints for Text Analysis Using Python**](https://github.com/blueprints-for-text-analytics-python/blueprints-text)  \n",
    "Jens Albrecht, Sidharth Ramachandran, Christian Winkler\n",
    "\n",
    "**If you like the book or the code examples here, please leave a friendly comment on [Amazon.com](https://www.amazon.com/Blueprints-Text-Analytics-Using-Python/dp/149207408X)!**\n",
    "<img src=\"../rating.png\" width=\"100\"/>\n",
    "\n",
    "\n",
    "# Chapter 12:<div class='tocSkip'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Knowledge Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remark<div class='tocSkip'/>\n",
    "\n",
    "The code in this notebook differs slightly from the printed book. For example we frequently use pretty print (`pp.pprint`) instead of `print` and `tqdm`'s `progress_apply` instead of Pandas' `apply`. \n",
    "\n",
    "Moreover, several layout and formatting commands, like `figsize` to control figure size or subplot commands are removed in the book.\n",
    "\n",
    "You may also find some lines marked with three hashes ###. Those are not in the book as well as they don't contribute to the concept.\n",
    "\n",
    "All of this is done to simplify the code in the book and put the focus on the important parts instead of formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup<div class='tocSkip'/>\n",
    "\n",
    "Set directory locations. If working on Google Colab: copy files and install required libraries.\n",
    "\n",
    "**On Colab:** Use runtime **with GPU (Menu&rarr;Runtime&rarr;Change runtime type)** for better performance **before** you start this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T17:56:43.836059Z",
     "start_time": "2021-02-25T17:56:43.815304Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    GIT_ROOT = 'https://github.com/blueprints-for-text-analytics-python/blueprints-text/raw/master'\n",
    "    os.system(f'wget {GIT_ROOT}/ch12/setup.py')\n",
    "\n",
    "%run -i setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Python Settings<div class=\"tocSkip\"/>\n",
    "\n",
    "Common imports, defaults for formatting in Matplotlib, Pandas etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T17:56:46.782473Z",
     "start_time": "2021-02-25T17:56:44.101317Z"
    }
   },
   "outputs": [],
   "source": [
    "%run \"$BASE_DIR/settings.py\"\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "\n",
    "# to print output of all statements and not just the last\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T17:56:46.873864Z",
     "start_time": "2021-02-25T17:56:46.825366Z"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append(BASE_DIR + '/packages') # to import blueprints package\n",
    "\n",
    "from blueprints.knowledge import display_ner, reset_pipeline, print_dep_tree, alias_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you'll learn and what we build\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Graphs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint to Query Wikidata for Aliases not in Book\n",
    "\n",
    "Below you find an example of what you can do with public ontologies like Wikidata. Here, we defined a SPARQL query to retrieve the names, aliases and URLs of all entities of type \"United States federal executive department\" (https://www.wikidata.org/wiki/Q910252)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T17:56:47.531617Z",
     "start_time": "2021-02-25T17:56:46.924378Z"
    }
   },
   "outputs": [],
   "source": [
    "# pip install sparqlwrapper\n",
    "# https://rdflib.github.io/sparqlwrapper/\n",
    "\n",
    "import sys\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT ?org ?orgLabel ?aliases ?urlLabel ?country ?countryLabel WITH {\n",
    "  SELECT ?org (group_concat(distinct ?alias;separator=\",\") as ?aliases)\n",
    "  WHERE {\n",
    "    ?org wdt:P31 wd:Q910252. # org is(P31) US department (Q910252)\n",
    "    ?org skos:altLabel ?alias. filter(lang(?alias)=\"en\")\n",
    "  } GROUP BY ?org } AS %i\n",
    "  WHERE {\n",
    "  include %i\n",
    "  ?org wdt:P856 ?url; # has official website (P856)\n",
    "       wdt:P17 ?country. # has country (P17)\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "ORDER BY ?orgLabel\n",
    "\"\"\"\n",
    "\n",
    "def sparql_df(endpoint_url, query):\n",
    "    user_agent = \"Wikidata-Service Python/%s.%s\" % (sys.version_info[0], sys.version_info[1])\n",
    "    sparql = SPARQLWrapper(endpoint_url, agent=user_agent)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "\n",
    "    columns = results['head']['vars']\n",
    "    rows = []\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        row = {}\n",
    "        for col in result:\n",
    "            row[col] = result[col]['value']\n",
    "        rows.append(row)\n",
    "\n",
    "    return pd.DataFrame.from_records(rows, columns=columns)\n",
    "\n",
    "wd_df = sparql_df(endpoint_url, query)\n",
    "\n",
    "# rename columns\n",
    "wd_df.columns = ['org_id', 'org', 'aliases', 'url', 'country_id', 'country']\n",
    "\n",
    "wd_df['org_id'] = wd_df['org_id'].str.replace('http://www.wikidata.org/entity/', '')\n",
    "wd_df['country_id'] = wd_df['country_id'].str.replace('http://www.wikidata.org/entity/', '')\n",
    "wd_df['aliases'] = wd_df['aliases'].str.split(',')\n",
    "\n",
    "wd_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Knowledge Graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing the Data Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T17:56:47.758460Z",
     "start_time": "2021-02-25T17:56:47.599448Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation of NLTK Reuters Corpus (not in book)\n",
    "\n",
    "This section contains the steps how to create the data frame for some of the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T17:56:48.627418Z",
     "start_time": "2021-02-25T17:56:47.820059Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "\n",
    "# List of documents\n",
    "documents = reuters.fileids()\n",
    "print(str(len(documents)) + \" documents\")\n",
    "print(str(len(reuters.categories())) + \" categories:\")\n",
    "print(reuters.categories()[:10] + ['...'])\n",
    "\n",
    "print(reuters.readme()[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each article is stored as a separated file. The data files are identified by a file ID of the form \"train/1234\" or \"test/5678\". We first create a data frame with the `fileid` column and then load the raw text for each ID into a second column. Finally, as we don't care whether it's train or test, we just the number from the file ID and use it as the index of our data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T17:56:49.023454Z",
     "start_time": "2021-02-25T17:56:48.695324Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "\n",
    "# create fileid column \n",
    "df = pd.DataFrame(reuters.fileids(\"acq\"), columns=['fileid'])\n",
    "# load raw texts\n",
    "df['raw'] = df['fileid'].progress_map(lambda f: reuters.raw(f))\n",
    "# set index to numeric id\n",
    "df.index = df['fileid'].map(lambda f: int(f.split('/')[1]))\n",
    "df.index.name = None\n",
    "df = df.drop(columns=['fileid']).sort_index()\n",
    "\n",
    "df.sample(3, random_state=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see from the example, we will still need some data cleaning before we can expect to get reasonably good results during named entity recognition. First, we separate headlines from the actual news text by splitting at the first newline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T17:56:49.373130Z",
     "start_time": "2021-02-25T17:56:49.091367Z"
    }
   },
   "outputs": [],
   "source": [
    "df[['headline', 'raw_text']] = df.progress_apply(lambda row: row['raw'].split('\\n', 1), \n",
    "                                        axis='columns', result_type='expand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the adapted data cleaning blueprint from Chapter 4 for to remove some disturbing artifacts, substitute some abbreviations (like \"dlr\" for dollar) and repair some typos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T17:56:49.530039Z",
     "start_time": "2021-02-25T17:56:49.484828Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = text.replace('&lt;','<') # html escape\n",
    "    text = re.sub(r'[<>]', '\"', text) # quotation marks instead of <>\n",
    "    text = re.sub(r'[ ]*\"[A-Z\\.]+\"', '', text) # drop stock symbols\n",
    "    text = re.sub(r'[ ]*\\([A-Z\\.]+\\)', '', text) # drop stock symbols\n",
    "    text = re.sub(r'\\bdlr(s?)\\b', r'dollar\\1', text, flags=re.I)\n",
    "    text = re.sub(r'\\bmln(s?)\\b', r'million\\1', text, flags=re.I)\n",
    "    text = re.sub(r'\\bpct\\b', r'%', text, flags=re.I)\n",
    "    # normalize INC to Inc\n",
    "    text = re.sub(r'\\b(Co|Corp|Inc|Plc|Ltd)\\b', lambda m: m.expand(r'\\1').capitalize(), text, flags=re.I)\n",
    "    text = re.sub(r'\"', r'', text) # quotation marks\n",
    "    text = re.sub(r'\\s+', ' ', text) # multiple whitespace by one\n",
    "    text = re.sub(r'acquisiton', 'acquisition', text) # typo\n",
    "    text = re.sub(r'Nippon bLife', 'Nippon Life', text) # typo\n",
    "    text = re.sub(r'COMSAT.COMSAT', 'COMSAT. COMSAT', text) # missing space at end of sentence\n",
    "    #text = re.sub(r'Audio/Video', 'Audio-Video', text) # missing space at end of sentence\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's have a look at the result of our data cleaning steps :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T17:56:49.700649Z",
     "start_time": "2021-02-25T17:56:49.674130Z"
    }
   },
   "outputs": [],
   "source": [
    "# that's what the substitutions do\n",
    "texts = [\n",
    "\"\"\"Trafalgar House Plc &lt;TRAF.L> said it has\\n  acquired the entire share capital \n",
    "of &lt;Capital Homes Inc> of the\\n  U.S. For 20 mln dlrs in cash.\"\"\",\n",
    "\"\"\"Equiticorp Holdings Ltd &lt;EQUW.WE> now owns\\n  or has received acceptances \n",
    "representing 59.93 pct of the\\n  issued ordinary share capital of \n",
    "Guinness Peat Group Plc\\n  &lt;GNSP.L>, Equiticorp said in a statement.\"\"\",\n",
    "\"\"\"Computer Terminal Systems Inc said it has completed the sale of 200,000 shares \n",
    "of its common stock, and warrants to acquire an additional one mln shares, \n",
    "to \"Sedio N.V.\" of Lugano, Switzerland for 50,000 dlrs.\"\"\",\n",
    "\"\"\"North American Group Ltd said it has a definitive agreement \n",
    "to buy 100  pct of Pioneer Business Group Inc of Atlanta.\"\"\" \n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    print(clean(text), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply it to the `raw_text` and create a new `text` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T17:56:50.421509Z",
     "start_time": "2021-02-25T17:56:49.775182Z"
    }
   },
   "outputs": [],
   "source": [
    "df['text'] = df['raw_text'].progress_map(clean)\n",
    "df['headline'] = df['headline'].progress_map(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The newly created column `text` contains the cleaned articles. But we have one disturbing artifact left in the data: a few articles, like the second one in the sample above, consist only of capital letters. In fact, here the raw text is identical to the headlines. We finally drop those because named entity recognition will not yield useful results on such a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T17:56:50.587066Z",
     "start_time": "2021-02-25T17:56:50.525803Z"
    }
   },
   "outputs": [],
   "source": [
    "# we will drop these articles with only capital letters\n",
    "df[df['raw_text'].map(lambda t: t.isupper())][['headline', 'raw_text']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T17:56:50.790806Z",
     "start_time": "2021-02-25T17:56:50.737887Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop articles with only capital letters\n",
    "df = df[df['raw_text'].map(lambda t: not t.isupper())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T17:56:50.895754Z",
     "start_time": "2021-02-25T17:56:50.869267Z"
    }
   },
   "outputs": [],
   "source": [
    "# this is our clean data set\n",
    "df[['headline', 'text']].sample(3, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T17:56:50.984257Z",
     "start_time": "2021-02-25T17:56:50.962373Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book section continues ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named-Entity Recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T17:56:51.715412Z",
     "start_time": "2021-02-25T17:56:51.062023Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm') \n",
    "print(*nlp.pipeline, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T17:56:51.835862Z",
     "start_time": "2021-02-25T17:56:51.786381Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"\"\"Hughes Tool Co Chairman W.A. Kistler said its merger with \n",
    "Baker International Corp was still under consideration.\n",
    "We hope to come soon to a mutual agreement, Kistler said.\n",
    "The directors of Baker filed a law suit in Texas to force Hughes \n",
    "to complete the merger.\"\"\"\n",
    "text = re.sub(r'\\s+', ' ', text).strip() ###\n",
    "doc = nlp(text)\n",
    "\n",
    "print(*[(e.text, e.label_) for e in doc.ents], sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T17:56:51.982257Z",
     "start_time": "2021-02-25T17:56:51.938019Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Rule-based Named-Entity Recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T17:56:52.190628Z",
     "start_time": "2021-02-25T17:56:52.134791Z"
    }
   },
   "outputs": [],
   "source": [
    "reset_pipeline(nlp, pipes=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T17:56:52.321838Z",
     "start_time": "2021-02-25T17:56:52.281818Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "departments = ['Justice', 'Transportation']\n",
    "patterns = [{\"label\": \"GOV\", \n",
    "             \"pattern\": [{\"TEXT\": \"U.S.\", \"OP\": \"?\"},\n",
    "                         {\"TEXT\": \"Department\"}, {\"TEXT\": \"of\"}, \n",
    "                         {\"TEXT\": {\"IN\": departments}, \"ENT_TYPE\": \"ORG\"}]},\n",
    "             {\"label\": \"GOV\", \n",
    "              \"pattern\": [{\"TEXT\": \"U.S.\", \"OP\": \"?\"},\n",
    "                          {\"TEXT\": {\"IN\": departments}, \"ENT_TYPE\": \"ORG\"},\n",
    "                          {\"TEXT\": \"Department\"}]},\n",
    "             {\"label\": \"GOV\",\n",
    "              \"pattern\": [{\"TEXT\": \"Securities\"}, {\"TEXT\": \"and\"},\n",
    "                          {\"TEXT\": \"Exchange\"}, {\"TEXT\": \"Commission\"}]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T17:56:52.611785Z",
     "start_time": "2021-02-25T17:56:52.563140Z"
    }
   },
   "outputs": [],
   "source": [
    "# not in book, but useful if you modify the rules\n",
    "if nlp.has_pipe('entity_ruler'):\n",
    "    nlp.remove_pipe('entity_ruler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T17:57:07.388387Z",
     "start_time": "2021-02-25T17:57:07.351428Z"
    }
   },
   "outputs": [],
   "source": [
    "entity_ruler.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T17:57:16.413646Z",
     "start_time": "2021-02-25T17:57:16.368739Z"
    }
   },
   "outputs": [],
   "source": [
    "entity_ruler = EntityRuler(nlp, patterns=patterns, overwrite_ents=True)\n",
    "nlp.add_pipe('entity_ruler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Justice Department is an alias for the U.S. Department of Justice.\n",
    "Department of Transportation and the Securities and Exchange Commission\n",
    "are government organisations, but the Sales Department is not.\"\"\"\n",
    "#text = re.sub(r'\\s+', ' ', text).strip() ###\n",
    "\n",
    "doc = nlp(text)\n",
    "# print(*[([t.text for t in e], e.label_) for e in doc.ents], sep='\\n') ###\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Normalizing Named-Entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_pipeline(nlp, [entity_ruler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Baker International's shares climbed on the New York Stock Exchange.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "print(*[([t.text for t in e], e.label_) for e in doc.ents], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bug fix (2021-02-25): added condition `len(ent) > 0` because entity contains just a determiner (which does not make sense in practise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "def norm_entities(doc):\n",
    "    ents = []\n",
    "    for ent in doc.ents:\n",
    "        if ent[0].pos_ == \"DET\": # leading article\n",
    "            ent = Span(doc, ent.start+1, ent.end, label=ent.label)\n",
    "        if len(ent) > 0:\n",
    "            if ent[-1].pos_ == \"PART\": # trailing particle like 's\n",
    "                ent = Span(doc, ent.start, ent.end-1, label=ent.label)\n",
    "            if len(ent) > 0:\n",
    "                ents.append(ent)\n",
    "    doc.ents = tuple(ents)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(norm_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "print(*[([t.text for t in e], e.label_) for e in doc.ents], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not in book\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Entity Tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.pipeline import merge_entities\n",
    "if nlp.has_pipe('merge_entities'): ###\n",
    "    _ = nlp.remove_pipe('merge_entities') ###\n",
    "nlp.add_pipe(merge_entities)\n",
    "\n",
    "doc = nlp(text)\n",
    "print(*[(t.text, t.ent_type_) for t in doc if t.ent_type_ != ''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the NER Pipeline on Sample Data (not in book)\n",
    "\n",
    "Take random samples from the text and display the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_pipeline(nlp, [entity_ruler, norm_entities, merge_entities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = df['text'].sample(1).index[0]\n",
    "print(\"Text Number:\", i)\n",
    "\n",
    "text = df['text'].loc[i][:600]\n",
    "text = re.sub(r'\\s+', ' ', text.strip())\n",
    "\n",
    "print(text)\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style='ent', jupyter=True)\n",
    "print(*[([t.text for t in e], e.label_) for e in doc.ents], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 500\n",
    "\n",
    "# blueprint function to show tokens with entity attributes\n",
    "display_ner(doc, include_punct=True).query('ent_type != \"\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coreference Resolution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Using spaCy's Token Extensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not in book, but usefule if you modify the extension\n",
    "from spacy.tokens import Token\n",
    "\n",
    "if Token.has_extension('ref_n'):\n",
    "    _ = Token.remove_extension('ref_n') \n",
    "if Token.has_extension('ref_t'):\n",
    "    _ = Token.remove_extension('ref_t') \n",
    "if Token.has_extension('ref_t_'):\n",
    "    _ = Token.remove_extension('ref_t_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Token\n",
    "Token.set_extension('ref_n', default='')\n",
    "Token.set_extension('ref_t', default='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_coref(doc):\n",
    "    for e in doc.ents:\n",
    "        if e.label_ in ['ORG', 'GOV', 'PERSON']:\n",
    "            e[0]._.ref_n, e[0]._.ref_t = e.text, e.label_\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Alias Resolution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blueprints.knowledge import alias_lookup\n",
    "\n",
    "for token in ['Transportation Department', 'DOT', 'SEC', 'TWA']:\n",
    "    print(token, ':', alias_lookup[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_pipeline(nlp, [entity_ruler, norm_entities, merge_entities, init_coref])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alias_resolver(doc):\n",
    "    \"\"\"Lookup aliases and store result in ref_t, ref_n\"\"\"\n",
    "    for ent in doc.ents:\n",
    "        token = ent[0].text\n",
    "        if token in alias_lookup:\n",
    "            a_name, a_type = alias_lookup[token]\n",
    "            ent[0]._.ref_n, ent[0]._.ref_t = a_name, a_type\n",
    "    return propagate_ent_type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate_ent_type(doc):\n",
    "    \"\"\"propagate entity type stored in ref_t\"\"\"\n",
    "    ents = []\n",
    "    for e in doc.ents:\n",
    "        if e[0]._.ref_n != '': # if e is a coreference\n",
    "            e = Span(doc, e.start, e.end, label=e[0]._.ref_t)\n",
    "        ents.append(e)\n",
    "    doc.ents = tuple(ents)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(alias_resolver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blueprints.knowledge import display_ner\n",
    "text = \"\"\"The deal of Trans World Airlines is under investigation by the\n",
    "U.S. Department of Transportation.\n",
    "The Transportation Department will block the deal of TWA.\"\"\"\n",
    "text = re.sub(r'\\s+', ' ', text).strip() ###\n",
    "doc = nlp(text)\n",
    "display_ner(doc).query(\"ref_n != ''\")[['text', 'ent_type', 'ref_n', 'ref_t']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Resolving Name Variations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_pipeline(nlp, [entity_ruler, norm_entities, merge_entities, init_coref, alias_resolver])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Hughes Tool Co Chairman W.A. Kistler said its merger with \n",
    "Baker International Corp. was still under consideration.\n",
    "We hope to come to a mutual agreement, Kistler said.\n",
    "Baker will force Hughes to complete the merger.\n",
    "\"\"\"\n",
    "text = re.sub(r'\\s+', ' ', text).strip() ### \n",
    "\n",
    "doc = nlp(text) \n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_match(m1, m2):\n",
    "    m2 = re.sub(r'[()\\.]', '', m2) # ignore parentheses and dots\n",
    "    m2 = r'\\b' + m2 + r'\\b' # \\b marks word boundary\n",
    "    m2 = re.sub(r'\\s+', r'\\\\b.*\\\\b', m2)\n",
    "    return re.search(m2, m1, flags=re.I) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_resolver(doc):\n",
    "    \"\"\"create name-based reference to e1 as primary mention of e2\"\"\"\n",
    "    ents = [e for e in doc.ents if e.label_ in ['ORG', 'PERSON']]\n",
    "    for i, e1 in enumerate(ents):\n",
    "        for e2 in ents[i+1:]:\n",
    "            if name_match(e1[0]._.ref_n, e2[0].text): \n",
    "                e2[0]._.ref_n = e1[0]._.ref_n\n",
    "                e2[0]._.ref_t = e1[0]._.ref_t\n",
    "    return propagate_ent_type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(name_resolver)\n",
    "\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_ner(doc).query(\"ref_n != ''\")[['text', 'ent_type', 'ref_n', 'ref_t']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Name Coreference Resolution Sample Data (not in book)\n",
    "\n",
    "Take random samples from the text and display the result. You may find examples where the resolution is not working correctly. We have put the emphasis on the simplicity of rules, so there will be cases in which they don't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_pipeline(nlp, [entity_ruler, norm_entities, merge_entities, init_coref, alias_resolver, name_resolver])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not in the book:\n",
    "# pick random examples to test the string matching\n",
    "\n",
    "i = df['text'].sample(1).index[0]\n",
    "i = 10\n",
    "print(\"Text Number:\", i)\n",
    "\n",
    "text = df['text'].loc[i]#[:300]\n",
    "# print(text)\n",
    "\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style='ent', jupyter=True)\n",
    "\n",
    "display_ner(doc).query(\"ref_n != ''\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Anaphora Resolution with NeuralCoref\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Hughes Tool Co said its merger with Baker\n",
    "was still under consideration. Hughes had a board meeting today.\n",
    "W.A. Kistler mentioned that the company hopes for a mutual agreement.\n",
    "He is reasonably confident.\"\"\"\n",
    "text = re.sub(r'\\s+', ' ', text).strip() ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_pipeline(nlp, [entity_ruler, norm_entities, merge_entities, \n",
    "                     init_coref, alias_resolver, name_resolver])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralcoref import NeuralCoref\n",
    "neural_coref = NeuralCoref(nlp.vocab, greedyness=0.45)\n",
    "nlp.add_pipe(neural_coref, name='neural_coref')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "print(*doc._.coref_clusters, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not in the book: Try the visualization of NeuralCoref!\n",
    "\n",
    "https://huggingface.co/coref/?text=Hughes%20Tool%20Co%20said%20its%20merger%20with%20Baker%20was%20still%20under%20consideration.%20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anaphor_coref(doc):\n",
    "    \"\"\"anaphora resolution\"\"\"\n",
    "    for token in doc:\n",
    "        # if token is coref and not already dereferenced\n",
    "        if token._.in_coref and token._.ref_n == '': \n",
    "            ref_span = token._.coref_clusters[0].main # get referred span\n",
    "            if len(ref_span) <= 3: # consider only short spans\n",
    "                for ref in ref_span: # find first dereferenced entity\n",
    "                    if ref._.ref_n != '':\n",
    "                        token._.ref_n = ref._.ref_n\n",
    "                        token._.ref_t = ref._.ref_t\n",
    "                        break\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nlp.has_pipe('anaphor_coref'): ###\n",
    "    nlp.remove_pipe('anaphor_coref') ###\n",
    "nlp.add_pipe(anaphor_coref)\n",
    "doc = nlp(text)\n",
    "display_ner(doc).query(\"ref_n != ''\") \\\n",
    "  [['text', 'ent_type', 'main_coref', 'ref_n', 'ref_t']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_legal_suffix(text):\n",
    "    return re.sub(r'(\\s+and)?(\\s+|\\b(Co|Corp|Inc|Plc|Ltd)\\b\\.?)*$', '', text)\n",
    "\n",
    "print(strip_legal_suffix('Hughes Tool Co'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_names(doc):\n",
    "    for t in doc:\n",
    "        if t._.ref_n != '' and t._.ref_t in ['ORG']:\n",
    "            t._.ref_n = strip_legal_suffix(t._.ref_n)\n",
    "            if t._.ref_n == '':\n",
    "                t._.ref_t = ''\n",
    "                \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(norm_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Linking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Coreference Resolution (not in book)\n",
    "\n",
    "Not in the book, but a good demonstration of what works good and what doesn't work, yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate pipeline\n",
    "reset_pipeline(nlp, [entity_ruler, norm_entities, merge_entities, \n",
    "                     init_coref, alias_resolver, name_resolver,\n",
    "                     neural_coref, anaphor_coref, norm_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick random examples and test\n",
    "\n",
    "i = df['text'].sample(1).index[0]\n",
    "i = 2948 # 1862, 1836,2948,7650,3013,2950,3095\n",
    "print(\"Text Number:\", i)\n",
    "\n",
    "text = df['text'].loc[i][:500]\n",
    "print(text)\n",
    "\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style='ent', jupyter=True)\n",
    "\n",
    "display_ner(doc).query(\"ref_n != ''\")[['text', 'ent_type', 'main_coref', 'ref_n', 'ref_t']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blueprint: Creating a Cooccurence Graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Largest connected component of the cooccurrence graph generated from the Reuters corpus**  \n",
    "The visualization was prepared with the help of [Gephi](https://gephi.org/).\n",
    "<img src=\"figures/cooc.png\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Cooccurrences from a Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def extract_coocs(doc, include_types):\n",
    "    ents = set([(e[0]._.ref_n, e[0]._.ref_t) \n",
    "                for e in doc.ents if e[0]._.ref_t in include_types])\n",
    "    yield from combinations(sorted(ents), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_pipeline(nlp, [entity_ruler, norm_entities, merge_entities, \n",
    "                     init_coref, alias_resolver, name_resolver,\n",
    "                     neural_coref, anaphor_coref, norm_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "batches = math.ceil(len(df)/batch_size) ###\n",
    "\n",
    "coocs = []\n",
    "for i in tqdm(range(0, len(df), batch_size), total=batches):\n",
    "    docs = nlp.pipe(df['text'][i:i+batch_size],\n",
    "                    disable=['neural_coref', 'anaphor_coref'])\n",
    "    for j, doc in enumerate(docs):\n",
    "        coocs.extend([(df.index[i+j], *c) \n",
    "                      for c in extract_coocs(doc, ['ORG', 'GOV'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*coocs[:3], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coocs = [([id], *e1, *e2) for (id, e1, e2) in coocs]\n",
    "cooc_df = pd.DataFrame.from_records(coocs, \n",
    "             columns=('article_id', 'ent1', 'type1', 'ent2', 'type2')) \n",
    "cooc_df = cooc_df.groupby(['ent1', 'type1', 'ent2', 'type2'])['article_id'] \\\n",
    "                 .agg(['count', 'sum']) \\\n",
    "                 .rename(columns={'count': 'freq', 'sum': 'articles'}) \\\n",
    "                 .reset_index().sort_values('freq', ascending=False)\n",
    "cooc_df['articles'] = cooc_df['articles'].map(\n",
    "                        lambda lst: ','.join([str(a) for a in lst[:5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Graph with Gephi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "graph = nx.from_pandas_edgelist(\n",
    "           cooc_df[['ent1', 'ent2', 'articles', 'freq']] \\\n",
    "           .query('freq > 3').rename(columns={'freq': 'weight'}),\n",
    "           source='ent1', target='ent2', edge_attr=True)\n",
    "\n",
    "nx.readwrite.write_gexf(graph, 'cooc.gexf', encoding='utf-8', \n",
    "                        prettyprint=True, version='1.2draft')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Graph with NetworkX (not in book)\n",
    "\n",
    "We can also use NetworkX for drawing, it's just not that nice. By executing the code below you will see more nodes than in the book, where we manually removed several nodes for the sake of clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the greatest component (connected subgraph)\n",
    "# and plot only that one\n",
    "giant_component = sorted(nx.connected_components(graph), key=len, reverse=True)\n",
    "graph = graph.subgraph(giant_component[0])\n",
    "\n",
    "pos = nx.kamada_kawai_layout(graph, weight='weight')\n",
    "# pos = nx.fruchterman_reingold_layout(graph, weight='weight')\n",
    "# pos = nx.circular_layout(graph)\n",
    "\n",
    "_ = plt.figure(figsize=(20, 20))\n",
    "nx.draw(graph, pos, \n",
    "        node_size=1000, \n",
    "        node_color='skyblue',\n",
    "        alpha=0.8,\n",
    "        with_labels = True)\n",
    "plt.title('Graph Visualization', size=15)\n",
    "\n",
    "for (node1,node2,data) in graph.edges(data=True):\n",
    "    width = data['weight'] \n",
    "    _ = nx.draw_networkx_edges(graph,pos,\n",
    "                               edgelist=[(node1, node2)],\n",
    "                               width=width,\n",
    "                               edge_color='#505050',\n",
    "                               alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Identifying Acronyms (not in book)\n",
    "\n",
    "It is very easy to generate a very good list of suggestions for acronyms if you search for frequent cooccurrences of acronyms. \n",
    "\n",
    "To find possible acronyms in the cooccurrence data frame, we look for all tuples that have an acronym (all capital letters) either as source or as target. As additional conditions, we require that the first letter in both is the same and the combination exists more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_pipeline(nlp, [entity_ruler, norm_entities, merge_entities, \n",
    "                     init_coref, name_resolver, norm_names]) # no alias resolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "batches = math.ceil(len(df)/batch_size) ###\n",
    "\n",
    "coocs = []\n",
    "for i in tqdm(range(0, len(df), batch_size), total=batches):\n",
    "    docs = nlp.pipe(df['text'][i:i+batch_size])\n",
    "    for j, doc in enumerate(docs):\n",
    "        coocs.extend([(df.index[i+j], *c) for c in extract_coocs(doc, ['ORG', 'GOV'])])\n",
    "\n",
    "coocs = [([id], *e1, *e2) for (id, e1, e2) in coocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc_df = pd.DataFrame.from_records(coocs, \n",
    "             columns=('article_id', 'ent1', 'type1', 'ent2', 'type2')) \n",
    "cooc_df = cooc_df.groupby(['ent1', 'ent2'])['article_id'] \\\n",
    "                 .agg(['count']).rename(columns={'count': 'freq'}) \\\n",
    "                 .reset_index().sort_values('freq', ascending=False)\n",
    "\n",
    "acro_pattern = (cooc_df['ent1'].str.isupper() | cooc_df['ent2'].str.isupper()) & \\\n",
    "               (cooc_df['ent1'].str[:1] == cooc_df['ent2'].str[:1]) & \\\n",
    "               (cooc_df['freq'] > 1)\n",
    "\n",
    "print(len(cooc_df[acro_pattern]))\n",
    "cooc_df[acro_pattern][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our corpus, this yields about 40 potential acronyms.\n",
    "\n",
    "We save them to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv\n",
    "cooc_df[acro_pattern][['ent1', 'ent2']] \\\n",
    "  .sort_values(['ent1', 'ent2']) \\\n",
    "  .to_csv('possible_acronyms.txt', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file has to be curated manually. After cleaning, we load the remaining acronyms and convert them to a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curate manually the csv\n",
    "acro_df = pd.read_csv('possible_acronyms.txt')\n",
    "acro_df.set_index('ent1')['ent2'].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We took this list, and curated it to create a dictionary that maps acronyms to their long names. It is  provided in the blueprints package for this chapter and part of `alias_lookup`. Here are some example entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blueprints.knowledge import _acronyms\n",
    "\n",
    "for acro in ['TWA', 'UCPB', 'SEC', 'DOT']:\n",
    "    print(acro, ' --> ', alias_lookup[acro])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relation Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Relation Extraction by Phrase Matching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use large model, otherwise the examples look different!\n",
    "# to make it work on Colab, we need to import the model directly\n",
    "# usually you would use nlp = spacy.load('en_core_web_lg') \n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "# need to re-create the entity ruler after reloading nlp\n",
    "# because new entity type 'GOV' needs to be added to nlp.vocab\n",
    "entity_ruler = EntityRuler(nlp, patterns=patterns, overwrite_ents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate pipeline\n",
    "reset_pipeline(nlp, [entity_ruler, norm_entities, merge_entities, \n",
    "                     init_coref, alias_resolver, name_resolver, norm_names,\n",
    "                     neural_coref, anaphor_coref])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Fujitsu plans to acquire 80% of Fairchild Corp, an industrial unit\n",
    "of Schlumberger.\"\"\"\n",
    "text = re.sub('\\s+', ' ', text).strip() ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "acq_synonyms = ['acquire', 'buy', 'purchase']\n",
    "pattern = [{'_': {'ref_t': 'ORG'}}, # subject\n",
    "           {'_': {'ref_t': {'NOT_IN': ['ORG']}}, 'OP': '*'},\n",
    "           {'POS': 'VERB', 'LEMMA': {'IN': acq_synonyms}},\n",
    "           {'_': {'ref_t': {'NOT_IN': ['ORG']}}, 'OP': '*'},\n",
    "           {'_': {'ref_t': 'ORG'}}] # object\n",
    "matcher.add('acquires', None, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_synonyms = ['subsidiary', 'unit']\n",
    "pattern = [{'_': {'ref_t': 'ORG'}}, # subject\n",
    "           {'_': {'ref_t': {'NOT_IN': ['ORG']}}, \n",
    "            'POS': {'NOT_IN': ['VERB']}, 'OP': '*'},\n",
    "           {'LOWER': {'IN': subs_synonyms}}, {'TEXT': 'of'},\n",
    "           {'_': {'ref_t': {'NOT_IN': ['ORG']}}, \n",
    "            'POS': {'NOT_IN': ['VERB']}, 'OP': '*'},\n",
    "           {'_': {'ref_t': 'ORG'}}] # object\n",
    "matcher.add('subsidiary-of', None, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rel_match(doc, matcher):\n",
    "    for sent in doc.sents:\n",
    "        for match_id, start, end in matcher(sent):\n",
    "            span = sent[start:end]  # matched span\n",
    "            pred = nlp.vocab.strings[match_id] # rule name\n",
    "            subj, obj = span[0], span[-1]\n",
    "            if pred.startswith('rev-'): # reversed relation\n",
    "                subj, obj = obj, subj\n",
    "                pred = pred[4:]\n",
    "            yield ((subj._.ref_n, subj._.ref_t), pred, \n",
    "                   (obj._.ref_n, obj._.ref_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{'_': {'ref_t': 'ORG'}}, # subject\n",
    "           {'LOWER': {'IN': subs_synonyms}}, # predicate\n",
    "           {'_': {'ref_t': 'ORG'}}] # object\n",
    "matcher.add('rev-subsidiary-of', None, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Fujitsu plans to acquire 80% of Fairchild Corp, an industrial unit \n",
    "of Schlumberger. The Schlumberger unit Fairchild Corp received an offer.\"\"\"\n",
    "text = re.sub('\\s+', ' ', text) ###\n",
    "doc = nlp(text)\n",
    "print(*extract_rel_match(doc, matcher), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Fairchild Corp was acquired by Fujitsu.\"\n",
    "print(*extract_rel_match(nlp(text), matcher), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Fujitsu, a competitor of NEC, acquired Fairchild Corp.\"\n",
    "print(*extract_rel_match(nlp(text), matcher), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if matcher.has_key(\"acquires\"): \n",
    "    matcher.remove(\"acquires\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Relation Extraction using Dependency Trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate pipeline\n",
    "reset_pipeline(nlp, [norm_entities, merge_entities, \n",
    "                     init_coref, alias_resolver, name_resolver, norm_names,\n",
    "                     neural_coref, anaphor_coref])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Fujitsu, a competitor of NEC, acquired Fairchild Corp.\"\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style='dep', jupyter=True, \n",
    "                options={'compact': False, 'distance': 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Fairchild Corp was acquired by Fujitsu.\"\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'compact': False, 'distance': 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the longer part of the code, that was skipped in the book.\n",
    "\n",
    "# Actually we search for the shortest path between the\n",
    "# subject running through our predicate (verb) to the object.\n",
    "# subject and object are organizations in our examples.\n",
    "\n",
    "# Here are the three helper functions omitted in the book:\n",
    "# - bfs: breadth first searching the closest subject/object \n",
    "# - is_passive: checks if noun or verb is in passive form\n",
    "# - find_subj: searches left part of tree for subject\n",
    "# - find_obj: searches right part of tree for object\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "def bfs(root, ent_type, deps, first_dep_only=False):\n",
    "    \"\"\"Return first child of root (included) that matches\n",
    "    ent_type and dependency list by breadth first search.\n",
    "    Search stops after first dependency match if first_dep_only\n",
    "    (used for subject search - do not \"jump\" over subjects)\"\"\"\n",
    "    to_visit = deque([root]) # queue for bfs\n",
    "\n",
    "    while len(to_visit) > 0:\n",
    "        child = to_visit.popleft()\n",
    "        # print(\"child\", child, child.dep_)\n",
    "        if child.dep_ in deps:\n",
    "            if child._.ref_t == ent_type:\n",
    "                return child\n",
    "            elif first_dep_only: # first match (subjects)\n",
    "                return None\n",
    "        elif child.dep_ == 'compound' and \\\n",
    "             child.head.dep_ in deps and \\\n",
    "             child._.ref_t == ent_type: # check if contained in compound\n",
    "            return child\n",
    "        to_visit.extend(list(child.children))\n",
    "    return None\n",
    "\n",
    "def is_passive(token):\n",
    "    if token.dep_.endswith('pass'): # noun\n",
    "        return True\n",
    "    for left in token.lefts: # verb\n",
    "        if left.dep_ == 'auxpass':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def find_subj(pred, ent_type, passive):\n",
    "    \"\"\"Find closest subject in predicates left subtree or\n",
    "    predicates parent's left subtree (recursive).\n",
    "    Has a filter on organizations.\"\"\"\n",
    "    for left in pred.lefts:\n",
    "        if passive: # if pred is passive, search for passive subject\n",
    "            subj = bfs(left, ent_type, ['nsubjpass', 'nsubj:pass'], True)\n",
    "        else:\n",
    "            subj = bfs(left, ent_type, ['nsubj'], True)\n",
    "        if subj is not None: # found it!\n",
    "            return subj\n",
    "    if pred.head != pred and not is_passive(pred): \n",
    "        return find_subj(pred.head, ent_type, passive) # climb up left subtree\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def find_obj(pred, ent_type, excl_prepos):\n",
    "    \"\"\"Find closest object in predicates right subtree.\n",
    "    Skip prepositional objects if the preposition is in exclude list.\n",
    "    Has a filter on organizations.\"\"\"\n",
    "    for right in pred.rights:\n",
    "        obj = bfs(right, ent_type, ['dobj', 'pobj', 'iobj', 'obj', 'obl'])\n",
    "        if obj is not None:\n",
    "            if obj.dep_ == 'pobj' and obj.head.lemma_.lower() in excl_prepos: # check preposition\n",
    "                continue\n",
    "            return obj\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rel_dep(doc, pred_name, pred_synonyms, excl_prepos=[]):\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'VERB' and token.lemma_ in pred_synonyms:\n",
    "            pred = token\n",
    "            passive = is_passive(pred)\n",
    "            subj = find_subj(pred, 'ORG', passive)\n",
    "            if subj is not None:\n",
    "                obj = find_obj(pred, 'ORG', excl_prepos)\n",
    "                if obj is not None:\n",
    "                    if passive: # switch roles\n",
    "                        obj, subj = subj, obj\n",
    "                    yield ((subj._.ref_n, subj._.ref_t), pred_name, \n",
    "                           (obj._.ref_n, obj._.ref_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Fujitsu said that Schlumberger Ltd has arranged \n",
    "to sell its stake in Fairchild Inc.\"\"\"\n",
    "doc = nlp(text)\n",
    "print(*extract_rel_dep(doc, 'sells', ['sell']), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Schlumberger Ltd has arranged to sell to Fujitsu its stake in Fairchild Inc.\"\n",
    "doc = nlp(text)\n",
    "print(*extract_rel_dep(doc, 'sells', ['sell']), sep='\\n')\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'compact': False, 'distance': 80})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"A:\", *extract_rel_dep(doc, 'sells', ['sell']))\n",
    "print(\"B:\", *extract_rel_dep(doc, 'sells', ['sell'], ['to', 'from']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [ \n",
    "     \"Fairchild Corp was bought by Fujitsu.\", # 1\n",
    "     \"Fujitsu, a competitor of NEC Co, acquired Fairchild Inc.\", # 2\n",
    "     \"Fujitsu is expanding.\" + \n",
    "     \"The company made an offer to acquire 80% of Fairchild Inc.\", # 3\n",
    "     \"Fujitsu plans to acquire 80% of Fairchild Corp.\", # 4\n",
    "     \"Fujitsu plans not to acquire Fairchild Corp.\", # 5\n",
    "     \"The competition forced Fujitsu to aquire Fairchild Corp.\" # 6\n",
    "]\n",
    "\n",
    "acq_synonyms = ['acquire', 'buy', 'purchase']\n",
    "for i, text in enumerate(texts):\n",
    "    doc = nlp(text)\n",
    "    rels = extract_rel_dep(doc, 'acquires', acq_synonyms, ['to', 'from'])\n",
    "    print(f'{i+1}:', *rels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Knowledge Graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On Colab**: Choose \"Runtime\"&rarr;\"Change Runtime Type\"&rarr;\"GPU\" to benefit from the GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if spacy.prefer_gpu():\n",
    "    print(\"Working on GPU.\")\n",
    "else:\n",
    "    print(\"No GPU found, working on CPU.\")\n",
    "\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to re-create the entity ruler after reloading nlp\n",
    "# because new entity type 'GOV' needs to be added to nlp.vocab\n",
    "entity_ruler = EntityRuler(nlp, patterns=patterns, overwrite_ents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipes = [entity_ruler, norm_entities, merge_entities,\n",
    "         init_coref, alias_resolver, name_resolver, \n",
    "         neural_coref, anaphor_coref, norm_names]\n",
    "for pipe in pipes:\n",
    "    nlp.add_pipe(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate matcher - same definition as above for these rules\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "subs_synonyms = ['subsidiary', 'unit']\n",
    "pattern = [{'_': {'ref_t': 'ORG'}}, # subject\n",
    "           {'_': {'ref_t': {'NOT_IN': ['ORG']}}, 'POS': {'NOT_IN': ['VERB']}, 'OP': '*'},\n",
    "           {'LOWER': {'IN': subs_synonyms}}, # predicate\n",
    "           {'TEXT': 'of'},\n",
    "           {'_': {'ref_t': {'NOT_IN': ['ORG']}}, 'POS': {'NOT_IN': ['VERB']}, 'OP': '*'},\n",
    "           {'_': {'ref_t': 'ORG'}}] # object\n",
    "matcher.add('subsidiary-of', None, pattern)\n",
    "\n",
    "pattern = [{'_': {'ref_t': 'ORG'}}, # subject\n",
    "           {'POS': 'PART', 'OP': '?'},\n",
    "           {'LOWER': {'IN': subs_synonyms}}, # predicate\n",
    "           {'_': {'ref_t': 'ORG'}}] # object\n",
    "matcher.add('rev-subsidiary-of', None, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceo_synonyms = ['chairman', 'president', 'director', 'ceo', 'executive']\n",
    "pattern = [{'ENT_TYPE': 'PERSON'},\n",
    "           {'ENT_TYPE': {'NOT_IN': ['ORG', 'PERSON']}, 'OP': '*'}, \n",
    "           {'LOWER': {'IN': ceo_synonyms}}, {'TEXT': 'of'},\n",
    "           {'ENT_TYPE': {'NOT_IN': ['ORG', 'PERSON']}, 'OP': '*'}, \n",
    "           {'ENT_TYPE': 'ORG'}] \n",
    "matcher.add('executive-of', None, pattern)\n",
    "\n",
    "pattern = [{'ENT_TYPE': 'ORG'}, \n",
    "           {'LOWER': {'IN': ceo_synonyms}},\n",
    "           {'ENT_TYPE': 'PERSON'}] \n",
    "matcher.add('rev-executive-of', None, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rels(doc):\n",
    "    yield from extract_rel_match(doc, matcher)\n",
    "    yield from extract_rel_dep(doc, 'acquires', acq_synonyms, ['to', 'from'])\n",
    "    yield from extract_rel_dep(doc, 'sells', ['sell'], ['to', 'from'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Relationship Extraction (not in book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Allied-Signal Inc and Schlumberger Ltd jointly announced \n",
    "that Schlumberger had acquired Allied-Signal's unit Neptune International. \n",
    "\"\"\"\n",
    "#text = df.text.loc[19975]\n",
    "\n",
    "text = re.sub(r'\\s+', ' ', text).strip()\n",
    "print(*textwrap.wrap(text, 100), sep='\\n')\n",
    "print()\n",
    "doc = nlp(text, disable='entity_ruler')\n",
    "#displacy.render(doc, style='ent')\n",
    "print(*extract_rels(doc), sep='\\n')\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'compact': False, 'distance': 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of Entities and Relations and Creation of Gephi-File (not in book)\n",
    "\n",
    "Batch-processing for entity extraction with subsequent relation extraction. Takes about 5 minutes,  80% of runtime for NeuralCoref."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "batch_size = 20\n",
    "batches = ceil(len(df) / batch_size) ###\n",
    "\n",
    "rels = []\n",
    "for i in tqdm(range(0, len(df), batch_size), total=batches):\n",
    "    docs = nlp.pipe(df['text'][i:i+batch_size])\n",
    "    for j, doc in enumerate(docs):\n",
    "        rels.extend([(df.index[i+j], *r) for r in extract_rels(doc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of the relation data frame including final curation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack subject and object \n",
    "rels = [(a_id, *subj, pred, *obj) for (a_id, subj, pred, obj) in rels]\n",
    "\n",
    "# create data frame\n",
    "rel_df = pd.DataFrame.from_records(rels, columns=('article_id', 'subj', 'subj_type', 'pred', 'obj', 'obj_type'))\n",
    "\n",
    "# false positives: subject cannot be object\n",
    "rel_df = rel_df.query('subj != obj')\n",
    "\n",
    "# filter entities that were not correctly detected\n",
    "# tokenizer produces \"-owned XYZ company\"\n",
    "rel_df = rel_df[~rel_df['subj'].str.startswith('-own')]\n",
    "rel_df = rel_df[~rel_df['obj'].str.startswith('-own')]\n",
    "\n",
    "# drop duplicate relations (within an article)\n",
    "rel_df = rel_df.drop_duplicates()\n",
    "\n",
    "# aggregate to produce one record per relation\n",
    "rel_df['article_id'] = rel_df['article_id'].map(lambda a: [a])\n",
    "rel_df = rel_df.groupby(['subj', 'subj_type', 'pred', 'obj', 'obj_type'])['article_id'] \\\n",
    "                 .agg(['count', 'sum']) \\\n",
    "                 .rename(columns={'count': 'freq', 'sum': 'articles'}) \\\n",
    "                 .reset_index().sort_values('freq', ascending=False)\n",
    "\n",
    "rel_df['articles'] = rel_df['articles'].map(lambda lst: ','.join(list(set([str(a) for a in lst]))))\n",
    "rel_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some statitics\n",
    "rel_df['pred'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try searching for a specific entity\n",
    "search = \"Trans World\"\n",
    "rel_df[(rel_df.subj.str.lower().str.contains(search.lower()) | \n",
    "        rel_df.obj.str.lower().str.contains(search.lower()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in fact, TWA acquires and sells parts of USAir according to the messages\n",
    "# look at a specific article\n",
    "text = df['text'][9487]\n",
    "print(*textwrap.wrap(text, 80), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the NetworkX graph be careful: We need a `MultiDiGraph` here, a directed graph allowing multiple edges between two nodes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx import MultiDiGraph\n",
    "\n",
    "graph = MultiDiGraph()\n",
    "for i, row in rel_df.iterrows():\n",
    "    graph.add_node(row['subj'], Type=row['subj_type'])\n",
    "    graph.add_node(row['obj'], Type=row['obj_type'])\n",
    "    _ = graph.add_edge(row['subj'], row['obj'], \n",
    "                   Articles=row['articles'], Rel=row['pred'])\n",
    "   \n",
    "nx.readwrite.write_gexf(graph, 'knowledge_graph.gexf', encoding='utf-8', \n",
    "                         prettyprint=True, version='1.2draft')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose merge strategy \"last\" when you load the data into Gephi, as relations with highest counts come last in the gexf file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book section continues ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't Blindly Trust the Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing Remarks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Reading\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "712px",
    "left": "100px",
    "top": "110.8px",
    "width": "199.985px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "472.4px",
    "left": "933.2px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
