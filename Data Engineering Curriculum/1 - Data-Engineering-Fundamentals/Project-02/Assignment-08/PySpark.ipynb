{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json parsing\n",
    "import json\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import explode, split\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the raw messages from the topic eduAssessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_messages = spark.read.format(\"kafka\")\\\n",
    ".option(\"kafka.bootstrap.servers\", \"kafka:29092\")\\\n",
    ".option(\"subscribe\",\"eduAssessments\").option(\"startingOffsets\", \"earliest\").option(\"endingOffsets\", \"latest\").load() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure everything is running well: Cache data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_messages.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ran perfectly, we have our dataframe with the data cached. Good sanity check before starting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualiza the Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_messages.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See the top 5 messages as a DataFrame (binary so far) - Visualization of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------+---------+------+--------------------+-------------+\n",
      "| key|               value|         topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+--------------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 6B 65 65 6...|eduAssessments|        0|     0|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|eduAssessments|        0|     1|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|eduAssessments|        0|     2|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|eduAssessments|        0|     3|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|eduAssessments|        0|     4|1969-12-31 23:59:...|            0|\n",
      "+----+--------------------+--------------+---------+------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_messages.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our analysis, we only care about the key-values pairs, which are encoded as binary by Kafka. All come from the same topic in this example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take all the values, and cast them as Strings. Extract the JSON from string messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
      "|        base_exam_id|certification|           exam_name|   keen_created_at|             keen_id|    keen_timestamp|max_attempts|           sequences|          started_at|        user_exam_id|\n",
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717442.735266|5a6745820eb8ab000...| 1516717442.735266|         1.0|Map(questions -> ...|2018-01-23T14:23:...|6d4089e4-bde5-4a2...|\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717377.639827|5a674541ab6b0a000...| 1516717377.639827|         1.0|Map(questions -> ...|2018-01-23T14:21:...|2fec1534-b41f-441...|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...| 1516738973.653394|5a67999d3ed3e3000...| 1516738973.653394|         1.0|Map(questions -> ...|2018-01-23T20:22:...|8edbc8a8-4d26-429...|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...|1516738921.1137421|5a6799694fc7c7000...|1516738921.1137421|         1.0|Map(questions -> ...|2018-01-23T20:21:...|c0ee680e-8892-4e6...|\n",
      "|6442707e-7488-11e...|        false|Introduction to B...| 1516737000.212122|5a6791e824fccd000...| 1516737000.212122|         1.0|Map(questions -> ...|2018-01-23T19:48:...|e4525b79-7904-405...|\n",
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = raw_messages.select(raw_messages.value.cast('string'))\n",
    "naive_extracted_messages = messages.rdd.map(lambda x: json.loads(x.value)).toDF()\n",
    "naive_extracted_messages.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this moment, the data estracted from JSON is a mess. The main reason being that we receive quite a nested JSON file, not a flat one. Some parts of the dataframe will be used as if, others we will use SparSQL to try to flatten the nests, specially in the Sequences field: where all the questions are stored. Let's first check the Schema of this DF. Also, because from sequences we need integer values (correct, incorrect, etc), we cannot cast to string yet, because those fields will become null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- base_exam_id: string (nullable = true)\n",
      " |-- certification: string (nullable = true)\n",
      " |-- exam_name: string (nullable = true)\n",
      " |-- keen_created_at: string (nullable = true)\n",
      " |-- keen_id: string (nullable = true)\n",
      " |-- keen_timestamp: string (nullable = true)\n",
      " |-- max_attempts: string (nullable = true)\n",
      " |-- sequences: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: array (valueContainsNull = true)\n",
      " |    |    |-- element: map (containsNull = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: boolean (valueContainsNull = true)\n",
      " |-- started_at: string (nullable = true)\n",
      " |-- user_exam_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "naive_extracted_messages.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nest become quite evident on the Sequences Field. Everything else is flat, and can be used as if. One way of solving it, because we're losing important information in the sequences field, is to enfornce a Schema from the data. The implied Schema works on the first level, but it doesn't work for nested levels. I'll enforce a Schema of the values that I care for this analysis: correct and total questions per attempt. The information in the questions map it's important, so we'll keep as is in the implied map, but we'll create a new DF with only the information need it for this analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_schema = StructType([StructField('base_exam_id', StringType(), True),\n",
    "                     StructField('certification', StringType(), True),\n",
    "                     StructField('exam_name', StringType(), True),\n",
    "                     StructField('keen_created_at', StringType(), True),\n",
    "                     StructField('keen_id', StringType(), True),\n",
    "                     StructField('keen_timestamp', StringType(), True),\n",
    "                     StructField('max_attempts', StringType(), True),\n",
    "                     StructField('sequences', StructType([\n",
    "                         StructField('attempt', IntegerType(), True),\n",
    "                         StructField('counts', StructType([\n",
    "                             StructField('all_correct', StringType(), True),\n",
    "                             StructField('correct', IntegerType(), True),\n",
    "                             StructField('total', IntegerType(), True)\n",
    "                         ]))]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+---------------+\n",
      "|        base_exam_id|certification|           exam_name|   keen_created_at|             keen_id|    keen_timestamp|max_attempts|      sequences|\n",
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+---------------+\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717442.735266|5a6745820eb8ab000...| 1516717442.735266|         1.0|[1,[false,2,4]]|\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717377.639827|5a674541ab6b0a000...| 1516717377.639827|         1.0|[1,[false,1,4]]|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...| 1516738973.653394|5a67999d3ed3e3000...| 1516738973.653394|         1.0|[1,[false,3,4]]|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...|1516738921.1137421|5a6799694fc7c7000...|1516738921.1137421|         1.0|[1,[false,2,4]]|\n",
      "|6442707e-7488-11e...|        false|Introduction to B...| 1516737000.212122|5a6791e824fccd000...| 1516737000.212122|         1.0|[1,[false,3,4]]|\n",
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "important_extracted_messages = messages.rdd.map(lambda x: json.loads(x.value)).toDF(schema=final_schema)\n",
    "important_extracted_messages.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we only have the important fields that we need, after enforcing a Schema on the JSON file. Now we can do SQL with the grades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction using SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Temp Table\n",
    "important_extracted_messages.registerTempTable('messages')\n",
    "naive_extracted_messages.registerTempTable('questions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|           exam_name|total|\n",
      "+--------------------+-----+\n",
      "|        Learning Git|  394|\n",
      "|Introduction to P...|  162|\n",
      "|Introduction to J...|  158|\n",
      "|Intermediate Pyth...|  158|\n",
      "|Learning to Progr...|  128|\n",
      "|Introduction to M...|  119|\n",
      "|Software Architec...|  109|\n",
      "|Beginning C# Prog...|   95|\n",
      "|    Learning Eclipse|   85|\n",
      "|Learning Apache M...|   80|\n",
      "|Beginning Program...|   79|\n",
      "|       Mastering Git|   77|\n",
      "|Introduction to B...|   75|\n",
      "|Advanced Machine ...|   67|\n",
      "|Learning Linux Sy...|   59|\n",
      "|JavaScript: The G...|   58|\n",
      "|        Learning SQL|   57|\n",
      "|Practical Java Pr...|   53|\n",
      "|    HTML5 The Basics|   52|\n",
      "|   Python Epiphanies|   51|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select exam_name, count(exam_name) as total from messages group by exam_name order by total desc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the JSON encoding and the tree structure, we can use SQL to extrat any values that we need inside the DataFrame. This last SQL command is the same extraction that I did using Pandas and dictionaries on the last Assignment, but in one line of code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the Grades for each assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+\n",
      "|           exam_name|avg_grades|std_grades|\n",
      "+--------------------+----------+----------+\n",
      "|Example Exam For ...|      null|      null|\n",
      "|Client-Side Data ...|      20.0|     28.28|\n",
      "|       View Updating|      25.0|      50.0|\n",
      "|Native Web Apps f...|      25.0|       NaN|\n",
      "|Arduino Prototypi...|     33.33|     23.57|\n",
      "|Mastering Advance...|     36.03|     32.68|\n",
      "|           Nullology|      37.5|     51.75|\n",
      "| Mastering Web Views|     41.67|     52.04|\n",
      "|Building Web Serv...|     41.67|     28.87|\n",
      "|Web & Native Work...|     41.67|     23.57|\n",
      "|Cloud Computing W...|     42.65|     38.29|\n",
      "|         Offline Web|     43.59|     34.39|\n",
      "|Learning C# Best ...|     46.29|     26.02|\n",
      "|Design Patterns i...|     46.67|     34.36|\n",
      "|Software Architec...|     47.94|     28.29|\n",
      "|  Learning Java EE 7|      48.0|     32.03|\n",
      "|Data Visualizatio...|     49.19|     34.45|\n",
      "|Being a Better In...|      50.0|     47.14|\n",
      "|Learning Data Mod...|      50.0|     27.95|\n",
      "|Learning iPython ...|      50.0|     34.23|\n",
      "+--------------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select exam_name, round(avg((sequences.counts.correct)*100/(sequences.counts.total)),2) as avg_grades, \\\n",
    "            round(stddev((sequences.counts.correct)*100/(sequences.counts.total)),2) as std_grades \\\n",
    "           from messages group by exam_name order by avg_grades\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the average grades for each course, in %. This type of statistic is very important for us to determine how our students are doing on their assignments, and later can be done dynamically. We also add the standard deviation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract all the tables to HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_messages.write.parquet('/tmp/raw_data1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Extracted JSON - for Data regarding the pool of Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|            key|value|\n",
      "+---------------+-----+\n",
      "|user_incomplete| true|\n",
      "|    user_result| null|\n",
      "| user_submitted| true|\n",
      "|        options| null|\n",
      "|   user_correct|false|\n",
      "+---------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = naive_extracted_messages.select(explode(naive_extracted_messages.sequences))\n",
    "b = a.select(explode(a.value))\n",
    "questions = b.select(explode(b.col))\n",
    "questions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions.write.parquet('/tmp/questions_pool_data1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the question pool, we needed to break the maps created by the implied Schema. I did that using explode two times, and then store it as question pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grades for each assignment by ID and by Course Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ID\n",
    "grades_ID = spark.sql(\"select base_exam_id, (sequences.counts.correct)*100/(sequences.counts.total) as grades \\\n",
    "           from messages\")\n",
    "grades_ID.write.parquet('/tmp/grades_ID1')\n",
    "#Course\n",
    "course_ID = spark.sql(\"select exam_name, round(avg((sequences.counts.correct)*100/(sequences.counts.total)),2) as avg_grades, \\\n",
    "            round(stddev((sequences.counts.correct)*100/(sequences.counts.total)),2) as std_grades \\\n",
    "           from messages group by exam_name order by avg_grades\")\n",
    "course_ID.write.parquet('/tmp/course_ID1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
