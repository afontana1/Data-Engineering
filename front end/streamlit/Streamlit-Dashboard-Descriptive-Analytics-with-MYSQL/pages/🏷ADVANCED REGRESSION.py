import streamlit as st
import pandas as pd
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.model_selection import train_test_split
from streamlit_extras.metric_cards import style_metric_cards

# from query import *
st.set_option("deprecation.showPyplotGlobalUse", False)

# navicon and header
st.set_page_config(page_title="Dashboard", page_icon="üìà", layout="wide")

with open("style.css") as f:
    st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)

# current date
from datetime import datetime

current_datetime = datetime.now()
formatted_date = current_datetime.strftime("%Y-%m-%d")
formatted_day = current_datetime.strftime("%A")

st.header(" MACHINE LEARNING WORKFLOW | MYSQL  ")
st.markdown(
    """
 <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet">
 <hr>

<div class="card mb-3">
<div class="card">
  <div class="card-body">
    <h3 class="card-title"style="color:#007710;"><strong>‚è± MULTIPLE REGRESSION ANALYSIS DASHBOARD</strong></h3>
    <p class="card-text">There are three features, InterestRate, UnemploymentRate and PriceIndex. The purpose is to check how far linear relationship  is between these variables, where InterestRate and UnemploymentRate are X features and IndexPrice is Y feature. This is a classification problem using probabilistic multiple regression analysis for the data that exists in mysql. Finnaly visualizing measure of Variations and Line of best fit</p>
    <p class="card-text"><small class="text-body-secondary"> </small></p>
  </div>
</div>
</div>
""",
    unsafe_allow_html=True,
)

# uncomment line 1,3 and 3 if you use mysql database
# 1. read data from mysql
# 2. result = view_all_data()
# 3. df = pd.DataFrame(result,columns=["id","year","month","interest_rate","unemployment_rate","index_price"])

df = pd.read_csv("advanced_regression.csv")
# logo


with st.sidebar:
    st.markdown(
        f"<h4 class='text-success'>{formatted_day}: {formatted_date}</h4>Analytics Dashboard V: 01/2023<hr>",
        unsafe_allow_html=True,
    )


# switcher
year_ = st.sidebar.multiselect(
    "PICK YEAR:", options=df["year"].unique(), default=df["year"].unique()
)
month_ = st.sidebar.multiselect(
    "PICK MONTH:",
    options=df["month"].unique(),
    default=df["month"].unique(),
)

df_selection = df.query("month == @month_ & year ==@year_")

# download csv
with st.sidebar:
    df_download = df_selection.to_csv(index=False).encode("utf-8")
    st.download_button(
        label="Download DataFrame from Mysql",
        data=df_download,
        key="download_dataframe.csv",
        file_name="my_dataframe.csv",
    )

# drop unnecessary fields
df_selection.drop(columns=["id", "year", "month"], axis=1, inplace=True)

# theme_plotly = None # None or streamlit

with st.expander("‚¨á EXPLORATORY  ANALYSIS"):
    st.write(
        "Examining the correlation between the independent variables (features) and the dependent variable before actually building and training a regression model. This is an important step in the initial data exploration and analysis phase to understand the relationships between variables."
    )
    col_a, col_b = st.columns(2)
    with col_a:
        st.subheader("Interest Vs Unemployment")
        plt.figure(figsize=(4, 4))
        sns.regplot(
            x=df_selection["interest_rate"],
            y=df_selection["unemployment_rate"],
            color="#007710",
        )
        plt.xlabel("Interest Rate")
        plt.ylabel("Unemployment Rate")
        plt.title("Interest Rate vs UnemploymentRate: Regression Plot")
        st.pyplot()


with col_b:
    plt.figure(figsize=(4, 4))
    st.subheader("Interest Vs Index Price")
    sns.regplot(
        x=df_selection["interest_rate"], y=df_selection["index_price"], color="#007710"
    )
    plt.xlabel("Interest Rate")
    plt.ylabel("Unemployment Rate")
    plt.title("InterestRate vs IndexPrice Regression Plot")
    st.pyplot()

    fig, ax = plt.subplots()
    st.subheader(
        "Variables outliers",
    )
    sns.boxplot(data=df, orient="h", color="#FF4B4B")
    plt.show()
    st.pyplot()

with st.expander("‚¨á EXPLORATORY VARIABLE DISTRIBUTIONS BY FREQUENCY: HISTOGRAM"):
    df_selection.hist(
        figsize=(16, 8),
        color="#007710",
        zorder=2,
        rwidth=0.9,
        legend=["unemployment_rate"],
    )
    st.pyplot()

with st.expander("‚¨á EXPLORATORY VARIABLES DISTRIBUTIONS:"):
    st.subheader(
        "Correlation between variables",
    )
    # https://seaborn.pydata.org/generated/seaborn.pairplot.html
    pairplot = sns.pairplot(
        df_selection, plot_kws=dict(marker="+", linewidth=1), diag_kws=dict(fill=True)
    )
    st.pyplot(pairplot)


# checking null value
with st.expander("‚¨á NULL VALUES, TENDENCY & VARIABLE DISPERSION"):
    a1, a2 = st.columns(2)
    a1.write("number of missing (NaN or None) values in each column of a DataFrame")
    a1.dataframe(df_selection.isnull().sum(), use_container_width=True)
    a2.write(
        "insights into the central tendency, dispersion, and distribution of the data."
    )
    a2.dataframe(df_selection.describe().T, use_container_width=True)


# train and test split
with st.expander("‚¨á DEFAULT CORRELATION"):
    st.dataframe(df_selection.corr())
    st.subheader(
        "Correlation",
    )
    st.write("correlation coefficients between Interest Rate Rate & Unemployment Rate")
    plt.scatter(df_selection["interest_rate"], df_selection["unemployment_rate"])
    plt.ylabel("Unemployment rate")
    plt.xlabel("Interest rate")
    st.pyplot()

try:

    # independent and dependent features
    X = df_selection.iloc[:, :-1]  # left a last column
    y = df_selection.iloc[:, -1]  # take a last column

    # train test split
    from sklearn.model_selection import train_test_split

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=42
    )

    with st.expander("‚¨á UNIFORM  DISTRIBUTION "):
        st.subheader(
            "Standard Scores (Z-Scores)",
        )
        st.write(
            "transform data so that it has a mean (average) of 0 and a standard deviation of 1. This process is also known as [feature scaling] or [standardization.]"
        )
        from sklearn.preprocessing import StandardScaler

        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_test = scaler.fit_transform(X_test)
        st.dataframe(X_train)

    from sklearn.linear_model import LinearRegression

    regression = LinearRegression()
    regression.fit(X_train, y_train)

    # cross validation
    from sklearn.model_selection import cross_val_score

    validation_score = cross_val_score(
        regression, X_train, y_train, scoring="neg_mean_squared_error", cv=3
    )

    col1, col3, col4, col5 = st.columns(4)
    col1.metric(
        label="üü° MEAN VALIDATION SCORE",
        value=np.mean(validation_score),
        delta=f"{ np.mean(validation_score):,.0f}",
    )

    # prediction
    y_pred = regression.predict(X_test)

    # performance metrics
    from sklearn.metrics import mean_squared_error, mean_absolute_error

    meansquareerror = mean_squared_error(y_test, y_pred)
    meanabsluteerror = mean_absolute_error(y_test, y_pred)
    rootmeansquareerror = np.sqrt(meansquareerror)

    col3.metric(
        label="üü° MEAN SQUARED ERROR ",
        value=np.mean(meansquareerror),
        delta=f"{ np.mean(meansquareerror):,.0f}",
    )
    col4.metric(
        label="üü° MEAN ABSOLUTE ERROR",
        value=np.mean(meanabsluteerror),
        delta=f"{ np.mean(meanabsluteerror):,.0f}",
    )
    col5.metric(
        label="üü° ROOT MEAN SQUARED ERROR",
        value=np.mean(rootmeansquareerror),
        delta=f"{ np.mean(rootmeansquareerror):,.0f}",
    )

    with st.expander("‚¨á COEFFICIENT OF DETERMINATION | R2"):
        from sklearn.metrics import r2_score

        score = r2_score(y_test, y_pred)
        st.metric(label="üî∑ r", value=score, delta=f"{ score:,.0f}")

    with st.expander("‚¨á ADJUSTED CORRERATION COEFFICIENT | R"):
        # display adjusted R_squared
        st.metric(
            label="üî∑ Adjusted R",
            value=(
                (
                    1
                    - (1 - score)
                    * (len(y_test) - 1)
                    / (len(y_test) - X_test.shape[1] - 1)
                )
            ),
            delta=f"{ ((1-(1-score)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1))):,.0f}",
        )

    with st.expander("‚¨á CORRERATION COEFFICIENT | r"):
        # display correlation
        st.write(regression.coef_)

    # https://seaborn.pydata.org/generated/seaborn.regplot.html
    c1, c2, c3 = st.columns(3)
    with c1:
        with st.expander("‚¨á LINE OF BEST FIT"):
            st.write(
                "regression line that best represents the relationship between the independent variable(s) and the dependent variable in a linear regression model. This line is determined through a mathematical process that aims to minimize the error between the observed data points and the predicted values generated by the model."
            )
            plt.figure(figsize=(8, 6))
            sns.regplot(
                x=y_test, y=y_pred, color="#FF4B4B", line_kws=dict(color="#007710")
            )
            plt.xlabel("Interest Rate")
            plt.ylabel("Unemployment Rate")
            plt.title("Interest Rate vs Unemployment_Rate Regression Plot")
            st.pyplot()

    with c2:
        with st.expander("‚¨á RESIDUAL"):
            st.write(
                "residuals: refers to the differences between the actual observed values (the dependent variable, often denoted as y) and the predicted values made by a regression model (often denoted as y_pred). These residuals represent how much the model's predictions deviate from the actual data points"
            )
            residuals = y_test - y_pred
            st.dataframe(residuals)

    with c3:
        with st.expander("‚¨á MODEL PERFORMANCE | NORMAL DISTRIBUTION CURVE"):
            st.write(
                "distribution of a continuous random variable where data tends to be symmetrically distributed around a mean (average) value. It is a fundamental concept in statistics and probability theory."
            )
            sns.displot(
                residuals, kind="kde", legend=True, color="#007710"
            )  # kernel density estimator
            st.pyplot()

    with st.expander("‚¨á OLS, or Ordinary Least Squares Method"):
        import statsmodels.api as sm

        model = sm.OLS(y_train, X_train).fit()
        st.write(model.summary())

    st.sidebar.image("data/logo1.png")
    style_metric_cards(
        background_color="#FFFFFF",
        border_left_color="#686664",
        border_color="#000000",
        box_shadow="#F71938",
    )

except:
    st.error(
        "‚ùå THE AMOUNT OF DATA YOU SELECTED IS NOT ENOUGH FOR THE MODEL TO PERFORM PROPERLY"
    )
